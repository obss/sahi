{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"SAHI: Slicing Aided Hyper Inference     A lightweight vision library for performing large scale object detection &amp; instance segmentation"},{"location":"#what-is-sahi","title":"What is SAHI?","text":"<p>SAHI (Slicing Aided Hyper Inference) is an open-source framework that provides a generic slicing-aided inference and fine-tuning pipeline for small object detection. Detecting small objects and those far from the camera is a major challenge in surveillance applications, as they are represented by a small number of pixels and lack sufficient detail for conventional detectors.</p> <p>SAHI addresses this by applying a unique methodology that can be used with any object detector without requiring additional fine-tuning. Experimental evaluations on the Visdrone and xView aerial object detection datasets show that SAHI can increase object detection AP by up to 6.8% for FCOS, 5.1% for VFNet, and 5.3% for TOOD detectors. With slicing-aided fine-tuning, the accuracy can be further improved, resulting in a cumulative increase of 12.7%, 13.4%, and 14.5% AP, respectively. The technique has been successfully integrated with Detectron2, MMDetection, and YOLOv5 models.</p> <ul> <li> <p> Getting Started</p> <p>Install <code>sahi</code> with pip and get up and running in minutes.</p> <p> Quickstart</p> </li> <li> <p> Predict</p> <p>Predict on new images, videos and streams with SAHI.</p> <p> Learn more</p> </li> <li> <p> Slicing</p> <p>Learn how to slice large images and datasets for inference.</p> <p> Learn more</p> </li> <li> <p> COCO Utilities</p> <p>Work with COCO format datasets, including creation, splitting, and filtering.</p> <p> Learn more</p> </li> <li> <p> CLI Commands</p> <p>Use SAHI from the command-line for prediction and dataset operations.</p> <p> Learn more</p> </li> </ul>"},{"location":"#interactive-examples","title":"Interactive Examples","text":"<p>All documentation files are complemented by interactive Jupyter notebooks in the demo directory:</p> <ul> <li> <p> Slicing</p> <p>Slicing operations demonstration.</p> <p> Open Notebook</p> </li> <li> <p> Ultralytics</p> <p>YOLOv8/YOLO11/YOLO12 integration.</p> <p> Open Notebook</p> </li> <li> <p> YOLOv5</p> <p>YOLOv5 integration.</p> <p> Open Notebook</p> </li> <li> <p> MMDetection</p> <p>MMDetection integration.</p> <p> Open Notebook</p> </li> <li> <p> HuggingFace</p> <p>HuggingFace models integration.</p> <p> Open Notebook</p> </li> <li> <p> TorchVision</p> <p>TorchVision models integration.</p> <p> Open Notebook</p> </li> <li> <p> RT-DETR</p> <p>RT-DETR integration.</p> <p> Open Notebook</p> </li> </ul>"},{"location":"annotation/","title":"Annotation","text":""},{"location":"annotation/#sahi.annotation","title":"<code>sahi.annotation</code>","text":""},{"location":"annotation/#sahi.annotation-classes","title":"Classes","text":""},{"location":"annotation/#sahi.annotation.BoundingBox","title":"<code>BoundingBox</code>  <code>dataclass</code>","text":"<p>BoundingBox represents a rectangular region in 2D space, typically used for object detection annotations.</p> <p>Attributes:</p> Name Type Description <code>box</code> <code>Tuple[float, float, float, float]</code> <p>The bounding box coordinates in the format (minx, miny, maxx, maxy). - minx (float): Minimum x-coordinate (left). - miny (float): Minimum y-coordinate (top). - maxx (float): Maximum x-coordinate (right). - maxy (float): Maximum y-coordinate (bottom).</p> <code>shift_amount</code> <code>Tuple[int, int]</code> <p>The amount to shift the bounding box in the x and y directions. Defaults to (0, 0).</p> <p>BoundingBox Usage Example</p> <pre><code>bbox = BoundingBox((10.0, 20.0, 50.0, 80.0))\narea = bbox.area\nexpanded_bbox = bbox.get_expanded_box(ratio=0.2)\nshifted_bbox = bbox.get_shifted_box()\ncoco_format = bbox.to_coco_bbox()\n</code></pre> Source code in <code>sahi/annotation.py</code> <pre><code>@dataclass(frozen=True)\nclass BoundingBox:\n    \"\"\"BoundingBox represents a rectangular region in 2D space, typically used for object detection annotations.\n\n    Attributes:\n        box (Tuple[float, float, float, float]): The bounding box coordinates in the format (minx, miny, maxx, maxy).\n            - minx (float): Minimum x-coordinate (left).\n            - miny (float): Minimum y-coordinate (top).\n            - maxx (float): Maximum x-coordinate (right).\n            - maxy (float): Maximum y-coordinate (bottom).\n        shift_amount (Tuple[int, int], optional): The amount to shift the bounding box in the x and y directions.\n            Defaults to (0, 0).\n\n    !!! example \"BoundingBox Usage Example\"\n        ```python\n        bbox = BoundingBox((10.0, 20.0, 50.0, 80.0))\n        area = bbox.area\n        expanded_bbox = bbox.get_expanded_box(ratio=0.2)\n        shifted_bbox = bbox.get_shifted_box()\n        coco_format = bbox.to_coco_bbox()\n        ```\n    \"\"\"\n\n    box: tuple[float, float, float, float] | list[float]\n    shift_amount: tuple[int, int] = (0, 0)\n\n    def __post_init__(self):\n        if len(self.box) != 4 or any(coord &lt; 0 for coord in self.box):\n            raise ValueError(\"box must be 4 non-negative floats: [minx, miny, maxx, maxy]\")\n        if len(self.shift_amount) != 2:\n            raise ValueError(\"shift_amount must be 2 integers: [shift_x, shift_y]\")\n\n    @property\n    def minx(self):\n        return self.box[0]\n\n    @property\n    def miny(self):\n        return self.box[1]\n\n    @property\n    def maxx(self):\n        return self.box[2]\n\n    @property\n    def maxy(self):\n        return self.box[3]\n\n    @property\n    def shift_x(self):\n        return self.shift_amount[0]\n\n    @property\n    def shift_y(self):\n        return self.shift_amount[1]\n\n    @property\n    def area(self):\n        return (self.maxx - self.minx) * (self.maxy - self.miny)\n\n    def get_expanded_box(self, ratio: float = 0.1, max_x: int | None = None, max_y: int | None = None):\n        \"\"\"Returns an expanded bounding box by increasing its size by a given ratio. The expansion is applied equally in\n        all directions. Optionally, the expanded box can be clipped to maximum x and y boundaries.\n\n        Args:\n            ratio (float, optional): The proportion by which to expand the box size.\n                Default is 0.1 (10%).\n            max_x (int, optional): The maximum allowed x-coordinate for the expanded box.\n                If None, no maximum is applied.\n            max_y (int, optional): The maximum allowed y-coordinate for the expanded box.\n                If None, no maximum is applied.\n\n        Returns:\n            BoundingBox: A new BoundingBox instance representing the expanded box.\n        \"\"\"\n\n        w = self.maxx - self.minx\n        h = self.maxy - self.miny\n        y_mar = int(w * ratio)\n        x_mar = int(h * ratio)\n        maxx = min(max_x, self.maxx + x_mar) if max_x else self.maxx + x_mar\n        minx = max(0, self.minx - x_mar)\n        maxy = min(max_y, self.maxy + y_mar) if max_y else self.maxy + y_mar\n        miny = max(0, self.miny - y_mar)\n        box: list[float] = [minx, miny, maxx, maxy]\n        return BoundingBox(box)\n\n    def to_xywh(self):\n        \"\"\"Returns [xmin, ymin, width, height]\n\n        Returns:\n            List[float]: A list containing the bounding box in the format [xmin, ymin, width, height].\n        \"\"\"\n\n        return [self.minx, self.miny, self.maxx - self.minx, self.maxy - self.miny]\n\n    def to_coco_bbox(self):\n        \"\"\"\n        Returns the bounding box in COCO format: [xmin, ymin, width, height]\n\n        Returns:\n            List[float]: A list containing the bounding box in COCO format.\n        \"\"\"\n        return self.to_xywh()\n\n    def to_xyxy(self):\n        \"\"\"\n        Returns: [xmin, ymin, xmax, ymax]\n\n        Returns:\n            List[float]: A list containing the bounding box in the format [xmin, ymin, xmax, ymax].\n        \"\"\"\n        return [self.minx, self.miny, self.maxx, self.maxy]\n\n    def to_voc_bbox(self):\n        \"\"\"\n        Returns the bounding box in VOC format: [xmin, ymin, xmax, ymax]\n\n        Returns:\n            List[float]: A list containing the bounding box in VOC format.\n        \"\"\"\n        return self.to_xyxy()\n\n    def get_shifted_box(self):\n        \"\"\"Returns shifted BoundingBox.\n\n        Returns:\n            BoundingBox: A new BoundingBox instance representing the shifted box.\n        \"\"\"\n        box = [\n            self.minx + self.shift_x,\n            self.miny + self.shift_y,\n            self.maxx + self.shift_x,\n            self.maxy + self.shift_y,\n        ]\n        return BoundingBox(box)\n\n    def __repr__(self):\n        return (\n            f\"BoundingBox: &lt;{(self.minx, self.miny, self.maxx, self.maxy)}, \"\n            f\"w: {self.maxx - self.minx}, h: {self.maxy - self.miny}&gt;\"\n        )\n</code></pre>"},{"location":"annotation/#sahi.annotation.BoundingBox-functions","title":"Functions","text":""},{"location":"annotation/#sahi.annotation.BoundingBox.get_expanded_box","title":"<code>get_expanded_box(ratio=0.1, max_x=None, max_y=None)</code>","text":"<p>Returns an expanded bounding box by increasing its size by a given ratio. The expansion is applied equally in all directions. Optionally, the expanded box can be clipped to maximum x and y boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> \u00b6 <code>float</code> <p>The proportion by which to expand the box size. Default is 0.1 (10%).</p> <code>0.1</code> <code>max_x</code> \u00b6 <code>int</code> <p>The maximum allowed x-coordinate for the expanded box. If None, no maximum is applied.</p> <code>None</code> <code>max_y</code> \u00b6 <code>int</code> <p>The maximum allowed y-coordinate for the expanded box. If None, no maximum is applied.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>BoundingBox</code> <p>A new BoundingBox instance representing the expanded box.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def get_expanded_box(self, ratio: float = 0.1, max_x: int | None = None, max_y: int | None = None):\n    \"\"\"Returns an expanded bounding box by increasing its size by a given ratio. The expansion is applied equally in\n    all directions. Optionally, the expanded box can be clipped to maximum x and y boundaries.\n\n    Args:\n        ratio (float, optional): The proportion by which to expand the box size.\n            Default is 0.1 (10%).\n        max_x (int, optional): The maximum allowed x-coordinate for the expanded box.\n            If None, no maximum is applied.\n        max_y (int, optional): The maximum allowed y-coordinate for the expanded box.\n            If None, no maximum is applied.\n\n    Returns:\n        BoundingBox: A new BoundingBox instance representing the expanded box.\n    \"\"\"\n\n    w = self.maxx - self.minx\n    h = self.maxy - self.miny\n    y_mar = int(w * ratio)\n    x_mar = int(h * ratio)\n    maxx = min(max_x, self.maxx + x_mar) if max_x else self.maxx + x_mar\n    minx = max(0, self.minx - x_mar)\n    maxy = min(max_y, self.maxy + y_mar) if max_y else self.maxy + y_mar\n    miny = max(0, self.miny - y_mar)\n    box: list[float] = [minx, miny, maxx, maxy]\n    return BoundingBox(box)\n</code></pre>"},{"location":"annotation/#sahi.annotation.BoundingBox.get_shifted_box","title":"<code>get_shifted_box()</code>","text":"<p>Returns shifted BoundingBox.</p> <p>Returns:</p> Name Type Description <code>BoundingBox</code> <p>A new BoundingBox instance representing the shifted box.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def get_shifted_box(self):\n    \"\"\"Returns shifted BoundingBox.\n\n    Returns:\n        BoundingBox: A new BoundingBox instance representing the shifted box.\n    \"\"\"\n    box = [\n        self.minx + self.shift_x,\n        self.miny + self.shift_y,\n        self.maxx + self.shift_x,\n        self.maxy + self.shift_y,\n    ]\n    return BoundingBox(box)\n</code></pre>"},{"location":"annotation/#sahi.annotation.BoundingBox.to_coco_bbox","title":"<code>to_coco_bbox()</code>","text":"<p>Returns the bounding box in COCO format: [xmin, ymin, width, height]</p> <p>Returns:</p> Type Description <p>List[float]: A list containing the bounding box in COCO format.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_coco_bbox(self):\n    \"\"\"\n    Returns the bounding box in COCO format: [xmin, ymin, width, height]\n\n    Returns:\n        List[float]: A list containing the bounding box in COCO format.\n    \"\"\"\n    return self.to_xywh()\n</code></pre>"},{"location":"annotation/#sahi.annotation.BoundingBox.to_voc_bbox","title":"<code>to_voc_bbox()</code>","text":"<p>Returns the bounding box in VOC format: [xmin, ymin, xmax, ymax]</p> <p>Returns:</p> Type Description <p>List[float]: A list containing the bounding box in VOC format.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_voc_bbox(self):\n    \"\"\"\n    Returns the bounding box in VOC format: [xmin, ymin, xmax, ymax]\n\n    Returns:\n        List[float]: A list containing the bounding box in VOC format.\n    \"\"\"\n    return self.to_xyxy()\n</code></pre>"},{"location":"annotation/#sahi.annotation.BoundingBox.to_xywh","title":"<code>to_xywh()</code>","text":"<p>Returns [xmin, ymin, width, height]</p> <p>Returns:</p> Type Description <p>List[float]: A list containing the bounding box in the format [xmin, ymin, width, height].</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_xywh(self):\n    \"\"\"Returns [xmin, ymin, width, height]\n\n    Returns:\n        List[float]: A list containing the bounding box in the format [xmin, ymin, width, height].\n    \"\"\"\n\n    return [self.minx, self.miny, self.maxx - self.minx, self.maxy - self.miny]\n</code></pre>"},{"location":"annotation/#sahi.annotation.BoundingBox.to_xyxy","title":"<code>to_xyxy()</code>","text":"<p>Returns: [xmin, ymin, xmax, ymax]</p> <p>Returns:</p> Type Description <p>List[float]: A list containing the bounding box in the format [xmin, ymin, xmax, ymax].</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_xyxy(self):\n    \"\"\"\n    Returns: [xmin, ymin, xmax, ymax]\n\n    Returns:\n        List[float]: A list containing the bounding box in the format [xmin, ymin, xmax, ymax].\n    \"\"\"\n    return [self.minx, self.miny, self.maxx, self.maxy]\n</code></pre>"},{"location":"annotation/#sahi.annotation.Category","title":"<code>Category</code>  <code>dataclass</code>","text":"<p>Category of the annotation.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Unique identifier for the category.</p> <code>name</code> <code>str</code> <p>Name of the category.</p> Source code in <code>sahi/annotation.py</code> <pre><code>@dataclass(frozen=True)\nclass Category:\n    \"\"\"Category of the annotation.\n\n    Attributes:\n        id (int): Unique identifier for the category.\n        name (str): Name of the category.\n    \"\"\"\n\n    id: int\n    name: str\n\n    def __post_init__(self):\n        if not isinstance(self.id, int):\n            raise TypeError(\"id should be integer\")\n        if not isinstance(self.name, str):\n            raise TypeError(\"name should be string\")\n\n    def __repr__(self):\n        return f\"Category: &lt;id: {self.id}, name: {self.name}&gt;\"\n</code></pre>"},{"location":"annotation/#sahi.annotation.Mask","title":"<code>Mask</code>","text":"<p>Init Mask from coco segmentation representation.</p> <p>Parameters:</p> Name Type Description Default <p>List[List] [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> required <code>list[int]</code> <p>List[int] Size of the full image, should be in the form of [height, width]</p> required <code>list</code> <p>List[int] To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>class Mask:\n    \"\"\"Init Mask from coco segmentation representation.\n\n    Args:\n        segmentation : List[List]\n            [\n                [x1, y1, x2, y2, x3, y3, ...],\n                [x1, y1, x2, y2, x3, y3, ...],\n                ...\n            ]\n        full_shape: List[int]\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List[int]\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n\n    def __init__(\n        self,\n        segmentation: list[list[float]],\n        full_shape: list[int],\n        shift_amount: list = [0, 0],\n    ):\n        if full_shape is None:\n            raise ValueError(\"full_shape must be provided\")  # pyright: ignore[reportUnreachable]\n\n        self.shift_x = shift_amount[0]\n        self.shift_y = shift_amount[1]\n        self.full_shape_height = full_shape[0]\n        self.full_shape_width = full_shape[1]\n        self.segmentation = segmentation\n\n    @classmethod\n    def from_float_mask(\n        cls,\n        mask: np.ndarray,\n        full_shape: list[int],\n        mask_threshold: float = 0.5,\n        shift_amount: list = [0, 0],\n    ):\n        \"\"\"\n        Args:\n            mask: np.ndarray of np.float elements\n                Mask values between 0 and 1 (should have a shape of height*width)\n            mask_threshold: float\n                Value to threshold mask pixels between 0 and 1\n            shift_amount: List\n                To shift the box and mask predictions from sliced image\n                to full sized image, should be in the form of [shift_x, shift_y]\n            full_shape: List[int]\n                Size of the full image after shifting, should be in the form of [height, width]\n        \"\"\"\n        bool_mask = mask &gt; mask_threshold\n        return cls(\n            segmentation=get_coco_segmentation_from_bool_mask(bool_mask),\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @classmethod\n    def from_bool_mask(\n        cls,\n        bool_mask: np.ndarray,\n        full_shape: list[int],\n        shift_amount: list = [0, 0],\n    ):\n        \"\"\"\n        Args:\n            bool_mask: np.ndarray with bool elements\n                2D mask of object, should have a shape of height*width\n            full_shape: List[int]\n                Size of the full image, should be in the form of [height, width]\n            shift_amount: List[int]\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n        \"\"\"\n        return cls(\n            segmentation=get_coco_segmentation_from_bool_mask(bool_mask),\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @property\n    def bool_mask(self) -&gt; np.ndarray:\n        return get_bool_mask_from_coco_segmentation(\n            self.segmentation, width=self.full_shape[1], height=self.full_shape[0]\n        )\n\n    @property\n    def shape(self) -&gt; list[int]:\n        \"\"\"Returns mask shape as [height, width]\"\"\"\n        return [self.bool_mask.shape[0], self.bool_mask.shape[1]]\n\n    @property\n    def full_shape(self) -&gt; list[int]:\n        \"\"\"Returns full mask shape after shifting as [height, width]\"\"\"\n        return [self.full_shape_height, self.full_shape_width]\n\n    @property\n    def shift_amount(self):\n        \"\"\"Returns the shift amount of the mask slice as [shift_x, shift_y]\"\"\"\n        return [self.shift_x, self.shift_y]\n\n    def get_shifted_mask(self) -&gt; Mask:\n        # Confirm full_shape is specified\n        if (self.full_shape_height is None) or (self.full_shape_width is None):\n            raise ValueError(\"full_shape is None\")\n        shifted_segmentation = []\n        for s in self.segmentation:\n            xs = [min(self.shift_x + s[i], self.full_shape_width) for i in range(0, len(s) - 1, 2)]\n            ys = [min(self.shift_y + s[i], self.full_shape_height) for i in range(1, len(s), 2)]\n            shifted_segmentation.append([j for i in zip(xs, ys) for j in i])\n        return Mask(\n            segmentation=shifted_segmentation,\n            shift_amount=[0, 0],\n            full_shape=self.full_shape,\n        )\n</code></pre>"},{"location":"annotation/#sahi.annotation.Mask(segmentation )","title":"<code>segmentation </code>","text":""},{"location":"annotation/#sahi.annotation.Mask(full_shape)","title":"<code>full_shape</code>","text":""},{"location":"annotation/#sahi.annotation.Mask(shift_amount)","title":"<code>shift_amount</code>","text":""},{"location":"annotation/#sahi.annotation.Mask-attributes","title":"Attributes","text":""},{"location":"annotation/#sahi.annotation.Mask.full_shape","title":"<code>full_shape</code>  <code>property</code>","text":"<p>Returns full mask shape after shifting as [height, width]</p>"},{"location":"annotation/#sahi.annotation.Mask.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Returns mask shape as [height, width]</p>"},{"location":"annotation/#sahi.annotation.Mask.shift_amount","title":"<code>shift_amount</code>  <code>property</code>","text":"<p>Returns the shift amount of the mask slice as [shift_x, shift_y]</p>"},{"location":"annotation/#sahi.annotation.Mask-functions","title":"Functions","text":""},{"location":"annotation/#sahi.annotation.Mask.from_bool_mask","title":"<code>from_bool_mask(bool_mask, full_shape, shift_amount=[0, 0])</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>bool_mask</code> \u00b6 <code>ndarray</code> <p>np.ndarray with bool elements 2D mask of object, should have a shape of height*width</p> required <code>full_shape</code> \u00b6 <code>list[int]</code> <p>List[int] Size of the full image, should be in the form of [height, width]</p> required <code>shift_amount</code> \u00b6 <code>list</code> <p>List[int] To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_bool_mask(\n    cls,\n    bool_mask: np.ndarray,\n    full_shape: list[int],\n    shift_amount: list = [0, 0],\n):\n    \"\"\"\n    Args:\n        bool_mask: np.ndarray with bool elements\n            2D mask of object, should have a shape of height*width\n        full_shape: List[int]\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List[int]\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n    return cls(\n        segmentation=get_coco_segmentation_from_bool_mask(bool_mask),\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre>"},{"location":"annotation/#sahi.annotation.Mask.from_float_mask","title":"<code>from_float_mask(mask, full_shape, mask_threshold=0.5, shift_amount=[0, 0])</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>mask</code> \u00b6 <code>ndarray</code> <p>np.ndarray of np.float elements Mask values between 0 and 1 (should have a shape of height*width)</p> required <code>mask_threshold</code> \u00b6 <code>float</code> <p>float Value to threshold mask pixels between 0 and 1</p> <code>0.5</code> <code>shift_amount</code> \u00b6 <code>list</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> <code>full_shape</code> \u00b6 <code>list[int]</code> <p>List[int] Size of the full image after shifting, should be in the form of [height, width]</p> required Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_float_mask(\n    cls,\n    mask: np.ndarray,\n    full_shape: list[int],\n    mask_threshold: float = 0.5,\n    shift_amount: list = [0, 0],\n):\n    \"\"\"\n    Args:\n        mask: np.ndarray of np.float elements\n            Mask values between 0 and 1 (should have a shape of height*width)\n        mask_threshold: float\n            Value to threshold mask pixels between 0 and 1\n        shift_amount: List\n            To shift the box and mask predictions from sliced image\n            to full sized image, should be in the form of [shift_x, shift_y]\n        full_shape: List[int]\n            Size of the full image after shifting, should be in the form of [height, width]\n    \"\"\"\n    bool_mask = mask &gt; mask_threshold\n    return cls(\n        segmentation=get_coco_segmentation_from_bool_mask(bool_mask),\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre>"},{"location":"annotation/#sahi.annotation.ObjectAnnotation","title":"<code>ObjectAnnotation</code>","text":"<p>All about an annotation such as Mask, Category, BoundingBox.</p> Source code in <code>sahi/annotation.py</code> <pre><code>class ObjectAnnotation:\n    \"\"\"All about an annotation such as Mask, Category, BoundingBox.\"\"\"\n\n    def __init__(\n        self,\n        bbox: list[int] | None = None,\n        segmentation: np.ndarray | None = None,\n        category_id: int | None = None,\n        category_name: str | None = None,\n        shift_amount: list[int] | None = [0, 0],\n        full_shape: list[int] | None = None,\n    ):\n        \"\"\"\n        Args:\n            bbox: List\n                [minx, miny, maxx, maxy]\n            segmentation: List[List]\n                [\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    ...\n                ]\n            category_id: int\n                ID of the object category\n            category_name: str\n                Name of the object category\n            shift_amount: List\n                To shift the box and mask predictions from sliced image\n                to full sized image, should be in the form of [shift_x, shift_y]\n            full_shape: List\n                Size of the full image after shifting, should be in\n                the form of [height, width]\n        \"\"\"\n        if not isinstance(category_id, int):\n            raise ValueError(\"category_id must be an integer\")\n        if (bbox is None) and (segmentation is None):\n            raise ValueError(\"you must provide a bbox or segmentation\")\n\n        self.mask: Mask | None = None\n        if segmentation is not None:\n            self.mask = Mask(\n                segmentation=segmentation,\n                shift_amount=shift_amount,\n                full_shape=full_shape,\n            )\n            bbox_from_segmentation = get_bbox_from_coco_segmentation(segmentation)\n            # https://github.com/obss/sahi/issues/235\n            if bbox_from_segmentation is not None:\n                bbox = bbox_from_segmentation\n            else:\n                raise ValueError(\"Invalid segmentation mask.\")\n\n        # if bbox is a numpy object, convert it to python List[float]\n        if type(bbox).__module__ == \"numpy\":\n            bbox = copy.deepcopy(bbox).tolist()\n\n        # make sure bbox coords lie inside [0, image_size]\n        xmin = max(bbox[0], 0)\n        ymin = max(bbox[1], 0)\n        if full_shape:\n            xmax = min(bbox[2], full_shape[1])\n            ymax = min(bbox[3], full_shape[0])\n        else:\n            xmax = bbox[2]\n            ymax = bbox[3]\n        bbox = [xmin, ymin, xmax, ymax]\n        # set bbox\n        self.bbox = BoundingBox(bbox, shift_amount)\n\n        category_name = category_name if category_name else str(category_id)\n        self.category = Category(\n            id=category_id,\n            name=category_name,\n        )\n\n        self.merged = None\n\n    @classmethod\n    def from_bool_mask(\n        cls,\n        bool_mask,\n        category_id: int | None = None,\n        category_name: str | None = None,\n        shift_amount: list[int] | None = [0, 0],\n        full_shape: list[int] | None = None,\n    ):\n        \"\"\"Creates ObjectAnnotation from bool_mask (2D np.ndarray)\n\n        Args:\n            bool_mask: np.ndarray with bool elements\n                2D mask of object, should have a shape of height*width\n            category_id: int\n                ID of the object category\n            category_name: str\n                Name of the object category\n            full_shape: List\n                Size of the full image, should be in the form of [height, width]\n            shift_amount: List\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n        \"\"\"\n        segmentation = get_coco_segmentation_from_bool_mask(bool_mask)\n        return cls(\n            category_id=category_id,\n            segmentation=segmentation,\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @classmethod\n    def from_coco_segmentation(\n        cls,\n        segmentation,\n        full_shape: list[int],\n        category_id: int | None = None,\n        category_name: str | None = None,\n        shift_amount: list[int] | None = [0, 0],\n    ):\n        \"\"\"\n        Creates ObjectAnnotation from coco segmentation:\n        [\n            [x1, y1, x2, y2, x3, y3, ...],\n            [x1, y1, x2, y2, x3, y3, ...],\n            ...\n        ]\n\n        Args:\n            segmentation: List[List]\n                [\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    ...\n                ]\n            category_id: int\n                ID of the object category\n            category_name: str\n                Name of the object category\n            full_shape: List\n                Size of the full image, should be in the form of [height, width]\n            shift_amount: List\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n        \"\"\"\n        return cls(\n            category_id=category_id,\n            segmentation=segmentation,\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @classmethod\n    def from_coco_bbox(\n        cls,\n        bbox: list[int],\n        category_id: int | None = None,\n        category_name: str | None = None,\n        shift_amount: list[int] | None = [0, 0],\n        full_shape: list[int] | None = None,\n    ):\n        \"\"\"Creates ObjectAnnotation from coco bbox [minx, miny, width, height]\n\n        Args:\n            bbox: List\n                [minx, miny, width, height]\n            category_id: int\n                ID of the object category\n            category_name: str\n                Name of the object category\n            full_shape: List\n                Size of the full image, should be in the form of [height, width]\n            shift_amount: List\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n        \"\"\"\n        xmin = bbox[0]\n        ymin = bbox[1]\n        xmax = bbox[0] + bbox[2]\n        ymax = bbox[1] + bbox[3]\n        bbox = [xmin, ymin, xmax, ymax]\n        return cls(\n            category_id=category_id,\n            bbox=bbox,\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @classmethod\n    def from_coco_annotation_dict(\n        cls,\n        annotation_dict: dict,\n        full_shape: list[int],\n        category_name: str | None = None,\n        shift_amount: list[int] | None = [0, 0],\n    ):\n        \"\"\"Creates ObjectAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\",\n        \"segmentation\", \"category_id\").\n\n        Args:\n            annotation_dict: dict\n                COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")\n            category_name: str\n                Category name of the annotation\n            full_shape: List\n                Size of the full image, should be in the form of [height, width]\n            shift_amount: List\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n        \"\"\"\n        if annotation_dict[\"segmentation\"]:\n            return cls.from_coco_segmentation(\n                segmentation=annotation_dict[\"segmentation\"],\n                category_id=annotation_dict[\"category_id\"],\n                category_name=category_name,\n                shift_amount=shift_amount,\n                full_shape=full_shape,\n            )\n        else:\n            return cls.from_coco_bbox(\n                bbox=annotation_dict[\"bbox\"],\n                category_id=annotation_dict[\"category_id\"],\n                category_name=category_name,\n                shift_amount=shift_amount,\n                full_shape=full_shape,\n            )\n\n    @classmethod\n    def from_shapely_annotation(\n        cls,\n        annotation: ShapelyAnnotation,\n        full_shape: list[int],\n        category_id: int | None = None,\n        category_name: str | None = None,\n        shift_amount: list[int] | None = [0, 0],\n    ):\n        \"\"\"Creates ObjectAnnotation from shapely_utils.ShapelyAnnotation.\n\n        Args:\n            annotation: shapely_utils.ShapelyAnnotation\n            category_id: int\n                ID of the object category\n            category_name: str\n                Name of the object category\n            full_shape: List\n                Size of the full image, should be in the form of [height, width]\n            shift_amount: List\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n        \"\"\"\n        return cls(\n            category_id=category_id,\n            segmentation=annotation.to_coco_segmentation(),\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @classmethod\n    def from_imantics_annotation(\n        cls,\n        annotation,\n        shift_amount: list[int] | None = [0, 0],\n        full_shape: list[int] | None = None,\n    ):\n        \"\"\"Creates ObjectAnnotation from imantics.annotation.Annotation.\n\n        Args:\n            annotation: imantics.annotation.Annotation\n            shift_amount: List\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n            full_shape: List\n                Size of the full image, should be in the form of [height, width]\n        \"\"\"\n        return cls(\n            category_id=annotation.category.id,\n            bool_mask=annotation.mask.array,\n            category_name=annotation.category.name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    def to_coco_annotation(self) -&gt; CocoAnnotation:\n        \"\"\"Returns sahi.utils.coco.CocoAnnotation representation of ObjectAnnotation.\"\"\"\n        if self.mask:\n            coco_annotation = CocoAnnotation.from_coco_segmentation(\n                segmentation=self.mask.segmentation,\n                category_id=self.category.id,\n                category_name=self.category.name,\n            )\n        else:\n            coco_annotation = CocoAnnotation.from_coco_bbox(\n                bbox=self.bbox.to_xywh(),\n                category_id=self.category.id,\n                category_name=self.category.name,\n            )\n        return coco_annotation\n\n    def to_coco_prediction(self) -&gt; CocoPrediction:\n        \"\"\"Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.\"\"\"\n        if self.mask:\n            coco_prediction = CocoPrediction.from_coco_segmentation(\n                segmentation=self.mask.segmentation,\n                category_id=self.category.id,\n                category_name=self.category.name,\n                score=1,\n            )\n        else:\n            coco_prediction = CocoPrediction.from_coco_bbox(\n                bbox=self.bbox.to_xywh(),\n                category_id=self.category.id,\n                category_name=self.category.name,\n                score=1,\n            )\n        return coco_prediction\n\n    def to_shapely_annotation(self) -&gt; ShapelyAnnotation:\n        \"\"\"Returns sahi.utils.shapely.ShapelyAnnotation representation of ObjectAnnotation.\"\"\"\n        if self.mask:\n            shapely_annotation = ShapelyAnnotation.from_coco_segmentation(\n                segmentation=self.mask.segmentation,\n            )\n        else:\n            shapely_annotation = ShapelyAnnotation.from_coco_bbox(\n                bbox=self.bbox.to_xywh(),\n            )\n        return shapely_annotation\n\n    def to_imantics_annotation(self):\n        \"\"\"Returns imantics.annotation.Annotation representation of ObjectAnnotation.\"\"\"\n        try:\n            import imantics\n        except ImportError:\n            raise ImportError('Please run \"pip install -U imantics\" to install imantics first for imantics conversion.')\n\n        imantics_category = imantics.Category(id=self.category.id, name=self.category.name)\n        if self.mask is not None:\n            imantics_mask = imantics.Mask.create(self.mask.bool_mask)\n            imantics_annotation = imantics.annotation.Annotation.from_mask(\n                mask=imantics_mask, category=imantics_category\n            )\n        else:\n            imantics_bbox = imantics.BBox.create(self.bbox.to_xyxy())\n            imantics_annotation = imantics.annotation.Annotation.from_bbox(\n                bbox=imantics_bbox, category=imantics_category\n            )\n        return imantics_annotation\n\n    def deepcopy(self):\n        \"\"\"\n        Returns: deepcopy of current ObjectAnnotation instance\n        \"\"\"\n        return copy.deepcopy(self)\n\n    @classmethod\n    def get_empty_mask(cls):\n        return Mask(bool_mask=None)\n\n    def get_shifted_object_annotation(self):\n        if self.mask:\n            shifted_mask = self.mask.get_shifted_mask()\n            return ObjectAnnotation(\n                bbox=self.bbox.get_shifted_box().to_xyxy(),\n                category_id=self.category.id,\n                segmentation=shifted_mask.segmentation,\n                category_name=self.category.name,\n                shift_amount=[0, 0],\n                full_shape=shifted_mask.full_shape,\n            )\n        else:\n            return ObjectAnnotation(\n                bbox=self.bbox.get_shifted_box().to_xyxy(),\n                category_id=self.category.id,\n                bool_mask=None,\n                category_name=self.category.name,\n                shift_amount=[0, 0],\n                full_shape=None,\n            )\n\n    def __repr__(self):\n        return f\"\"\"ObjectAnnotation&lt;\n    bbox: {self.bbox},\n    mask: {self.mask},\n    category: {self.category}&gt;\"\"\"\n</code></pre>"},{"location":"annotation/#sahi.annotation.ObjectAnnotation-functions","title":"Functions","text":""},{"location":"annotation/#sahi.annotation.ObjectAnnotation.__init__","title":"<code>__init__(bbox=None, segmentation=None, category_id=None, category_name=None, shift_amount=[0, 0], full_shape=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>bbox</code> \u00b6 <code>list[int] | None</code> <p>List [minx, miny, maxx, maxy]</p> <code>None</code> <code>segmentation</code> \u00b6 <code>ndarray | None</code> <p>List[List] [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> <code>None</code> <code>category_id</code> \u00b6 <code>int | None</code> <p>int ID of the object category</p> <code>None</code> <code>category_name</code> \u00b6 <code>str | None</code> <p>str Name of the object category</p> <code>None</code> <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> <code>full_shape</code> \u00b6 <code>list[int] | None</code> <p>List Size of the full image after shifting, should be in the form of [height, width]</p> <code>None</code> Source code in <code>sahi/annotation.py</code> <pre><code>def __init__(\n    self,\n    bbox: list[int] | None = None,\n    segmentation: np.ndarray | None = None,\n    category_id: int | None = None,\n    category_name: str | None = None,\n    shift_amount: list[int] | None = [0, 0],\n    full_shape: list[int] | None = None,\n):\n    \"\"\"\n    Args:\n        bbox: List\n            [minx, miny, maxx, maxy]\n        segmentation: List[List]\n            [\n                [x1, y1, x2, y2, x3, y3, ...],\n                [x1, y1, x2, y2, x3, y3, ...],\n                ...\n            ]\n        category_id: int\n            ID of the object category\n        category_name: str\n            Name of the object category\n        shift_amount: List\n            To shift the box and mask predictions from sliced image\n            to full sized image, should be in the form of [shift_x, shift_y]\n        full_shape: List\n            Size of the full image after shifting, should be in\n            the form of [height, width]\n    \"\"\"\n    if not isinstance(category_id, int):\n        raise ValueError(\"category_id must be an integer\")\n    if (bbox is None) and (segmentation is None):\n        raise ValueError(\"you must provide a bbox or segmentation\")\n\n    self.mask: Mask | None = None\n    if segmentation is not None:\n        self.mask = Mask(\n            segmentation=segmentation,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n        bbox_from_segmentation = get_bbox_from_coco_segmentation(segmentation)\n        # https://github.com/obss/sahi/issues/235\n        if bbox_from_segmentation is not None:\n            bbox = bbox_from_segmentation\n        else:\n            raise ValueError(\"Invalid segmentation mask.\")\n\n    # if bbox is a numpy object, convert it to python List[float]\n    if type(bbox).__module__ == \"numpy\":\n        bbox = copy.deepcopy(bbox).tolist()\n\n    # make sure bbox coords lie inside [0, image_size]\n    xmin = max(bbox[0], 0)\n    ymin = max(bbox[1], 0)\n    if full_shape:\n        xmax = min(bbox[2], full_shape[1])\n        ymax = min(bbox[3], full_shape[0])\n    else:\n        xmax = bbox[2]\n        ymax = bbox[3]\n    bbox = [xmin, ymin, xmax, ymax]\n    # set bbox\n    self.bbox = BoundingBox(bbox, shift_amount)\n\n    category_name = category_name if category_name else str(category_id)\n    self.category = Category(\n        id=category_id,\n        name=category_name,\n    )\n\n    self.merged = None\n</code></pre>"},{"location":"annotation/#sahi.annotation.ObjectAnnotation.deepcopy","title":"<code>deepcopy()</code>","text":"<p>Returns: deepcopy of current ObjectAnnotation instance</p> Source code in <code>sahi/annotation.py</code> <pre><code>def deepcopy(self):\n    \"\"\"\n    Returns: deepcopy of current ObjectAnnotation instance\n    \"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"annotation/#sahi.annotation.ObjectAnnotation.from_bool_mask","title":"<code>from_bool_mask(bool_mask, category_id=None, category_name=None, shift_amount=[0, 0], full_shape=None)</code>  <code>classmethod</code>","text":"<p>Creates ObjectAnnotation from bool_mask (2D np.ndarray)</p> <p>Parameters:</p> Name Type Description Default <code>bool_mask</code> \u00b6 <p>np.ndarray with bool elements 2D mask of object, should have a shape of height*width</p> required <code>category_id</code> \u00b6 <code>int | None</code> <p>int ID of the object category</p> <code>None</code> <code>category_name</code> \u00b6 <code>str | None</code> <p>str Name of the object category</p> <code>None</code> <code>full_shape</code> \u00b6 <code>list[int] | None</code> <p>List Size of the full image, should be in the form of [height, width]</p> <code>None</code> <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_bool_mask(\n    cls,\n    bool_mask,\n    category_id: int | None = None,\n    category_name: str | None = None,\n    shift_amount: list[int] | None = [0, 0],\n    full_shape: list[int] | None = None,\n):\n    \"\"\"Creates ObjectAnnotation from bool_mask (2D np.ndarray)\n\n    Args:\n        bool_mask: np.ndarray with bool elements\n            2D mask of object, should have a shape of height*width\n        category_id: int\n            ID of the object category\n        category_name: str\n            Name of the object category\n        full_shape: List\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n    segmentation = get_coco_segmentation_from_bool_mask(bool_mask)\n    return cls(\n        category_id=category_id,\n        segmentation=segmentation,\n        category_name=category_name,\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre>"},{"location":"annotation/#sahi.annotation.ObjectAnnotation.from_coco_annotation_dict","title":"<code>from_coco_annotation_dict(annotation_dict, full_shape, category_name=None, shift_amount=[0, 0])</code>  <code>classmethod</code>","text":"<p>Creates ObjectAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\").</p> <p>Parameters:</p> Name Type Description Default <code>annotation_dict</code> \u00b6 <code>dict</code> <p>dict COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")</p> required <code>category_name</code> \u00b6 <code>str | None</code> <p>str Category name of the annotation</p> <code>None</code> <code>full_shape</code> \u00b6 <code>list[int]</code> <p>List Size of the full image, should be in the form of [height, width]</p> required <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_coco_annotation_dict(\n    cls,\n    annotation_dict: dict,\n    full_shape: list[int],\n    category_name: str | None = None,\n    shift_amount: list[int] | None = [0, 0],\n):\n    \"\"\"Creates ObjectAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\",\n    \"segmentation\", \"category_id\").\n\n    Args:\n        annotation_dict: dict\n            COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")\n        category_name: str\n            Category name of the annotation\n        full_shape: List\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n    if annotation_dict[\"segmentation\"]:\n        return cls.from_coco_segmentation(\n            segmentation=annotation_dict[\"segmentation\"],\n            category_id=annotation_dict[\"category_id\"],\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n    else:\n        return cls.from_coco_bbox(\n            bbox=annotation_dict[\"bbox\"],\n            category_id=annotation_dict[\"category_id\"],\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n</code></pre>"},{"location":"annotation/#sahi.annotation.ObjectAnnotation.from_coco_bbox","title":"<code>from_coco_bbox(bbox, category_id=None, category_name=None, shift_amount=[0, 0], full_shape=None)</code>  <code>classmethod</code>","text":"<p>Creates ObjectAnnotation from coco bbox [minx, miny, width, height]</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> \u00b6 <code>list[int]</code> <p>List [minx, miny, width, height]</p> required <code>category_id</code> \u00b6 <code>int | None</code> <p>int ID of the object category</p> <code>None</code> <code>category_name</code> \u00b6 <code>str | None</code> <p>str Name of the object category</p> <code>None</code> <code>full_shape</code> \u00b6 <code>list[int] | None</code> <p>List Size of the full image, should be in the form of [height, width]</p> <code>None</code> <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_coco_bbox(\n    cls,\n    bbox: list[int],\n    category_id: int | None = None,\n    category_name: str | None = None,\n    shift_amount: list[int] | None = [0, 0],\n    full_shape: list[int] | None = None,\n):\n    \"\"\"Creates ObjectAnnotation from coco bbox [minx, miny, width, height]\n\n    Args:\n        bbox: List\n            [minx, miny, width, height]\n        category_id: int\n            ID of the object category\n        category_name: str\n            Name of the object category\n        full_shape: List\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n    xmin = bbox[0]\n    ymin = bbox[1]\n    xmax = bbox[0] + bbox[2]\n    ymax = bbox[1] + bbox[3]\n    bbox = [xmin, ymin, xmax, ymax]\n    return cls(\n        category_id=category_id,\n        bbox=bbox,\n        category_name=category_name,\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre>"},{"location":"annotation/#sahi.annotation.ObjectAnnotation.from_coco_segmentation","title":"<code>from_coco_segmentation(segmentation, full_shape, category_id=None, category_name=None, shift_amount=[0, 0])</code>  <code>classmethod</code>","text":"<p>Creates ObjectAnnotation from coco segmentation: [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> <p>Parameters:</p> Name Type Description Default <code>segmentation</code> \u00b6 <p>List[List] [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> required <code>category_id</code> \u00b6 <code>int | None</code> <p>int ID of the object category</p> <code>None</code> <code>category_name</code> \u00b6 <code>str | None</code> <p>str Name of the object category</p> <code>None</code> <code>full_shape</code> \u00b6 <code>list[int]</code> <p>List Size of the full image, should be in the form of [height, width]</p> required <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_coco_segmentation(\n    cls,\n    segmentation,\n    full_shape: list[int],\n    category_id: int | None = None,\n    category_name: str | None = None,\n    shift_amount: list[int] | None = [0, 0],\n):\n    \"\"\"\n    Creates ObjectAnnotation from coco segmentation:\n    [\n        [x1, y1, x2, y2, x3, y3, ...],\n        [x1, y1, x2, y2, x3, y3, ...],\n        ...\n    ]\n\n    Args:\n        segmentation: List[List]\n            [\n                [x1, y1, x2, y2, x3, y3, ...],\n                [x1, y1, x2, y2, x3, y3, ...],\n                ...\n            ]\n        category_id: int\n            ID of the object category\n        category_name: str\n            Name of the object category\n        full_shape: List\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n    return cls(\n        category_id=category_id,\n        segmentation=segmentation,\n        category_name=category_name,\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre>"},{"location":"annotation/#sahi.annotation.ObjectAnnotation.from_imantics_annotation","title":"<code>from_imantics_annotation(annotation, shift_amount=[0, 0], full_shape=None)</code>  <code>classmethod</code>","text":"<p>Creates ObjectAnnotation from imantics.annotation.Annotation.</p> <p>Parameters:</p> Name Type Description Default <code>annotation</code> \u00b6 <p>imantics.annotation.Annotation</p> required <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> <code>full_shape</code> \u00b6 <code>list[int] | None</code> <p>List Size of the full image, should be in the form of [height, width]</p> <code>None</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_imantics_annotation(\n    cls,\n    annotation,\n    shift_amount: list[int] | None = [0, 0],\n    full_shape: list[int] | None = None,\n):\n    \"\"\"Creates ObjectAnnotation from imantics.annotation.Annotation.\n\n    Args:\n        annotation: imantics.annotation.Annotation\n        shift_amount: List\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n        full_shape: List\n            Size of the full image, should be in the form of [height, width]\n    \"\"\"\n    return cls(\n        category_id=annotation.category.id,\n        bool_mask=annotation.mask.array,\n        category_name=annotation.category.name,\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre>"},{"location":"annotation/#sahi.annotation.ObjectAnnotation.from_shapely_annotation","title":"<code>from_shapely_annotation(annotation, full_shape, category_id=None, category_name=None, shift_amount=[0, 0])</code>  <code>classmethod</code>","text":"<p>Creates ObjectAnnotation from shapely_utils.ShapelyAnnotation.</p> <p>Parameters:</p> Name Type Description Default <code>annotation</code> \u00b6 <code>ShapelyAnnotation</code> <p>shapely_utils.ShapelyAnnotation</p> required <code>category_id</code> \u00b6 <code>int | None</code> <p>int ID of the object category</p> <code>None</code> <code>category_name</code> \u00b6 <code>str | None</code> <p>str Name of the object category</p> <code>None</code> <code>full_shape</code> \u00b6 <code>list[int]</code> <p>List Size of the full image, should be in the form of [height, width]</p> required <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_shapely_annotation(\n    cls,\n    annotation: ShapelyAnnotation,\n    full_shape: list[int],\n    category_id: int | None = None,\n    category_name: str | None = None,\n    shift_amount: list[int] | None = [0, 0],\n):\n    \"\"\"Creates ObjectAnnotation from shapely_utils.ShapelyAnnotation.\n\n    Args:\n        annotation: shapely_utils.ShapelyAnnotation\n        category_id: int\n            ID of the object category\n        category_name: str\n            Name of the object category\n        full_shape: List\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n    return cls(\n        category_id=category_id,\n        segmentation=annotation.to_coco_segmentation(),\n        category_name=category_name,\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre>"},{"location":"annotation/#sahi.annotation.ObjectAnnotation.to_coco_annotation","title":"<code>to_coco_annotation()</code>","text":"<p>Returns sahi.utils.coco.CocoAnnotation representation of ObjectAnnotation.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_coco_annotation(self) -&gt; CocoAnnotation:\n    \"\"\"Returns sahi.utils.coco.CocoAnnotation representation of ObjectAnnotation.\"\"\"\n    if self.mask:\n        coco_annotation = CocoAnnotation.from_coco_segmentation(\n            segmentation=self.mask.segmentation,\n            category_id=self.category.id,\n            category_name=self.category.name,\n        )\n    else:\n        coco_annotation = CocoAnnotation.from_coco_bbox(\n            bbox=self.bbox.to_xywh(),\n            category_id=self.category.id,\n            category_name=self.category.name,\n        )\n    return coco_annotation\n</code></pre>"},{"location":"annotation/#sahi.annotation.ObjectAnnotation.to_coco_prediction","title":"<code>to_coco_prediction()</code>","text":"<p>Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_coco_prediction(self) -&gt; CocoPrediction:\n    \"\"\"Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.\"\"\"\n    if self.mask:\n        coco_prediction = CocoPrediction.from_coco_segmentation(\n            segmentation=self.mask.segmentation,\n            category_id=self.category.id,\n            category_name=self.category.name,\n            score=1,\n        )\n    else:\n        coco_prediction = CocoPrediction.from_coco_bbox(\n            bbox=self.bbox.to_xywh(),\n            category_id=self.category.id,\n            category_name=self.category.name,\n            score=1,\n        )\n    return coco_prediction\n</code></pre>"},{"location":"annotation/#sahi.annotation.ObjectAnnotation.to_imantics_annotation","title":"<code>to_imantics_annotation()</code>","text":"<p>Returns imantics.annotation.Annotation representation of ObjectAnnotation.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_imantics_annotation(self):\n    \"\"\"Returns imantics.annotation.Annotation representation of ObjectAnnotation.\"\"\"\n    try:\n        import imantics\n    except ImportError:\n        raise ImportError('Please run \"pip install -U imantics\" to install imantics first for imantics conversion.')\n\n    imantics_category = imantics.Category(id=self.category.id, name=self.category.name)\n    if self.mask is not None:\n        imantics_mask = imantics.Mask.create(self.mask.bool_mask)\n        imantics_annotation = imantics.annotation.Annotation.from_mask(\n            mask=imantics_mask, category=imantics_category\n        )\n    else:\n        imantics_bbox = imantics.BBox.create(self.bbox.to_xyxy())\n        imantics_annotation = imantics.annotation.Annotation.from_bbox(\n            bbox=imantics_bbox, category=imantics_category\n        )\n    return imantics_annotation\n</code></pre>"},{"location":"annotation/#sahi.annotation.ObjectAnnotation.to_shapely_annotation","title":"<code>to_shapely_annotation()</code>","text":"<p>Returns sahi.utils.shapely.ShapelyAnnotation representation of ObjectAnnotation.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_shapely_annotation(self) -&gt; ShapelyAnnotation:\n    \"\"\"Returns sahi.utils.shapely.ShapelyAnnotation representation of ObjectAnnotation.\"\"\"\n    if self.mask:\n        shapely_annotation = ShapelyAnnotation.from_coco_segmentation(\n            segmentation=self.mask.segmentation,\n        )\n    else:\n        shapely_annotation = ShapelyAnnotation.from_coco_bbox(\n            bbox=self.bbox.to_xywh(),\n        )\n    return shapely_annotation\n</code></pre>"},{"location":"annotation/#sahi.annotation-functions","title":"Functions","text":""},{"location":"api/","title":"API Reference Index","text":""},{"location":"api/#sahi","title":"<code>sahi</code>","text":""},{"location":"api/#sahi-classes","title":"Classes","text":""},{"location":"api/#sahi.AutoDetectionModel","title":"<code>AutoDetectionModel</code>","text":"Source code in <code>sahi/auto_model.py</code> <pre><code>class AutoDetectionModel:\n    @staticmethod\n    def from_pretrained(\n        model_type: str,\n        model_path: str | None = None,\n        model: Any | None = None,\n        config_path: str | None = None,\n        device: str | None = None,\n        mask_threshold: float = 0.5,\n        confidence_threshold: float = 0.3,\n        category_mapping: dict | None = None,\n        category_remapping: dict | None = None,\n        load_at_init: bool = True,\n        image_size: int | None = None,\n        **kwargs,\n    ) -&gt; DetectionModel:\n        \"\"\"Loads a DetectionModel from given path.\n\n        Args:\n            model_type: str\n                Name of the detection framework (example: \"ultralytics\", \"huggingface\", \"torchvision\")\n            model_path: str\n                Path of the detection model (ex. 'model.pt')\n            model: Any\n                A pre-initialized model instance, if available\n            config_path: str\n                Path of the config file (ex. 'mmdet/configs/cascade_rcnn_r50_fpn_1x.py')\n            device: str\n                Device, \"cpu\" or \"cuda:0\"\n            mask_threshold: float\n                Value to threshold mask pixels, should be between 0 and 1\n            confidence_threshold: float\n                All predictions with score &lt; confidence_threshold will be discarded\n            category_mapping: dict: str to str\n                Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n            category_remapping: dict: str to int\n                Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n            load_at_init: bool\n                If True, automatically loads the model at initialization\n            image_size: int\n                Inference input size.\n\n        Returns:\n            Returns an instance of a DetectionModel\n\n        Raises:\n            ImportError: If given {model_type} framework is not installed\n        \"\"\"\n        if model_type in ULTRALYTICS_MODEL_NAMES:\n            model_type = \"ultralytics\"\n        model_class_name = MODEL_TYPE_TO_MODEL_CLASS_NAME[model_type]\n        DetectionModel = import_model_class(model_type, model_class_name)\n\n        return DetectionModel(\n            model_path=model_path,\n            model=model,\n            config_path=config_path,\n            device=device,\n            mask_threshold=mask_threshold,\n            confidence_threshold=confidence_threshold,\n            category_mapping=category_mapping,\n            category_remapping=category_remapping,\n            load_at_init=load_at_init,\n            image_size=image_size,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#sahi.AutoDetectionModel-functions","title":"Functions","text":""},{"location":"api/#sahi.AutoDetectionModel.from_pretrained","title":"<code>from_pretrained(model_type, model_path=None, model=None, config_path=None, device=None, mask_threshold=0.5, confidence_threshold=0.3, category_mapping=None, category_remapping=None, load_at_init=True, image_size=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Loads a DetectionModel from given path.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> \u00b6 <code>str</code> <p>str Name of the detection framework (example: \"ultralytics\", \"huggingface\", \"torchvision\")</p> required <code>model_path</code> \u00b6 <code>str | None</code> <p>str Path of the detection model (ex. 'model.pt')</p> <code>None</code> <code>model</code> \u00b6 <code>Any | None</code> <p>Any A pre-initialized model instance, if available</p> <code>None</code> <code>config_path</code> \u00b6 <code>str | None</code> <p>str Path of the config file (ex. 'mmdet/configs/cascade_rcnn_r50_fpn_1x.py')</p> <code>None</code> <code>device</code> \u00b6 <code>str | None</code> <p>str Device, \"cpu\" or \"cuda:0\"</p> <code>None</code> <code>mask_threshold</code> \u00b6 <code>float</code> <p>float Value to threshold mask pixels, should be between 0 and 1</p> <code>0.5</code> <code>confidence_threshold</code> \u00b6 <code>float</code> <p>float All predictions with score &lt; confidence_threshold will be discarded</p> <code>0.3</code> <code>category_mapping</code> \u00b6 <code>dict | None</code> <p>dict: str to str Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}</p> <code>None</code> <code>category_remapping</code> \u00b6 <code>dict | None</code> <p>dict: str to int Remap category ids based on category names, after performing inference e.g. {\"car\": 3}</p> <code>None</code> <code>load_at_init</code> \u00b6 <code>bool</code> <p>bool If True, automatically loads the model at initialization</p> <code>True</code> <code>image_size</code> \u00b6 <code>int | None</code> <p>int Inference input size.</p> <code>None</code> <p>Returns:</p> Type Description <code>DetectionModel</code> <p>Returns an instance of a DetectionModel</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If given {model_type} framework is not installed</p> Source code in <code>sahi/auto_model.py</code> <pre><code>@staticmethod\ndef from_pretrained(\n    model_type: str,\n    model_path: str | None = None,\n    model: Any | None = None,\n    config_path: str | None = None,\n    device: str | None = None,\n    mask_threshold: float = 0.5,\n    confidence_threshold: float = 0.3,\n    category_mapping: dict | None = None,\n    category_remapping: dict | None = None,\n    load_at_init: bool = True,\n    image_size: int | None = None,\n    **kwargs,\n) -&gt; DetectionModel:\n    \"\"\"Loads a DetectionModel from given path.\n\n    Args:\n        model_type: str\n            Name of the detection framework (example: \"ultralytics\", \"huggingface\", \"torchvision\")\n        model_path: str\n            Path of the detection model (ex. 'model.pt')\n        model: Any\n            A pre-initialized model instance, if available\n        config_path: str\n            Path of the config file (ex. 'mmdet/configs/cascade_rcnn_r50_fpn_1x.py')\n        device: str\n            Device, \"cpu\" or \"cuda:0\"\n        mask_threshold: float\n            Value to threshold mask pixels, should be between 0 and 1\n        confidence_threshold: float\n            All predictions with score &lt; confidence_threshold will be discarded\n        category_mapping: dict: str to str\n            Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n        category_remapping: dict: str to int\n            Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n        load_at_init: bool\n            If True, automatically loads the model at initialization\n        image_size: int\n            Inference input size.\n\n    Returns:\n        Returns an instance of a DetectionModel\n\n    Raises:\n        ImportError: If given {model_type} framework is not installed\n    \"\"\"\n    if model_type in ULTRALYTICS_MODEL_NAMES:\n        model_type = \"ultralytics\"\n    model_class_name = MODEL_TYPE_TO_MODEL_CLASS_NAME[model_type]\n    DetectionModel = import_model_class(model_type, model_class_name)\n\n    return DetectionModel(\n        model_path=model_path,\n        model=model,\n        config_path=config_path,\n        device=device,\n        mask_threshold=mask_threshold,\n        confidence_threshold=confidence_threshold,\n        category_mapping=category_mapping,\n        category_remapping=category_remapping,\n        load_at_init=load_at_init,\n        image_size=image_size,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#sahi.BoundingBox","title":"<code>BoundingBox</code>  <code>dataclass</code>","text":"<p>BoundingBox represents a rectangular region in 2D space, typically used for object detection annotations.</p> <p>Attributes:</p> Name Type Description <code>box</code> <code>Tuple[float, float, float, float]</code> <p>The bounding box coordinates in the format (minx, miny, maxx, maxy). - minx (float): Minimum x-coordinate (left). - miny (float): Minimum y-coordinate (top). - maxx (float): Maximum x-coordinate (right). - maxy (float): Maximum y-coordinate (bottom).</p> <code>shift_amount</code> <code>Tuple[int, int]</code> <p>The amount to shift the bounding box in the x and y directions. Defaults to (0, 0).</p> <p>BoundingBox Usage Example</p> <pre><code>bbox = BoundingBox((10.0, 20.0, 50.0, 80.0))\narea = bbox.area\nexpanded_bbox = bbox.get_expanded_box(ratio=0.2)\nshifted_bbox = bbox.get_shifted_box()\ncoco_format = bbox.to_coco_bbox()\n</code></pre> Source code in <code>sahi/annotation.py</code> <pre><code>@dataclass(frozen=True)\nclass BoundingBox:\n    \"\"\"BoundingBox represents a rectangular region in 2D space, typically used for object detection annotations.\n\n    Attributes:\n        box (Tuple[float, float, float, float]): The bounding box coordinates in the format (minx, miny, maxx, maxy).\n            - minx (float): Minimum x-coordinate (left).\n            - miny (float): Minimum y-coordinate (top).\n            - maxx (float): Maximum x-coordinate (right).\n            - maxy (float): Maximum y-coordinate (bottom).\n        shift_amount (Tuple[int, int], optional): The amount to shift the bounding box in the x and y directions.\n            Defaults to (0, 0).\n\n    !!! example \"BoundingBox Usage Example\"\n        ```python\n        bbox = BoundingBox((10.0, 20.0, 50.0, 80.0))\n        area = bbox.area\n        expanded_bbox = bbox.get_expanded_box(ratio=0.2)\n        shifted_bbox = bbox.get_shifted_box()\n        coco_format = bbox.to_coco_bbox()\n        ```\n    \"\"\"\n\n    box: tuple[float, float, float, float] | list[float]\n    shift_amount: tuple[int, int] = (0, 0)\n\n    def __post_init__(self):\n        if len(self.box) != 4 or any(coord &lt; 0 for coord in self.box):\n            raise ValueError(\"box must be 4 non-negative floats: [minx, miny, maxx, maxy]\")\n        if len(self.shift_amount) != 2:\n            raise ValueError(\"shift_amount must be 2 integers: [shift_x, shift_y]\")\n\n    @property\n    def minx(self):\n        return self.box[0]\n\n    @property\n    def miny(self):\n        return self.box[1]\n\n    @property\n    def maxx(self):\n        return self.box[2]\n\n    @property\n    def maxy(self):\n        return self.box[3]\n\n    @property\n    def shift_x(self):\n        return self.shift_amount[0]\n\n    @property\n    def shift_y(self):\n        return self.shift_amount[1]\n\n    @property\n    def area(self):\n        return (self.maxx - self.minx) * (self.maxy - self.miny)\n\n    def get_expanded_box(self, ratio: float = 0.1, max_x: int | None = None, max_y: int | None = None):\n        \"\"\"Returns an expanded bounding box by increasing its size by a given ratio. The expansion is applied equally in\n        all directions. Optionally, the expanded box can be clipped to maximum x and y boundaries.\n\n        Args:\n            ratio (float, optional): The proportion by which to expand the box size.\n                Default is 0.1 (10%).\n            max_x (int, optional): The maximum allowed x-coordinate for the expanded box.\n                If None, no maximum is applied.\n            max_y (int, optional): The maximum allowed y-coordinate for the expanded box.\n                If None, no maximum is applied.\n\n        Returns:\n            BoundingBox: A new BoundingBox instance representing the expanded box.\n        \"\"\"\n\n        w = self.maxx - self.minx\n        h = self.maxy - self.miny\n        y_mar = int(w * ratio)\n        x_mar = int(h * ratio)\n        maxx = min(max_x, self.maxx + x_mar) if max_x else self.maxx + x_mar\n        minx = max(0, self.minx - x_mar)\n        maxy = min(max_y, self.maxy + y_mar) if max_y else self.maxy + y_mar\n        miny = max(0, self.miny - y_mar)\n        box: list[float] = [minx, miny, maxx, maxy]\n        return BoundingBox(box)\n\n    def to_xywh(self):\n        \"\"\"Returns [xmin, ymin, width, height]\n\n        Returns:\n            List[float]: A list containing the bounding box in the format [xmin, ymin, width, height].\n        \"\"\"\n\n        return [self.minx, self.miny, self.maxx - self.minx, self.maxy - self.miny]\n\n    def to_coco_bbox(self):\n        \"\"\"\n        Returns the bounding box in COCO format: [xmin, ymin, width, height]\n\n        Returns:\n            List[float]: A list containing the bounding box in COCO format.\n        \"\"\"\n        return self.to_xywh()\n\n    def to_xyxy(self):\n        \"\"\"\n        Returns: [xmin, ymin, xmax, ymax]\n\n        Returns:\n            List[float]: A list containing the bounding box in the format [xmin, ymin, xmax, ymax].\n        \"\"\"\n        return [self.minx, self.miny, self.maxx, self.maxy]\n\n    def to_voc_bbox(self):\n        \"\"\"\n        Returns the bounding box in VOC format: [xmin, ymin, xmax, ymax]\n\n        Returns:\n            List[float]: A list containing the bounding box in VOC format.\n        \"\"\"\n        return self.to_xyxy()\n\n    def get_shifted_box(self):\n        \"\"\"Returns shifted BoundingBox.\n\n        Returns:\n            BoundingBox: A new BoundingBox instance representing the shifted box.\n        \"\"\"\n        box = [\n            self.minx + self.shift_x,\n            self.miny + self.shift_y,\n            self.maxx + self.shift_x,\n            self.maxy + self.shift_y,\n        ]\n        return BoundingBox(box)\n\n    def __repr__(self):\n        return (\n            f\"BoundingBox: &lt;{(self.minx, self.miny, self.maxx, self.maxy)}, \"\n            f\"w: {self.maxx - self.minx}, h: {self.maxy - self.miny}&gt;\"\n        )\n</code></pre>"},{"location":"api/#sahi.BoundingBox-functions","title":"Functions","text":""},{"location":"api/#sahi.BoundingBox.get_expanded_box","title":"<code>get_expanded_box(ratio=0.1, max_x=None, max_y=None)</code>","text":"<p>Returns an expanded bounding box by increasing its size by a given ratio. The expansion is applied equally in all directions. Optionally, the expanded box can be clipped to maximum x and y boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> \u00b6 <code>float</code> <p>The proportion by which to expand the box size. Default is 0.1 (10%).</p> <code>0.1</code> <code>max_x</code> \u00b6 <code>int</code> <p>The maximum allowed x-coordinate for the expanded box. If None, no maximum is applied.</p> <code>None</code> <code>max_y</code> \u00b6 <code>int</code> <p>The maximum allowed y-coordinate for the expanded box. If None, no maximum is applied.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>BoundingBox</code> <p>A new BoundingBox instance representing the expanded box.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def get_expanded_box(self, ratio: float = 0.1, max_x: int | None = None, max_y: int | None = None):\n    \"\"\"Returns an expanded bounding box by increasing its size by a given ratio. The expansion is applied equally in\n    all directions. Optionally, the expanded box can be clipped to maximum x and y boundaries.\n\n    Args:\n        ratio (float, optional): The proportion by which to expand the box size.\n            Default is 0.1 (10%).\n        max_x (int, optional): The maximum allowed x-coordinate for the expanded box.\n            If None, no maximum is applied.\n        max_y (int, optional): The maximum allowed y-coordinate for the expanded box.\n            If None, no maximum is applied.\n\n    Returns:\n        BoundingBox: A new BoundingBox instance representing the expanded box.\n    \"\"\"\n\n    w = self.maxx - self.minx\n    h = self.maxy - self.miny\n    y_mar = int(w * ratio)\n    x_mar = int(h * ratio)\n    maxx = min(max_x, self.maxx + x_mar) if max_x else self.maxx + x_mar\n    minx = max(0, self.minx - x_mar)\n    maxy = min(max_y, self.maxy + y_mar) if max_y else self.maxy + y_mar\n    miny = max(0, self.miny - y_mar)\n    box: list[float] = [minx, miny, maxx, maxy]\n    return BoundingBox(box)\n</code></pre>"},{"location":"api/#sahi.BoundingBox.get_shifted_box","title":"<code>get_shifted_box()</code>","text":"<p>Returns shifted BoundingBox.</p> <p>Returns:</p> Name Type Description <code>BoundingBox</code> <p>A new BoundingBox instance representing the shifted box.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def get_shifted_box(self):\n    \"\"\"Returns shifted BoundingBox.\n\n    Returns:\n        BoundingBox: A new BoundingBox instance representing the shifted box.\n    \"\"\"\n    box = [\n        self.minx + self.shift_x,\n        self.miny + self.shift_y,\n        self.maxx + self.shift_x,\n        self.maxy + self.shift_y,\n    ]\n    return BoundingBox(box)\n</code></pre>"},{"location":"api/#sahi.BoundingBox.to_coco_bbox","title":"<code>to_coco_bbox()</code>","text":"<p>Returns the bounding box in COCO format: [xmin, ymin, width, height]</p> <p>Returns:</p> Type Description <p>List[float]: A list containing the bounding box in COCO format.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_coco_bbox(self):\n    \"\"\"\n    Returns the bounding box in COCO format: [xmin, ymin, width, height]\n\n    Returns:\n        List[float]: A list containing the bounding box in COCO format.\n    \"\"\"\n    return self.to_xywh()\n</code></pre>"},{"location":"api/#sahi.BoundingBox.to_voc_bbox","title":"<code>to_voc_bbox()</code>","text":"<p>Returns the bounding box in VOC format: [xmin, ymin, xmax, ymax]</p> <p>Returns:</p> Type Description <p>List[float]: A list containing the bounding box in VOC format.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_voc_bbox(self):\n    \"\"\"\n    Returns the bounding box in VOC format: [xmin, ymin, xmax, ymax]\n\n    Returns:\n        List[float]: A list containing the bounding box in VOC format.\n    \"\"\"\n    return self.to_xyxy()\n</code></pre>"},{"location":"api/#sahi.BoundingBox.to_xywh","title":"<code>to_xywh()</code>","text":"<p>Returns [xmin, ymin, width, height]</p> <p>Returns:</p> Type Description <p>List[float]: A list containing the bounding box in the format [xmin, ymin, width, height].</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_xywh(self):\n    \"\"\"Returns [xmin, ymin, width, height]\n\n    Returns:\n        List[float]: A list containing the bounding box in the format [xmin, ymin, width, height].\n    \"\"\"\n\n    return [self.minx, self.miny, self.maxx - self.minx, self.maxy - self.miny]\n</code></pre>"},{"location":"api/#sahi.BoundingBox.to_xyxy","title":"<code>to_xyxy()</code>","text":"<p>Returns: [xmin, ymin, xmax, ymax]</p> <p>Returns:</p> Type Description <p>List[float]: A list containing the bounding box in the format [xmin, ymin, xmax, ymax].</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_xyxy(self):\n    \"\"\"\n    Returns: [xmin, ymin, xmax, ymax]\n\n    Returns:\n        List[float]: A list containing the bounding box in the format [xmin, ymin, xmax, ymax].\n    \"\"\"\n    return [self.minx, self.miny, self.maxx, self.maxy]\n</code></pre>"},{"location":"api/#sahi.Category","title":"<code>Category</code>  <code>dataclass</code>","text":"<p>Category of the annotation.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Unique identifier for the category.</p> <code>name</code> <code>str</code> <p>Name of the category.</p> Source code in <code>sahi/annotation.py</code> <pre><code>@dataclass(frozen=True)\nclass Category:\n    \"\"\"Category of the annotation.\n\n    Attributes:\n        id (int): Unique identifier for the category.\n        name (str): Name of the category.\n    \"\"\"\n\n    id: int\n    name: str\n\n    def __post_init__(self):\n        if not isinstance(self.id, int):\n            raise TypeError(\"id should be integer\")\n        if not isinstance(self.name, str):\n            raise TypeError(\"name should be string\")\n\n    def __repr__(self):\n        return f\"Category: &lt;id: {self.id}, name: {self.name}&gt;\"\n</code></pre>"},{"location":"api/#sahi.DetectionModel","title":"<code>DetectionModel</code>","text":"Source code in <code>sahi/models/base.py</code> <pre><code>class DetectionModel:\n    required_packages: list[str] | None = None\n\n    def __init__(\n        self,\n        model_path: str | None = None,\n        model: Any | None = None,\n        config_path: str | None = None,\n        device: str | None = None,\n        mask_threshold: float = 0.5,\n        confidence_threshold: float = 0.3,\n        category_mapping: dict | None = None,\n        category_remapping: dict | None = None,\n        load_at_init: bool = True,\n        image_size: int | None = None,\n    ):\n        \"\"\"Init object detection/instance segmentation model.\n\n        Args:\n            model_path: str\n                Path for the instance segmentation model weight\n            config_path: str\n                Path for the mmdetection instance segmentation model config file\n            device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n            mask_threshold: float\n                Value to threshold mask pixels, should be between 0 and 1\n            confidence_threshold: float\n                All predictions with score &lt; confidence_threshold will be discarded\n            category_mapping: dict: str to str\n                Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n            category_remapping: dict: str to int\n                Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n            load_at_init: bool\n                If True, automatically loads the model at initialization\n            image_size: int\n                Inference input size.\n        \"\"\"\n\n        self.model_path = model_path\n        self.config_path = config_path\n        self.model = None\n        self.mask_threshold = mask_threshold\n        self.confidence_threshold = confidence_threshold\n        self.category_mapping = category_mapping\n        self.category_remapping = category_remapping\n        self.image_size = image_size\n        self._original_predictions = None\n        self._object_prediction_list_per_image = None\n        self.set_device(device)\n\n        # automatically ensure dependencies\n        self.check_dependencies()\n\n        # automatically load model if load_at_init is True\n        if load_at_init:\n            if model:\n                self.set_model(model)\n            else:\n                self.load_model()\n\n    def check_dependencies(self, packages: list[str] | None = None) -&gt; None:\n        \"\"\"Ensures required dependencies are installed.\n\n        If 'packages' is None, uses self.required_packages. Subclasses may still call with a custom list for dynamic\n        needs.\n        \"\"\"\n        pkgs = packages if packages is not None else getattr(self, \"required_packages\", [])\n        if pkgs:\n            check_requirements(pkgs)\n\n    def load_model(self):\n        \"\"\"This function should be implemented in a way that detection model should be initialized and set to\n        self.model.\n\n        (self.model_path, self.config_path, and self.device should be utilized)\n        \"\"\"\n        raise NotImplementedError()\n\n    def set_model(self, model: Any, **kwargs):\n        \"\"\"\n        This function should be implemented to instantiate a DetectionModel out of an already loaded model\n        Args:\n            model: Any\n                Loaded model\n        \"\"\"\n        raise NotImplementedError()\n\n    def set_device(self, device: str | None = None):\n        \"\"\"Sets the device pytorch should use for the model.\n\n        Args:\n            device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n        \"\"\"\n\n        self.device = select_device(device)\n\n    def unload_model(self):\n        \"\"\"Unloads the model from CPU/GPU.\"\"\"\n        self.model = None\n        empty_cuda_cache()\n\n    def perform_inference(self, image: np.ndarray):\n        \"\"\"This function should be implemented in a way that prediction should be performed using self.model and the\n        prediction result should be set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"This function should be implemented in a way that self._original_predictions should be converted to a list of\n        prediction.ObjectPrediction and set to self._object_prediction_list.\n\n        self.mask_threshold can also be utilized.\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        raise NotImplementedError()\n\n    def _apply_category_remapping(self):\n        \"\"\"Applies category remapping based on mapping given in self.category_remapping.\"\"\"\n        # confirm self.category_remapping is not None\n        if self.category_remapping is None:\n            raise ValueError(\"self.category_remapping cannot be None\")\n        # remap categories\n        if not isinstance(self._object_prediction_list_per_image, list):\n            logger.error(\n                f\"Unknown type for self._object_prediction_list_per_image: \"\n                f\"{type(self._object_prediction_list_per_image)}\"\n            )\n            return\n        for object_prediction_list in self._object_prediction_list_per_image:  # type: ignore\n            for object_prediction in object_prediction_list:\n                old_category_id_str = str(object_prediction.category.id)\n                new_category_id_int = self.category_remapping[old_category_id_str]\n                object_prediction.category = Category(id=new_category_id_int, name=object_prediction.category.name)\n\n    def convert_original_predictions(\n        self,\n        shift_amount: list[list[int]] | None = [[0, 0]],\n        full_shape: list[list[int]] | None = None,\n    ):\n        \"\"\"Converts original predictions of the detection model to a list of prediction.ObjectPrediction object.\n\n        Should be called after perform_inference().\n        Args:\n            shift_amount: list\n                To shift the box and mask predictions from sliced image to full sized image,\n                    should be in the form of [shift_x, shift_y]\n            full_shape: list\n                Size of the full image after shifting, should be in the form of [height, width]\n        \"\"\"\n        self._create_object_prediction_list_from_original_predictions(\n            shift_amount_list=shift_amount,\n            full_shape_list=full_shape,\n        )\n        if self.category_remapping:\n            self._apply_category_remapping()\n\n    @property\n    def object_prediction_list(self) -&gt; list[list[ObjectPrediction]]:\n        if self._object_prediction_list_per_image is None:\n            return []\n        if len(self._object_prediction_list_per_image) == 0:\n            return []\n        return self._object_prediction_list_per_image[0]\n\n    @property\n    def object_prediction_list_per_image(self) -&gt; list[list[ObjectPrediction]]:\n        return self._object_prediction_list_per_image or []\n\n    @property\n    def original_predictions(self):\n        return self._original_predictions\n</code></pre>"},{"location":"api/#sahi.DetectionModel-functions","title":"Functions","text":""},{"location":"api/#sahi.DetectionModel.__init__","title":"<code>__init__(model_path=None, model=None, config_path=None, device=None, mask_threshold=0.5, confidence_threshold=0.3, category_mapping=None, category_remapping=None, load_at_init=True, image_size=None)</code>","text":"<p>Init object detection/instance segmentation model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> \u00b6 <code>str | None</code> <p>str Path for the instance segmentation model weight</p> <code>None</code> <code>config_path</code> \u00b6 <code>str | None</code> <p>str Path for the mmdetection instance segmentation model config file</p> <code>None</code> <code>device</code> \u00b6 <code>str | None</code> <p>Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.</p> <code>None</code> <code>mask_threshold</code> \u00b6 <code>float</code> <p>float Value to threshold mask pixels, should be between 0 and 1</p> <code>0.5</code> <code>confidence_threshold</code> \u00b6 <code>float</code> <p>float All predictions with score &lt; confidence_threshold will be discarded</p> <code>0.3</code> <code>category_mapping</code> \u00b6 <code>dict | None</code> <p>dict: str to str Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}</p> <code>None</code> <code>category_remapping</code> \u00b6 <code>dict | None</code> <p>dict: str to int Remap category ids based on category names, after performing inference e.g. {\"car\": 3}</p> <code>None</code> <code>load_at_init</code> \u00b6 <code>bool</code> <p>bool If True, automatically loads the model at initialization</p> <code>True</code> <code>image_size</code> \u00b6 <code>int | None</code> <p>int Inference input size.</p> <code>None</code> Source code in <code>sahi/models/base.py</code> <pre><code>def __init__(\n    self,\n    model_path: str | None = None,\n    model: Any | None = None,\n    config_path: str | None = None,\n    device: str | None = None,\n    mask_threshold: float = 0.5,\n    confidence_threshold: float = 0.3,\n    category_mapping: dict | None = None,\n    category_remapping: dict | None = None,\n    load_at_init: bool = True,\n    image_size: int | None = None,\n):\n    \"\"\"Init object detection/instance segmentation model.\n\n    Args:\n        model_path: str\n            Path for the instance segmentation model weight\n        config_path: str\n            Path for the mmdetection instance segmentation model config file\n        device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n        mask_threshold: float\n            Value to threshold mask pixels, should be between 0 and 1\n        confidence_threshold: float\n            All predictions with score &lt; confidence_threshold will be discarded\n        category_mapping: dict: str to str\n            Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n        category_remapping: dict: str to int\n            Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n        load_at_init: bool\n            If True, automatically loads the model at initialization\n        image_size: int\n            Inference input size.\n    \"\"\"\n\n    self.model_path = model_path\n    self.config_path = config_path\n    self.model = None\n    self.mask_threshold = mask_threshold\n    self.confidence_threshold = confidence_threshold\n    self.category_mapping = category_mapping\n    self.category_remapping = category_remapping\n    self.image_size = image_size\n    self._original_predictions = None\n    self._object_prediction_list_per_image = None\n    self.set_device(device)\n\n    # automatically ensure dependencies\n    self.check_dependencies()\n\n    # automatically load model if load_at_init is True\n    if load_at_init:\n        if model:\n            self.set_model(model)\n        else:\n            self.load_model()\n</code></pre>"},{"location":"api/#sahi.DetectionModel.check_dependencies","title":"<code>check_dependencies(packages=None)</code>","text":"<p>Ensures required dependencies are installed.</p> <p>If 'packages' is None, uses self.required_packages. Subclasses may still call with a custom list for dynamic needs.</p> Source code in <code>sahi/models/base.py</code> <pre><code>def check_dependencies(self, packages: list[str] | None = None) -&gt; None:\n    \"\"\"Ensures required dependencies are installed.\n\n    If 'packages' is None, uses self.required_packages. Subclasses may still call with a custom list for dynamic\n    needs.\n    \"\"\"\n    pkgs = packages if packages is not None else getattr(self, \"required_packages\", [])\n    if pkgs:\n        check_requirements(pkgs)\n</code></pre>"},{"location":"api/#sahi.DetectionModel.convert_original_predictions","title":"<code>convert_original_predictions(shift_amount=[[0, 0]], full_shape=None)</code>","text":"<p>Converts original predictions of the detection model to a list of prediction.ObjectPrediction object.</p> <p>Should be called after perform_inference(). Args:     shift_amount: list         To shift the box and mask predictions from sliced image to full sized image,             should be in the form of [shift_x, shift_y]     full_shape: list         Size of the full image after shifting, should be in the form of [height, width]</p> Source code in <code>sahi/models/base.py</code> <pre><code>def convert_original_predictions(\n    self,\n    shift_amount: list[list[int]] | None = [[0, 0]],\n    full_shape: list[list[int]] | None = None,\n):\n    \"\"\"Converts original predictions of the detection model to a list of prediction.ObjectPrediction object.\n\n    Should be called after perform_inference().\n    Args:\n        shift_amount: list\n            To shift the box and mask predictions from sliced image to full sized image,\n                should be in the form of [shift_x, shift_y]\n        full_shape: list\n            Size of the full image after shifting, should be in the form of [height, width]\n    \"\"\"\n    self._create_object_prediction_list_from_original_predictions(\n        shift_amount_list=shift_amount,\n        full_shape_list=full_shape,\n    )\n    if self.category_remapping:\n        self._apply_category_remapping()\n</code></pre>"},{"location":"api/#sahi.DetectionModel.load_model","title":"<code>load_model()</code>","text":"<p>This function should be implemented in a way that detection model should be initialized and set to self.model.</p> <p>(self.model_path, self.config_path, and self.device should be utilized)</p> Source code in <code>sahi/models/base.py</code> <pre><code>def load_model(self):\n    \"\"\"This function should be implemented in a way that detection model should be initialized and set to\n    self.model.\n\n    (self.model_path, self.config_path, and self.device should be utilized)\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/#sahi.DetectionModel.perform_inference","title":"<code>perform_inference(image)</code>","text":"<p>This function should be implemented in a way that prediction should be performed using self.model and the prediction result should be set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted.</p> required Source code in <code>sahi/models/base.py</code> <pre><code>def perform_inference(self, image: np.ndarray):\n    \"\"\"This function should be implemented in a way that prediction should be performed using self.model and the\n    prediction result should be set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/#sahi.DetectionModel.set_device","title":"<code>set_device(device=None)</code>","text":"<p>Sets the device pytorch should use for the model.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> \u00b6 <code>str | None</code> <p>Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.</p> <code>None</code> Source code in <code>sahi/models/base.py</code> <pre><code>def set_device(self, device: str | None = None):\n    \"\"\"Sets the device pytorch should use for the model.\n\n    Args:\n        device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n    \"\"\"\n\n    self.device = select_device(device)\n</code></pre>"},{"location":"api/#sahi.DetectionModel.set_model","title":"<code>set_model(model, **kwargs)</code>","text":"<p>This function should be implemented to instantiate a DetectionModel out of an already loaded model Args:     model: Any         Loaded model</p> Source code in <code>sahi/models/base.py</code> <pre><code>def set_model(self, model: Any, **kwargs):\n    \"\"\"\n    This function should be implemented to instantiate a DetectionModel out of an already loaded model\n    Args:\n        model: Any\n            Loaded model\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/#sahi.DetectionModel.unload_model","title":"<code>unload_model()</code>","text":"<p>Unloads the model from CPU/GPU.</p> Source code in <code>sahi/models/base.py</code> <pre><code>def unload_model(self):\n    \"\"\"Unloads the model from CPU/GPU.\"\"\"\n    self.model = None\n    empty_cuda_cache()\n</code></pre>"},{"location":"api/#sahi.Mask","title":"<code>Mask</code>","text":"<p>Init Mask from coco segmentation representation.</p> <p>Parameters:</p> Name Type Description Default <p>List[List] [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> required <code>list[int]</code> <p>List[int] Size of the full image, should be in the form of [height, width]</p> required <code>list</code> <p>List[int] To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>class Mask:\n    \"\"\"Init Mask from coco segmentation representation.\n\n    Args:\n        segmentation : List[List]\n            [\n                [x1, y1, x2, y2, x3, y3, ...],\n                [x1, y1, x2, y2, x3, y3, ...],\n                ...\n            ]\n        full_shape: List[int]\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List[int]\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n\n    def __init__(\n        self,\n        segmentation: list[list[float]],\n        full_shape: list[int],\n        shift_amount: list = [0, 0],\n    ):\n        if full_shape is None:\n            raise ValueError(\"full_shape must be provided\")  # pyright: ignore[reportUnreachable]\n\n        self.shift_x = shift_amount[0]\n        self.shift_y = shift_amount[1]\n        self.full_shape_height = full_shape[0]\n        self.full_shape_width = full_shape[1]\n        self.segmentation = segmentation\n\n    @classmethod\n    def from_float_mask(\n        cls,\n        mask: np.ndarray,\n        full_shape: list[int],\n        mask_threshold: float = 0.5,\n        shift_amount: list = [0, 0],\n    ):\n        \"\"\"\n        Args:\n            mask: np.ndarray of np.float elements\n                Mask values between 0 and 1 (should have a shape of height*width)\n            mask_threshold: float\n                Value to threshold mask pixels between 0 and 1\n            shift_amount: List\n                To shift the box and mask predictions from sliced image\n                to full sized image, should be in the form of [shift_x, shift_y]\n            full_shape: List[int]\n                Size of the full image after shifting, should be in the form of [height, width]\n        \"\"\"\n        bool_mask = mask &gt; mask_threshold\n        return cls(\n            segmentation=get_coco_segmentation_from_bool_mask(bool_mask),\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @classmethod\n    def from_bool_mask(\n        cls,\n        bool_mask: np.ndarray,\n        full_shape: list[int],\n        shift_amount: list = [0, 0],\n    ):\n        \"\"\"\n        Args:\n            bool_mask: np.ndarray with bool elements\n                2D mask of object, should have a shape of height*width\n            full_shape: List[int]\n                Size of the full image, should be in the form of [height, width]\n            shift_amount: List[int]\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n        \"\"\"\n        return cls(\n            segmentation=get_coco_segmentation_from_bool_mask(bool_mask),\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @property\n    def bool_mask(self) -&gt; np.ndarray:\n        return get_bool_mask_from_coco_segmentation(\n            self.segmentation, width=self.full_shape[1], height=self.full_shape[0]\n        )\n\n    @property\n    def shape(self) -&gt; list[int]:\n        \"\"\"Returns mask shape as [height, width]\"\"\"\n        return [self.bool_mask.shape[0], self.bool_mask.shape[1]]\n\n    @property\n    def full_shape(self) -&gt; list[int]:\n        \"\"\"Returns full mask shape after shifting as [height, width]\"\"\"\n        return [self.full_shape_height, self.full_shape_width]\n\n    @property\n    def shift_amount(self):\n        \"\"\"Returns the shift amount of the mask slice as [shift_x, shift_y]\"\"\"\n        return [self.shift_x, self.shift_y]\n\n    def get_shifted_mask(self) -&gt; Mask:\n        # Confirm full_shape is specified\n        if (self.full_shape_height is None) or (self.full_shape_width is None):\n            raise ValueError(\"full_shape is None\")\n        shifted_segmentation = []\n        for s in self.segmentation:\n            xs = [min(self.shift_x + s[i], self.full_shape_width) for i in range(0, len(s) - 1, 2)]\n            ys = [min(self.shift_y + s[i], self.full_shape_height) for i in range(1, len(s), 2)]\n            shifted_segmentation.append([j for i in zip(xs, ys) for j in i])\n        return Mask(\n            segmentation=shifted_segmentation,\n            shift_amount=[0, 0],\n            full_shape=self.full_shape,\n        )\n</code></pre>"},{"location":"api/#sahi.Mask(segmentation )","title":"<code>segmentation </code>","text":""},{"location":"api/#sahi.Mask(full_shape)","title":"<code>full_shape</code>","text":""},{"location":"api/#sahi.Mask(shift_amount)","title":"<code>shift_amount</code>","text":""},{"location":"api/#sahi.Mask-attributes","title":"Attributes","text":""},{"location":"api/#sahi.Mask.full_shape","title":"<code>full_shape</code>  <code>property</code>","text":"<p>Returns full mask shape after shifting as [height, width]</p>"},{"location":"api/#sahi.Mask.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Returns mask shape as [height, width]</p>"},{"location":"api/#sahi.Mask.shift_amount","title":"<code>shift_amount</code>  <code>property</code>","text":"<p>Returns the shift amount of the mask slice as [shift_x, shift_y]</p>"},{"location":"api/#sahi.Mask-functions","title":"Functions","text":""},{"location":"api/#sahi.Mask.from_bool_mask","title":"<code>from_bool_mask(bool_mask, full_shape, shift_amount=[0, 0])</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>bool_mask</code> \u00b6 <code>ndarray</code> <p>np.ndarray with bool elements 2D mask of object, should have a shape of height*width</p> required <code>full_shape</code> \u00b6 <code>list[int]</code> <p>List[int] Size of the full image, should be in the form of [height, width]</p> required <code>shift_amount</code> \u00b6 <code>list</code> <p>List[int] To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_bool_mask(\n    cls,\n    bool_mask: np.ndarray,\n    full_shape: list[int],\n    shift_amount: list = [0, 0],\n):\n    \"\"\"\n    Args:\n        bool_mask: np.ndarray with bool elements\n            2D mask of object, should have a shape of height*width\n        full_shape: List[int]\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List[int]\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n    return cls(\n        segmentation=get_coco_segmentation_from_bool_mask(bool_mask),\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre>"},{"location":"api/#sahi.Mask.from_float_mask","title":"<code>from_float_mask(mask, full_shape, mask_threshold=0.5, shift_amount=[0, 0])</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>mask</code> \u00b6 <code>ndarray</code> <p>np.ndarray of np.float elements Mask values between 0 and 1 (should have a shape of height*width)</p> required <code>mask_threshold</code> \u00b6 <code>float</code> <p>float Value to threshold mask pixels between 0 and 1</p> <code>0.5</code> <code>shift_amount</code> \u00b6 <code>list</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> <code>full_shape</code> \u00b6 <code>list[int]</code> <p>List[int] Size of the full image after shifting, should be in the form of [height, width]</p> required Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_float_mask(\n    cls,\n    mask: np.ndarray,\n    full_shape: list[int],\n    mask_threshold: float = 0.5,\n    shift_amount: list = [0, 0],\n):\n    \"\"\"\n    Args:\n        mask: np.ndarray of np.float elements\n            Mask values between 0 and 1 (should have a shape of height*width)\n        mask_threshold: float\n            Value to threshold mask pixels between 0 and 1\n        shift_amount: List\n            To shift the box and mask predictions from sliced image\n            to full sized image, should be in the form of [shift_x, shift_y]\n        full_shape: List[int]\n            Size of the full image after shifting, should be in the form of [height, width]\n    \"\"\"\n    bool_mask = mask &gt; mask_threshold\n    return cls(\n        segmentation=get_coco_segmentation_from_bool_mask(bool_mask),\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre>"},{"location":"api/#sahi.ObjectPrediction","title":"<code>ObjectPrediction</code>","text":"<p>               Bases: <code>ObjectAnnotation</code></p> <p>Class for handling detection model predictions.</p> Source code in <code>sahi/prediction.py</code> <pre><code>class ObjectPrediction(ObjectAnnotation):\n    \"\"\"Class for handling detection model predictions.\"\"\"\n\n    def __init__(\n        self,\n        bbox: list[int] | None = None,\n        category_id: int | None = None,\n        category_name: str | None = None,\n        segmentation: list[list[float]] | None = None,\n        score: float = 0.0,\n        shift_amount: list[int] | None = [0, 0],\n        full_shape: list[int] | None = None,\n    ):\n        \"\"\"Creates ObjectPrediction from bbox, score, category_id, category_name, segmentation.\n\n        Args:\n            bbox: list\n                [minx, miny, maxx, maxy]\n            score: float\n                Prediction score between 0 and 1\n            category_id: int\n                ID of the object category\n            category_name: str\n                Name of the object category\n            segmentation: List[List]\n                [\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    ...\n                ]\n            shift_amount: list\n                To shift the box and mask predictions from sliced image\n                to full sized image, should be in the form of [shift_x, shift_y]\n            full_shape: list\n                Size of the full image after shifting, should be in\n                the form of [height, width]\n        \"\"\"\n        self.score = PredictionScore(score)\n        super().__init__(\n            bbox=bbox,\n            category_id=category_id,\n            segmentation=segmentation,\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    def get_shifted_object_prediction(self):\n        \"\"\"Returns shifted version ObjectPrediction.\n\n        Shifts bbox and mask coords. Used for mapping sliced predictions over full image.\n        \"\"\"\n        if self.mask:\n            shifted_mask = self.mask.get_shifted_mask()\n            return ObjectPrediction(\n                bbox=self.bbox.get_shifted_box().to_xyxy(),\n                category_id=self.category.id,\n                score=self.score.value,\n                segmentation=shifted_mask.segmentation,\n                category_name=self.category.name,\n                shift_amount=[0, 0],\n                full_shape=shifted_mask.full_shape,\n            )\n        else:\n            return ObjectPrediction(\n                bbox=self.bbox.get_shifted_box().to_xyxy(),\n                category_id=self.category.id,\n                score=self.score.value,\n                segmentation=None,\n                category_name=self.category.name,\n                shift_amount=[0, 0],\n                full_shape=None,\n            )\n\n    def to_coco_prediction(self, image_id=None):\n        \"\"\"Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.\"\"\"\n        if self.mask:\n            coco_prediction = CocoPrediction.from_coco_segmentation(\n                segmentation=self.mask.segmentation,\n                category_id=self.category.id,\n                category_name=self.category.name,\n                score=self.score.value,\n                image_id=image_id,\n            )\n        else:\n            coco_prediction = CocoPrediction.from_coco_bbox(\n                bbox=self.bbox.to_xywh(),\n                category_id=self.category.id,\n                category_name=self.category.name,\n                score=self.score.value,\n                image_id=image_id,\n            )\n        return coco_prediction\n\n    def to_fiftyone_detection(self, image_height: int, image_width: int):\n        \"\"\"Returns fiftyone.Detection representation of ObjectPrediction.\"\"\"\n        try:\n            import fiftyone as fo\n        except ImportError:\n            raise ImportError('Please run \"pip install -U fiftyone\" to install fiftyone first for fiftyone conversion.')\n\n        x1, y1, x2, y2 = self.bbox.to_xyxy()\n        rel_box = [x1 / image_width, y1 / image_height, (x2 - x1) / image_width, (y2 - y1) / image_height]\n        fiftyone_detection = fo.Detection(label=self.category.name, bounding_box=rel_box, confidence=self.score.value)\n        return fiftyone_detection\n\n    def __repr__(self):\n        return f\"\"\"ObjectPrediction&lt;\n    bbox: {self.bbox},\n    mask: {self.mask},\n    score: {self.score},\n    category: {self.category}&gt;\"\"\"\n</code></pre>"},{"location":"api/#sahi.ObjectPrediction-functions","title":"Functions","text":""},{"location":"api/#sahi.ObjectPrediction.__init__","title":"<code>__init__(bbox=None, category_id=None, category_name=None, segmentation=None, score=0.0, shift_amount=[0, 0], full_shape=None)</code>","text":"<p>Creates ObjectPrediction from bbox, score, category_id, category_name, segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> \u00b6 <code>list[int] | None</code> <p>list [minx, miny, maxx, maxy]</p> <code>None</code> <code>score</code> \u00b6 <code>float</code> <p>float Prediction score between 0 and 1</p> <code>0.0</code> <code>category_id</code> \u00b6 <code>int | None</code> <p>int ID of the object category</p> <code>None</code> <code>category_name</code> \u00b6 <code>str | None</code> <p>str Name of the object category</p> <code>None</code> <code>segmentation</code> \u00b6 <code>list[list[float]] | None</code> <p>List[List] [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> <code>None</code> <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>list To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> <code>full_shape</code> \u00b6 <code>list[int] | None</code> <p>list Size of the full image after shifting, should be in the form of [height, width]</p> <code>None</code> Source code in <code>sahi/prediction.py</code> <pre><code>def __init__(\n    self,\n    bbox: list[int] | None = None,\n    category_id: int | None = None,\n    category_name: str | None = None,\n    segmentation: list[list[float]] | None = None,\n    score: float = 0.0,\n    shift_amount: list[int] | None = [0, 0],\n    full_shape: list[int] | None = None,\n):\n    \"\"\"Creates ObjectPrediction from bbox, score, category_id, category_name, segmentation.\n\n    Args:\n        bbox: list\n            [minx, miny, maxx, maxy]\n        score: float\n            Prediction score between 0 and 1\n        category_id: int\n            ID of the object category\n        category_name: str\n            Name of the object category\n        segmentation: List[List]\n            [\n                [x1, y1, x2, y2, x3, y3, ...],\n                [x1, y1, x2, y2, x3, y3, ...],\n                ...\n            ]\n        shift_amount: list\n            To shift the box and mask predictions from sliced image\n            to full sized image, should be in the form of [shift_x, shift_y]\n        full_shape: list\n            Size of the full image after shifting, should be in\n            the form of [height, width]\n    \"\"\"\n    self.score = PredictionScore(score)\n    super().__init__(\n        bbox=bbox,\n        category_id=category_id,\n        segmentation=segmentation,\n        category_name=category_name,\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre>"},{"location":"api/#sahi.ObjectPrediction.get_shifted_object_prediction","title":"<code>get_shifted_object_prediction()</code>","text":"<p>Returns shifted version ObjectPrediction.</p> <p>Shifts bbox and mask coords. Used for mapping sliced predictions over full image.</p> Source code in <code>sahi/prediction.py</code> <pre><code>def get_shifted_object_prediction(self):\n    \"\"\"Returns shifted version ObjectPrediction.\n\n    Shifts bbox and mask coords. Used for mapping sliced predictions over full image.\n    \"\"\"\n    if self.mask:\n        shifted_mask = self.mask.get_shifted_mask()\n        return ObjectPrediction(\n            bbox=self.bbox.get_shifted_box().to_xyxy(),\n            category_id=self.category.id,\n            score=self.score.value,\n            segmentation=shifted_mask.segmentation,\n            category_name=self.category.name,\n            shift_amount=[0, 0],\n            full_shape=shifted_mask.full_shape,\n        )\n    else:\n        return ObjectPrediction(\n            bbox=self.bbox.get_shifted_box().to_xyxy(),\n            category_id=self.category.id,\n            score=self.score.value,\n            segmentation=None,\n            category_name=self.category.name,\n            shift_amount=[0, 0],\n            full_shape=None,\n        )\n</code></pre>"},{"location":"api/#sahi.ObjectPrediction.to_coco_prediction","title":"<code>to_coco_prediction(image_id=None)</code>","text":"<p>Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.</p> Source code in <code>sahi/prediction.py</code> <pre><code>def to_coco_prediction(self, image_id=None):\n    \"\"\"Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.\"\"\"\n    if self.mask:\n        coco_prediction = CocoPrediction.from_coco_segmentation(\n            segmentation=self.mask.segmentation,\n            category_id=self.category.id,\n            category_name=self.category.name,\n            score=self.score.value,\n            image_id=image_id,\n        )\n    else:\n        coco_prediction = CocoPrediction.from_coco_bbox(\n            bbox=self.bbox.to_xywh(),\n            category_id=self.category.id,\n            category_name=self.category.name,\n            score=self.score.value,\n            image_id=image_id,\n        )\n    return coco_prediction\n</code></pre>"},{"location":"api/#sahi.ObjectPrediction.to_fiftyone_detection","title":"<code>to_fiftyone_detection(image_height, image_width)</code>","text":"<p>Returns fiftyone.Detection representation of ObjectPrediction.</p> Source code in <code>sahi/prediction.py</code> <pre><code>def to_fiftyone_detection(self, image_height: int, image_width: int):\n    \"\"\"Returns fiftyone.Detection representation of ObjectPrediction.\"\"\"\n    try:\n        import fiftyone as fo\n    except ImportError:\n        raise ImportError('Please run \"pip install -U fiftyone\" to install fiftyone first for fiftyone conversion.')\n\n    x1, y1, x2, y2 = self.bbox.to_xyxy()\n    rel_box = [x1 / image_width, y1 / image_height, (x2 - x1) / image_width, (y2 - y1) / image_height]\n    fiftyone_detection = fo.Detection(label=self.category.name, bounding_box=rel_box, confidence=self.score.value)\n    return fiftyone_detection\n</code></pre>"},{"location":"api/#sahi-modules","title":"Modules","text":""},{"location":"api/#sahi.annotation","title":"<code>annotation</code>","text":""},{"location":"api/#sahi.annotation-classes","title":"Classes","text":""},{"location":"api/#sahi.annotation.BoundingBox","title":"<code>BoundingBox</code>  <code>dataclass</code>","text":"<p>BoundingBox represents a rectangular region in 2D space, typically used for object detection annotations.</p> <p>Attributes:</p> Name Type Description <code>box</code> <code>Tuple[float, float, float, float]</code> <p>The bounding box coordinates in the format (minx, miny, maxx, maxy). - minx (float): Minimum x-coordinate (left). - miny (float): Minimum y-coordinate (top). - maxx (float): Maximum x-coordinate (right). - maxy (float): Maximum y-coordinate (bottom).</p> <code>shift_amount</code> <code>Tuple[int, int]</code> <p>The amount to shift the bounding box in the x and y directions. Defaults to (0, 0).</p> <p>BoundingBox Usage Example</p> <pre><code>bbox = BoundingBox((10.0, 20.0, 50.0, 80.0))\narea = bbox.area\nexpanded_bbox = bbox.get_expanded_box(ratio=0.2)\nshifted_bbox = bbox.get_shifted_box()\ncoco_format = bbox.to_coco_bbox()\n</code></pre> Source code in <code>sahi/annotation.py</code> <pre><code>@dataclass(frozen=True)\nclass BoundingBox:\n    \"\"\"BoundingBox represents a rectangular region in 2D space, typically used for object detection annotations.\n\n    Attributes:\n        box (Tuple[float, float, float, float]): The bounding box coordinates in the format (minx, miny, maxx, maxy).\n            - minx (float): Minimum x-coordinate (left).\n            - miny (float): Minimum y-coordinate (top).\n            - maxx (float): Maximum x-coordinate (right).\n            - maxy (float): Maximum y-coordinate (bottom).\n        shift_amount (Tuple[int, int], optional): The amount to shift the bounding box in the x and y directions.\n            Defaults to (0, 0).\n\n    !!! example \"BoundingBox Usage Example\"\n        ```python\n        bbox = BoundingBox((10.0, 20.0, 50.0, 80.0))\n        area = bbox.area\n        expanded_bbox = bbox.get_expanded_box(ratio=0.2)\n        shifted_bbox = bbox.get_shifted_box()\n        coco_format = bbox.to_coco_bbox()\n        ```\n    \"\"\"\n\n    box: tuple[float, float, float, float] | list[float]\n    shift_amount: tuple[int, int] = (0, 0)\n\n    def __post_init__(self):\n        if len(self.box) != 4 or any(coord &lt; 0 for coord in self.box):\n            raise ValueError(\"box must be 4 non-negative floats: [minx, miny, maxx, maxy]\")\n        if len(self.shift_amount) != 2:\n            raise ValueError(\"shift_amount must be 2 integers: [shift_x, shift_y]\")\n\n    @property\n    def minx(self):\n        return self.box[0]\n\n    @property\n    def miny(self):\n        return self.box[1]\n\n    @property\n    def maxx(self):\n        return self.box[2]\n\n    @property\n    def maxy(self):\n        return self.box[3]\n\n    @property\n    def shift_x(self):\n        return self.shift_amount[0]\n\n    @property\n    def shift_y(self):\n        return self.shift_amount[1]\n\n    @property\n    def area(self):\n        return (self.maxx - self.minx) * (self.maxy - self.miny)\n\n    def get_expanded_box(self, ratio: float = 0.1, max_x: int | None = None, max_y: int | None = None):\n        \"\"\"Returns an expanded bounding box by increasing its size by a given ratio. The expansion is applied equally in\n        all directions. Optionally, the expanded box can be clipped to maximum x and y boundaries.\n\n        Args:\n            ratio (float, optional): The proportion by which to expand the box size.\n                Default is 0.1 (10%).\n            max_x (int, optional): The maximum allowed x-coordinate for the expanded box.\n                If None, no maximum is applied.\n            max_y (int, optional): The maximum allowed y-coordinate for the expanded box.\n                If None, no maximum is applied.\n\n        Returns:\n            BoundingBox: A new BoundingBox instance representing the expanded box.\n        \"\"\"\n\n        w = self.maxx - self.minx\n        h = self.maxy - self.miny\n        y_mar = int(w * ratio)\n        x_mar = int(h * ratio)\n        maxx = min(max_x, self.maxx + x_mar) if max_x else self.maxx + x_mar\n        minx = max(0, self.minx - x_mar)\n        maxy = min(max_y, self.maxy + y_mar) if max_y else self.maxy + y_mar\n        miny = max(0, self.miny - y_mar)\n        box: list[float] = [minx, miny, maxx, maxy]\n        return BoundingBox(box)\n\n    def to_xywh(self):\n        \"\"\"Returns [xmin, ymin, width, height]\n\n        Returns:\n            List[float]: A list containing the bounding box in the format [xmin, ymin, width, height].\n        \"\"\"\n\n        return [self.minx, self.miny, self.maxx - self.minx, self.maxy - self.miny]\n\n    def to_coco_bbox(self):\n        \"\"\"\n        Returns the bounding box in COCO format: [xmin, ymin, width, height]\n\n        Returns:\n            List[float]: A list containing the bounding box in COCO format.\n        \"\"\"\n        return self.to_xywh()\n\n    def to_xyxy(self):\n        \"\"\"\n        Returns: [xmin, ymin, xmax, ymax]\n\n        Returns:\n            List[float]: A list containing the bounding box in the format [xmin, ymin, xmax, ymax].\n        \"\"\"\n        return [self.minx, self.miny, self.maxx, self.maxy]\n\n    def to_voc_bbox(self):\n        \"\"\"\n        Returns the bounding box in VOC format: [xmin, ymin, xmax, ymax]\n\n        Returns:\n            List[float]: A list containing the bounding box in VOC format.\n        \"\"\"\n        return self.to_xyxy()\n\n    def get_shifted_box(self):\n        \"\"\"Returns shifted BoundingBox.\n\n        Returns:\n            BoundingBox: A new BoundingBox instance representing the shifted box.\n        \"\"\"\n        box = [\n            self.minx + self.shift_x,\n            self.miny + self.shift_y,\n            self.maxx + self.shift_x,\n            self.maxy + self.shift_y,\n        ]\n        return BoundingBox(box)\n\n    def __repr__(self):\n        return (\n            f\"BoundingBox: &lt;{(self.minx, self.miny, self.maxx, self.maxy)}, \"\n            f\"w: {self.maxx - self.minx}, h: {self.maxy - self.miny}&gt;\"\n        )\n</code></pre> Functions\u00b6 <code></code> <code>get_expanded_box(ratio=0.1, max_x=None, max_y=None)</code> \u00b6 <p>Returns an expanded bounding box by increasing its size by a given ratio. The expansion is applied equally in all directions. Optionally, the expanded box can be clipped to maximum x and y boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> \u00b6 <code>float</code> <p>The proportion by which to expand the box size. Default is 0.1 (10%).</p> <code>0.1</code> <code>max_x</code> \u00b6 <code>int</code> <p>The maximum allowed x-coordinate for the expanded box. If None, no maximum is applied.</p> <code>None</code> <code>max_y</code> \u00b6 <code>int</code> <p>The maximum allowed y-coordinate for the expanded box. If None, no maximum is applied.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>BoundingBox</code> <p>A new BoundingBox instance representing the expanded box.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def get_expanded_box(self, ratio: float = 0.1, max_x: int | None = None, max_y: int | None = None):\n    \"\"\"Returns an expanded bounding box by increasing its size by a given ratio. The expansion is applied equally in\n    all directions. Optionally, the expanded box can be clipped to maximum x and y boundaries.\n\n    Args:\n        ratio (float, optional): The proportion by which to expand the box size.\n            Default is 0.1 (10%).\n        max_x (int, optional): The maximum allowed x-coordinate for the expanded box.\n            If None, no maximum is applied.\n        max_y (int, optional): The maximum allowed y-coordinate for the expanded box.\n            If None, no maximum is applied.\n\n    Returns:\n        BoundingBox: A new BoundingBox instance representing the expanded box.\n    \"\"\"\n\n    w = self.maxx - self.minx\n    h = self.maxy - self.miny\n    y_mar = int(w * ratio)\n    x_mar = int(h * ratio)\n    maxx = min(max_x, self.maxx + x_mar) if max_x else self.maxx + x_mar\n    minx = max(0, self.minx - x_mar)\n    maxy = min(max_y, self.maxy + y_mar) if max_y else self.maxy + y_mar\n    miny = max(0, self.miny - y_mar)\n    box: list[float] = [minx, miny, maxx, maxy]\n    return BoundingBox(box)\n</code></pre> <code></code> <code>get_shifted_box()</code> \u00b6 <p>Returns shifted BoundingBox.</p> <p>Returns:</p> Name Type Description <code>BoundingBox</code> <p>A new BoundingBox instance representing the shifted box.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def get_shifted_box(self):\n    \"\"\"Returns shifted BoundingBox.\n\n    Returns:\n        BoundingBox: A new BoundingBox instance representing the shifted box.\n    \"\"\"\n    box = [\n        self.minx + self.shift_x,\n        self.miny + self.shift_y,\n        self.maxx + self.shift_x,\n        self.maxy + self.shift_y,\n    ]\n    return BoundingBox(box)\n</code></pre> <code></code> <code>to_coco_bbox()</code> \u00b6 <p>Returns the bounding box in COCO format: [xmin, ymin, width, height]</p> <p>Returns:</p> Type Description <p>List[float]: A list containing the bounding box in COCO format.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_coco_bbox(self):\n    \"\"\"\n    Returns the bounding box in COCO format: [xmin, ymin, width, height]\n\n    Returns:\n        List[float]: A list containing the bounding box in COCO format.\n    \"\"\"\n    return self.to_xywh()\n</code></pre> <code></code> <code>to_voc_bbox()</code> \u00b6 <p>Returns the bounding box in VOC format: [xmin, ymin, xmax, ymax]</p> <p>Returns:</p> Type Description <p>List[float]: A list containing the bounding box in VOC format.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_voc_bbox(self):\n    \"\"\"\n    Returns the bounding box in VOC format: [xmin, ymin, xmax, ymax]\n\n    Returns:\n        List[float]: A list containing the bounding box in VOC format.\n    \"\"\"\n    return self.to_xyxy()\n</code></pre> <code></code> <code>to_xywh()</code> \u00b6 <p>Returns [xmin, ymin, width, height]</p> <p>Returns:</p> Type Description <p>List[float]: A list containing the bounding box in the format [xmin, ymin, width, height].</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_xywh(self):\n    \"\"\"Returns [xmin, ymin, width, height]\n\n    Returns:\n        List[float]: A list containing the bounding box in the format [xmin, ymin, width, height].\n    \"\"\"\n\n    return [self.minx, self.miny, self.maxx - self.minx, self.maxy - self.miny]\n</code></pre> <code></code> <code>to_xyxy()</code> \u00b6 <p>Returns: [xmin, ymin, xmax, ymax]</p> <p>Returns:</p> Type Description <p>List[float]: A list containing the bounding box in the format [xmin, ymin, xmax, ymax].</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_xyxy(self):\n    \"\"\"\n    Returns: [xmin, ymin, xmax, ymax]\n\n    Returns:\n        List[float]: A list containing the bounding box in the format [xmin, ymin, xmax, ymax].\n    \"\"\"\n    return [self.minx, self.miny, self.maxx, self.maxy]\n</code></pre>"},{"location":"api/#sahi.annotation.Category","title":"<code>Category</code>  <code>dataclass</code>","text":"<p>Category of the annotation.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Unique identifier for the category.</p> <code>name</code> <code>str</code> <p>Name of the category.</p> Source code in <code>sahi/annotation.py</code> <pre><code>@dataclass(frozen=True)\nclass Category:\n    \"\"\"Category of the annotation.\n\n    Attributes:\n        id (int): Unique identifier for the category.\n        name (str): Name of the category.\n    \"\"\"\n\n    id: int\n    name: str\n\n    def __post_init__(self):\n        if not isinstance(self.id, int):\n            raise TypeError(\"id should be integer\")\n        if not isinstance(self.name, str):\n            raise TypeError(\"name should be string\")\n\n    def __repr__(self):\n        return f\"Category: &lt;id: {self.id}, name: {self.name}&gt;\"\n</code></pre>"},{"location":"api/#sahi.annotation.Mask","title":"<code>Mask</code>","text":"<p>Init Mask from coco segmentation representation.</p> <p>Parameters:</p> Name Type Description Default <code>segmentation </code> \u00b6 <p>List[List] [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> required <code>full_shape</code> \u00b6 <code>list[int]</code> <p>List[int] Size of the full image, should be in the form of [height, width]</p> required <code>shift_amount</code> \u00b6 <code>list</code> <p>List[int] To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>class Mask:\n    \"\"\"Init Mask from coco segmentation representation.\n\n    Args:\n        segmentation : List[List]\n            [\n                [x1, y1, x2, y2, x3, y3, ...],\n                [x1, y1, x2, y2, x3, y3, ...],\n                ...\n            ]\n        full_shape: List[int]\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List[int]\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n\n    def __init__(\n        self,\n        segmentation: list[list[float]],\n        full_shape: list[int],\n        shift_amount: list = [0, 0],\n    ):\n        if full_shape is None:\n            raise ValueError(\"full_shape must be provided\")  # pyright: ignore[reportUnreachable]\n\n        self.shift_x = shift_amount[0]\n        self.shift_y = shift_amount[1]\n        self.full_shape_height = full_shape[0]\n        self.full_shape_width = full_shape[1]\n        self.segmentation = segmentation\n\n    @classmethod\n    def from_float_mask(\n        cls,\n        mask: np.ndarray,\n        full_shape: list[int],\n        mask_threshold: float = 0.5,\n        shift_amount: list = [0, 0],\n    ):\n        \"\"\"\n        Args:\n            mask: np.ndarray of np.float elements\n                Mask values between 0 and 1 (should have a shape of height*width)\n            mask_threshold: float\n                Value to threshold mask pixels between 0 and 1\n            shift_amount: List\n                To shift the box and mask predictions from sliced image\n                to full sized image, should be in the form of [shift_x, shift_y]\n            full_shape: List[int]\n                Size of the full image after shifting, should be in the form of [height, width]\n        \"\"\"\n        bool_mask = mask &gt; mask_threshold\n        return cls(\n            segmentation=get_coco_segmentation_from_bool_mask(bool_mask),\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @classmethod\n    def from_bool_mask(\n        cls,\n        bool_mask: np.ndarray,\n        full_shape: list[int],\n        shift_amount: list = [0, 0],\n    ):\n        \"\"\"\n        Args:\n            bool_mask: np.ndarray with bool elements\n                2D mask of object, should have a shape of height*width\n            full_shape: List[int]\n                Size of the full image, should be in the form of [height, width]\n            shift_amount: List[int]\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n        \"\"\"\n        return cls(\n            segmentation=get_coco_segmentation_from_bool_mask(bool_mask),\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @property\n    def bool_mask(self) -&gt; np.ndarray:\n        return get_bool_mask_from_coco_segmentation(\n            self.segmentation, width=self.full_shape[1], height=self.full_shape[0]\n        )\n\n    @property\n    def shape(self) -&gt; list[int]:\n        \"\"\"Returns mask shape as [height, width]\"\"\"\n        return [self.bool_mask.shape[0], self.bool_mask.shape[1]]\n\n    @property\n    def full_shape(self) -&gt; list[int]:\n        \"\"\"Returns full mask shape after shifting as [height, width]\"\"\"\n        return [self.full_shape_height, self.full_shape_width]\n\n    @property\n    def shift_amount(self):\n        \"\"\"Returns the shift amount of the mask slice as [shift_x, shift_y]\"\"\"\n        return [self.shift_x, self.shift_y]\n\n    def get_shifted_mask(self) -&gt; Mask:\n        # Confirm full_shape is specified\n        if (self.full_shape_height is None) or (self.full_shape_width is None):\n            raise ValueError(\"full_shape is None\")\n        shifted_segmentation = []\n        for s in self.segmentation:\n            xs = [min(self.shift_x + s[i], self.full_shape_width) for i in range(0, len(s) - 1, 2)]\n            ys = [min(self.shift_y + s[i], self.full_shape_height) for i in range(1, len(s), 2)]\n            shifted_segmentation.append([j for i in zip(xs, ys) for j in i])\n        return Mask(\n            segmentation=shifted_segmentation,\n            shift_amount=[0, 0],\n            full_shape=self.full_shape,\n        )\n</code></pre> Attributes\u00b6 <code></code> <code>full_shape</code> <code>property</code> \u00b6 <p>Returns full mask shape after shifting as [height, width]</p> <code></code> <code>shape</code> <code>property</code> \u00b6 <p>Returns mask shape as [height, width]</p> <code></code> <code>shift_amount</code> <code>property</code> \u00b6 <p>Returns the shift amount of the mask slice as [shift_x, shift_y]</p> Functions\u00b6 <code></code> <code>from_bool_mask(bool_mask, full_shape, shift_amount=[0, 0])</code> <code>classmethod</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>bool_mask</code> \u00b6 <code>ndarray</code> <p>np.ndarray with bool elements 2D mask of object, should have a shape of height*width</p> required <code>full_shape</code> \u00b6 <code>list[int]</code> <p>List[int] Size of the full image, should be in the form of [height, width]</p> required <code>shift_amount</code> \u00b6 <code>list</code> <p>List[int] To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_bool_mask(\n    cls,\n    bool_mask: np.ndarray,\n    full_shape: list[int],\n    shift_amount: list = [0, 0],\n):\n    \"\"\"\n    Args:\n        bool_mask: np.ndarray with bool elements\n            2D mask of object, should have a shape of height*width\n        full_shape: List[int]\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List[int]\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n    return cls(\n        segmentation=get_coco_segmentation_from_bool_mask(bool_mask),\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre> <code></code> <code>from_float_mask(mask, full_shape, mask_threshold=0.5, shift_amount=[0, 0])</code> <code>classmethod</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>mask</code> \u00b6 <code>ndarray</code> <p>np.ndarray of np.float elements Mask values between 0 and 1 (should have a shape of height*width)</p> required <code>mask_threshold</code> \u00b6 <code>float</code> <p>float Value to threshold mask pixels between 0 and 1</p> <code>0.5</code> <code>shift_amount</code> \u00b6 <code>list</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> <code>full_shape</code> \u00b6 <code>list[int]</code> <p>List[int] Size of the full image after shifting, should be in the form of [height, width]</p> required Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_float_mask(\n    cls,\n    mask: np.ndarray,\n    full_shape: list[int],\n    mask_threshold: float = 0.5,\n    shift_amount: list = [0, 0],\n):\n    \"\"\"\n    Args:\n        mask: np.ndarray of np.float elements\n            Mask values between 0 and 1 (should have a shape of height*width)\n        mask_threshold: float\n            Value to threshold mask pixels between 0 and 1\n        shift_amount: List\n            To shift the box and mask predictions from sliced image\n            to full sized image, should be in the form of [shift_x, shift_y]\n        full_shape: List[int]\n            Size of the full image after shifting, should be in the form of [height, width]\n    \"\"\"\n    bool_mask = mask &gt; mask_threshold\n    return cls(\n        segmentation=get_coco_segmentation_from_bool_mask(bool_mask),\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre>"},{"location":"api/#sahi.annotation.ObjectAnnotation","title":"<code>ObjectAnnotation</code>","text":"<p>All about an annotation such as Mask, Category, BoundingBox.</p> Source code in <code>sahi/annotation.py</code> <pre><code>class ObjectAnnotation:\n    \"\"\"All about an annotation such as Mask, Category, BoundingBox.\"\"\"\n\n    def __init__(\n        self,\n        bbox: list[int] | None = None,\n        segmentation: np.ndarray | None = None,\n        category_id: int | None = None,\n        category_name: str | None = None,\n        shift_amount: list[int] | None = [0, 0],\n        full_shape: list[int] | None = None,\n    ):\n        \"\"\"\n        Args:\n            bbox: List\n                [minx, miny, maxx, maxy]\n            segmentation: List[List]\n                [\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    ...\n                ]\n            category_id: int\n                ID of the object category\n            category_name: str\n                Name of the object category\n            shift_amount: List\n                To shift the box and mask predictions from sliced image\n                to full sized image, should be in the form of [shift_x, shift_y]\n            full_shape: List\n                Size of the full image after shifting, should be in\n                the form of [height, width]\n        \"\"\"\n        if not isinstance(category_id, int):\n            raise ValueError(\"category_id must be an integer\")\n        if (bbox is None) and (segmentation is None):\n            raise ValueError(\"you must provide a bbox or segmentation\")\n\n        self.mask: Mask | None = None\n        if segmentation is not None:\n            self.mask = Mask(\n                segmentation=segmentation,\n                shift_amount=shift_amount,\n                full_shape=full_shape,\n            )\n            bbox_from_segmentation = get_bbox_from_coco_segmentation(segmentation)\n            # https://github.com/obss/sahi/issues/235\n            if bbox_from_segmentation is not None:\n                bbox = bbox_from_segmentation\n            else:\n                raise ValueError(\"Invalid segmentation mask.\")\n\n        # if bbox is a numpy object, convert it to python List[float]\n        if type(bbox).__module__ == \"numpy\":\n            bbox = copy.deepcopy(bbox).tolist()\n\n        # make sure bbox coords lie inside [0, image_size]\n        xmin = max(bbox[0], 0)\n        ymin = max(bbox[1], 0)\n        if full_shape:\n            xmax = min(bbox[2], full_shape[1])\n            ymax = min(bbox[3], full_shape[0])\n        else:\n            xmax = bbox[2]\n            ymax = bbox[3]\n        bbox = [xmin, ymin, xmax, ymax]\n        # set bbox\n        self.bbox = BoundingBox(bbox, shift_amount)\n\n        category_name = category_name if category_name else str(category_id)\n        self.category = Category(\n            id=category_id,\n            name=category_name,\n        )\n\n        self.merged = None\n\n    @classmethod\n    def from_bool_mask(\n        cls,\n        bool_mask,\n        category_id: int | None = None,\n        category_name: str | None = None,\n        shift_amount: list[int] | None = [0, 0],\n        full_shape: list[int] | None = None,\n    ):\n        \"\"\"Creates ObjectAnnotation from bool_mask (2D np.ndarray)\n\n        Args:\n            bool_mask: np.ndarray with bool elements\n                2D mask of object, should have a shape of height*width\n            category_id: int\n                ID of the object category\n            category_name: str\n                Name of the object category\n            full_shape: List\n                Size of the full image, should be in the form of [height, width]\n            shift_amount: List\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n        \"\"\"\n        segmentation = get_coco_segmentation_from_bool_mask(bool_mask)\n        return cls(\n            category_id=category_id,\n            segmentation=segmentation,\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @classmethod\n    def from_coco_segmentation(\n        cls,\n        segmentation,\n        full_shape: list[int],\n        category_id: int | None = None,\n        category_name: str | None = None,\n        shift_amount: list[int] | None = [0, 0],\n    ):\n        \"\"\"\n        Creates ObjectAnnotation from coco segmentation:\n        [\n            [x1, y1, x2, y2, x3, y3, ...],\n            [x1, y1, x2, y2, x3, y3, ...],\n            ...\n        ]\n\n        Args:\n            segmentation: List[List]\n                [\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    ...\n                ]\n            category_id: int\n                ID of the object category\n            category_name: str\n                Name of the object category\n            full_shape: List\n                Size of the full image, should be in the form of [height, width]\n            shift_amount: List\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n        \"\"\"\n        return cls(\n            category_id=category_id,\n            segmentation=segmentation,\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @classmethod\n    def from_coco_bbox(\n        cls,\n        bbox: list[int],\n        category_id: int | None = None,\n        category_name: str | None = None,\n        shift_amount: list[int] | None = [0, 0],\n        full_shape: list[int] | None = None,\n    ):\n        \"\"\"Creates ObjectAnnotation from coco bbox [minx, miny, width, height]\n\n        Args:\n            bbox: List\n                [minx, miny, width, height]\n            category_id: int\n                ID of the object category\n            category_name: str\n                Name of the object category\n            full_shape: List\n                Size of the full image, should be in the form of [height, width]\n            shift_amount: List\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n        \"\"\"\n        xmin = bbox[0]\n        ymin = bbox[1]\n        xmax = bbox[0] + bbox[2]\n        ymax = bbox[1] + bbox[3]\n        bbox = [xmin, ymin, xmax, ymax]\n        return cls(\n            category_id=category_id,\n            bbox=bbox,\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @classmethod\n    def from_coco_annotation_dict(\n        cls,\n        annotation_dict: dict,\n        full_shape: list[int],\n        category_name: str | None = None,\n        shift_amount: list[int] | None = [0, 0],\n    ):\n        \"\"\"Creates ObjectAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\",\n        \"segmentation\", \"category_id\").\n\n        Args:\n            annotation_dict: dict\n                COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")\n            category_name: str\n                Category name of the annotation\n            full_shape: List\n                Size of the full image, should be in the form of [height, width]\n            shift_amount: List\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n        \"\"\"\n        if annotation_dict[\"segmentation\"]:\n            return cls.from_coco_segmentation(\n                segmentation=annotation_dict[\"segmentation\"],\n                category_id=annotation_dict[\"category_id\"],\n                category_name=category_name,\n                shift_amount=shift_amount,\n                full_shape=full_shape,\n            )\n        else:\n            return cls.from_coco_bbox(\n                bbox=annotation_dict[\"bbox\"],\n                category_id=annotation_dict[\"category_id\"],\n                category_name=category_name,\n                shift_amount=shift_amount,\n                full_shape=full_shape,\n            )\n\n    @classmethod\n    def from_shapely_annotation(\n        cls,\n        annotation: ShapelyAnnotation,\n        full_shape: list[int],\n        category_id: int | None = None,\n        category_name: str | None = None,\n        shift_amount: list[int] | None = [0, 0],\n    ):\n        \"\"\"Creates ObjectAnnotation from shapely_utils.ShapelyAnnotation.\n\n        Args:\n            annotation: shapely_utils.ShapelyAnnotation\n            category_id: int\n                ID of the object category\n            category_name: str\n                Name of the object category\n            full_shape: List\n                Size of the full image, should be in the form of [height, width]\n            shift_amount: List\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n        \"\"\"\n        return cls(\n            category_id=category_id,\n            segmentation=annotation.to_coco_segmentation(),\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    @classmethod\n    def from_imantics_annotation(\n        cls,\n        annotation,\n        shift_amount: list[int] | None = [0, 0],\n        full_shape: list[int] | None = None,\n    ):\n        \"\"\"Creates ObjectAnnotation from imantics.annotation.Annotation.\n\n        Args:\n            annotation: imantics.annotation.Annotation\n            shift_amount: List\n                To shift the box and mask predictions from sliced image to full\n                sized image, should be in the form of [shift_x, shift_y]\n            full_shape: List\n                Size of the full image, should be in the form of [height, width]\n        \"\"\"\n        return cls(\n            category_id=annotation.category.id,\n            bool_mask=annotation.mask.array,\n            category_name=annotation.category.name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    def to_coco_annotation(self) -&gt; CocoAnnotation:\n        \"\"\"Returns sahi.utils.coco.CocoAnnotation representation of ObjectAnnotation.\"\"\"\n        if self.mask:\n            coco_annotation = CocoAnnotation.from_coco_segmentation(\n                segmentation=self.mask.segmentation,\n                category_id=self.category.id,\n                category_name=self.category.name,\n            )\n        else:\n            coco_annotation = CocoAnnotation.from_coco_bbox(\n                bbox=self.bbox.to_xywh(),\n                category_id=self.category.id,\n                category_name=self.category.name,\n            )\n        return coco_annotation\n\n    def to_coco_prediction(self) -&gt; CocoPrediction:\n        \"\"\"Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.\"\"\"\n        if self.mask:\n            coco_prediction = CocoPrediction.from_coco_segmentation(\n                segmentation=self.mask.segmentation,\n                category_id=self.category.id,\n                category_name=self.category.name,\n                score=1,\n            )\n        else:\n            coco_prediction = CocoPrediction.from_coco_bbox(\n                bbox=self.bbox.to_xywh(),\n                category_id=self.category.id,\n                category_name=self.category.name,\n                score=1,\n            )\n        return coco_prediction\n\n    def to_shapely_annotation(self) -&gt; ShapelyAnnotation:\n        \"\"\"Returns sahi.utils.shapely.ShapelyAnnotation representation of ObjectAnnotation.\"\"\"\n        if self.mask:\n            shapely_annotation = ShapelyAnnotation.from_coco_segmentation(\n                segmentation=self.mask.segmentation,\n            )\n        else:\n            shapely_annotation = ShapelyAnnotation.from_coco_bbox(\n                bbox=self.bbox.to_xywh(),\n            )\n        return shapely_annotation\n\n    def to_imantics_annotation(self):\n        \"\"\"Returns imantics.annotation.Annotation representation of ObjectAnnotation.\"\"\"\n        try:\n            import imantics\n        except ImportError:\n            raise ImportError('Please run \"pip install -U imantics\" to install imantics first for imantics conversion.')\n\n        imantics_category = imantics.Category(id=self.category.id, name=self.category.name)\n        if self.mask is not None:\n            imantics_mask = imantics.Mask.create(self.mask.bool_mask)\n            imantics_annotation = imantics.annotation.Annotation.from_mask(\n                mask=imantics_mask, category=imantics_category\n            )\n        else:\n            imantics_bbox = imantics.BBox.create(self.bbox.to_xyxy())\n            imantics_annotation = imantics.annotation.Annotation.from_bbox(\n                bbox=imantics_bbox, category=imantics_category\n            )\n        return imantics_annotation\n\n    def deepcopy(self):\n        \"\"\"\n        Returns: deepcopy of current ObjectAnnotation instance\n        \"\"\"\n        return copy.deepcopy(self)\n\n    @classmethod\n    def get_empty_mask(cls):\n        return Mask(bool_mask=None)\n\n    def get_shifted_object_annotation(self):\n        if self.mask:\n            shifted_mask = self.mask.get_shifted_mask()\n            return ObjectAnnotation(\n                bbox=self.bbox.get_shifted_box().to_xyxy(),\n                category_id=self.category.id,\n                segmentation=shifted_mask.segmentation,\n                category_name=self.category.name,\n                shift_amount=[0, 0],\n                full_shape=shifted_mask.full_shape,\n            )\n        else:\n            return ObjectAnnotation(\n                bbox=self.bbox.get_shifted_box().to_xyxy(),\n                category_id=self.category.id,\n                bool_mask=None,\n                category_name=self.category.name,\n                shift_amount=[0, 0],\n                full_shape=None,\n            )\n\n    def __repr__(self):\n        return f\"\"\"ObjectAnnotation&lt;\n    bbox: {self.bbox},\n    mask: {self.mask},\n    category: {self.category}&gt;\"\"\"\n</code></pre> Functions\u00b6 <code></code> <code>__init__(bbox=None, segmentation=None, category_id=None, category_name=None, shift_amount=[0, 0], full_shape=None)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>bbox</code> \u00b6 <code>list[int] | None</code> <p>List [minx, miny, maxx, maxy]</p> <code>None</code> <code>segmentation</code> \u00b6 <code>ndarray | None</code> <p>List[List] [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> <code>None</code> <code>category_id</code> \u00b6 <code>int | None</code> <p>int ID of the object category</p> <code>None</code> <code>category_name</code> \u00b6 <code>str | None</code> <p>str Name of the object category</p> <code>None</code> <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> <code>full_shape</code> \u00b6 <code>list[int] | None</code> <p>List Size of the full image after shifting, should be in the form of [height, width]</p> <code>None</code> Source code in <code>sahi/annotation.py</code> <pre><code>def __init__(\n    self,\n    bbox: list[int] | None = None,\n    segmentation: np.ndarray | None = None,\n    category_id: int | None = None,\n    category_name: str | None = None,\n    shift_amount: list[int] | None = [0, 0],\n    full_shape: list[int] | None = None,\n):\n    \"\"\"\n    Args:\n        bbox: List\n            [minx, miny, maxx, maxy]\n        segmentation: List[List]\n            [\n                [x1, y1, x2, y2, x3, y3, ...],\n                [x1, y1, x2, y2, x3, y3, ...],\n                ...\n            ]\n        category_id: int\n            ID of the object category\n        category_name: str\n            Name of the object category\n        shift_amount: List\n            To shift the box and mask predictions from sliced image\n            to full sized image, should be in the form of [shift_x, shift_y]\n        full_shape: List\n            Size of the full image after shifting, should be in\n            the form of [height, width]\n    \"\"\"\n    if not isinstance(category_id, int):\n        raise ValueError(\"category_id must be an integer\")\n    if (bbox is None) and (segmentation is None):\n        raise ValueError(\"you must provide a bbox or segmentation\")\n\n    self.mask: Mask | None = None\n    if segmentation is not None:\n        self.mask = Mask(\n            segmentation=segmentation,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n        bbox_from_segmentation = get_bbox_from_coco_segmentation(segmentation)\n        # https://github.com/obss/sahi/issues/235\n        if bbox_from_segmentation is not None:\n            bbox = bbox_from_segmentation\n        else:\n            raise ValueError(\"Invalid segmentation mask.\")\n\n    # if bbox is a numpy object, convert it to python List[float]\n    if type(bbox).__module__ == \"numpy\":\n        bbox = copy.deepcopy(bbox).tolist()\n\n    # make sure bbox coords lie inside [0, image_size]\n    xmin = max(bbox[0], 0)\n    ymin = max(bbox[1], 0)\n    if full_shape:\n        xmax = min(bbox[2], full_shape[1])\n        ymax = min(bbox[3], full_shape[0])\n    else:\n        xmax = bbox[2]\n        ymax = bbox[3]\n    bbox = [xmin, ymin, xmax, ymax]\n    # set bbox\n    self.bbox = BoundingBox(bbox, shift_amount)\n\n    category_name = category_name if category_name else str(category_id)\n    self.category = Category(\n        id=category_id,\n        name=category_name,\n    )\n\n    self.merged = None\n</code></pre> <code></code> <code>deepcopy()</code> \u00b6 <p>Returns: deepcopy of current ObjectAnnotation instance</p> Source code in <code>sahi/annotation.py</code> <pre><code>def deepcopy(self):\n    \"\"\"\n    Returns: deepcopy of current ObjectAnnotation instance\n    \"\"\"\n    return copy.deepcopy(self)\n</code></pre> <code></code> <code>from_bool_mask(bool_mask, category_id=None, category_name=None, shift_amount=[0, 0], full_shape=None)</code> <code>classmethod</code> \u00b6 <p>Creates ObjectAnnotation from bool_mask (2D np.ndarray)</p> <p>Parameters:</p> Name Type Description Default <code>bool_mask</code> \u00b6 <p>np.ndarray with bool elements 2D mask of object, should have a shape of height*width</p> required <code>category_id</code> \u00b6 <code>int | None</code> <p>int ID of the object category</p> <code>None</code> <code>category_name</code> \u00b6 <code>str | None</code> <p>str Name of the object category</p> <code>None</code> <code>full_shape</code> \u00b6 <code>list[int] | None</code> <p>List Size of the full image, should be in the form of [height, width]</p> <code>None</code> <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_bool_mask(\n    cls,\n    bool_mask,\n    category_id: int | None = None,\n    category_name: str | None = None,\n    shift_amount: list[int] | None = [0, 0],\n    full_shape: list[int] | None = None,\n):\n    \"\"\"Creates ObjectAnnotation from bool_mask (2D np.ndarray)\n\n    Args:\n        bool_mask: np.ndarray with bool elements\n            2D mask of object, should have a shape of height*width\n        category_id: int\n            ID of the object category\n        category_name: str\n            Name of the object category\n        full_shape: List\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n    segmentation = get_coco_segmentation_from_bool_mask(bool_mask)\n    return cls(\n        category_id=category_id,\n        segmentation=segmentation,\n        category_name=category_name,\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre> <code></code> <code>from_coco_annotation_dict(annotation_dict, full_shape, category_name=None, shift_amount=[0, 0])</code> <code>classmethod</code> \u00b6 <p>Creates ObjectAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\").</p> <p>Parameters:</p> Name Type Description Default <code>annotation_dict</code> \u00b6 <code>dict</code> <p>dict COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")</p> required <code>category_name</code> \u00b6 <code>str | None</code> <p>str Category name of the annotation</p> <code>None</code> <code>full_shape</code> \u00b6 <code>list[int]</code> <p>List Size of the full image, should be in the form of [height, width]</p> required <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_coco_annotation_dict(\n    cls,\n    annotation_dict: dict,\n    full_shape: list[int],\n    category_name: str | None = None,\n    shift_amount: list[int] | None = [0, 0],\n):\n    \"\"\"Creates ObjectAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\",\n    \"segmentation\", \"category_id\").\n\n    Args:\n        annotation_dict: dict\n            COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")\n        category_name: str\n            Category name of the annotation\n        full_shape: List\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n    if annotation_dict[\"segmentation\"]:\n        return cls.from_coco_segmentation(\n            segmentation=annotation_dict[\"segmentation\"],\n            category_id=annotation_dict[\"category_id\"],\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n    else:\n        return cls.from_coco_bbox(\n            bbox=annotation_dict[\"bbox\"],\n            category_id=annotation_dict[\"category_id\"],\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n</code></pre> <code></code> <code>from_coco_bbox(bbox, category_id=None, category_name=None, shift_amount=[0, 0], full_shape=None)</code> <code>classmethod</code> \u00b6 <p>Creates ObjectAnnotation from coco bbox [minx, miny, width, height]</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> \u00b6 <code>list[int]</code> <p>List [minx, miny, width, height]</p> required <code>category_id</code> \u00b6 <code>int | None</code> <p>int ID of the object category</p> <code>None</code> <code>category_name</code> \u00b6 <code>str | None</code> <p>str Name of the object category</p> <code>None</code> <code>full_shape</code> \u00b6 <code>list[int] | None</code> <p>List Size of the full image, should be in the form of [height, width]</p> <code>None</code> <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_coco_bbox(\n    cls,\n    bbox: list[int],\n    category_id: int | None = None,\n    category_name: str | None = None,\n    shift_amount: list[int] | None = [0, 0],\n    full_shape: list[int] | None = None,\n):\n    \"\"\"Creates ObjectAnnotation from coco bbox [minx, miny, width, height]\n\n    Args:\n        bbox: List\n            [minx, miny, width, height]\n        category_id: int\n            ID of the object category\n        category_name: str\n            Name of the object category\n        full_shape: List\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n    xmin = bbox[0]\n    ymin = bbox[1]\n    xmax = bbox[0] + bbox[2]\n    ymax = bbox[1] + bbox[3]\n    bbox = [xmin, ymin, xmax, ymax]\n    return cls(\n        category_id=category_id,\n        bbox=bbox,\n        category_name=category_name,\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre> <code></code> <code>from_coco_segmentation(segmentation, full_shape, category_id=None, category_name=None, shift_amount=[0, 0])</code> <code>classmethod</code> \u00b6 <p>Creates ObjectAnnotation from coco segmentation: [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> <p>Parameters:</p> Name Type Description Default <code>segmentation</code> \u00b6 <p>List[List] [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> required <code>category_id</code> \u00b6 <code>int | None</code> <p>int ID of the object category</p> <code>None</code> <code>category_name</code> \u00b6 <code>str | None</code> <p>str Name of the object category</p> <code>None</code> <code>full_shape</code> \u00b6 <code>list[int]</code> <p>List Size of the full image, should be in the form of [height, width]</p> required <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_coco_segmentation(\n    cls,\n    segmentation,\n    full_shape: list[int],\n    category_id: int | None = None,\n    category_name: str | None = None,\n    shift_amount: list[int] | None = [0, 0],\n):\n    \"\"\"\n    Creates ObjectAnnotation from coco segmentation:\n    [\n        [x1, y1, x2, y2, x3, y3, ...],\n        [x1, y1, x2, y2, x3, y3, ...],\n        ...\n    ]\n\n    Args:\n        segmentation: List[List]\n            [\n                [x1, y1, x2, y2, x3, y3, ...],\n                [x1, y1, x2, y2, x3, y3, ...],\n                ...\n            ]\n        category_id: int\n            ID of the object category\n        category_name: str\n            Name of the object category\n        full_shape: List\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n    return cls(\n        category_id=category_id,\n        segmentation=segmentation,\n        category_name=category_name,\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre> <code></code> <code>from_imantics_annotation(annotation, shift_amount=[0, 0], full_shape=None)</code> <code>classmethod</code> \u00b6 <p>Creates ObjectAnnotation from imantics.annotation.Annotation.</p> <p>Parameters:</p> Name Type Description Default <code>annotation</code> \u00b6 <p>imantics.annotation.Annotation</p> required <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> <code>full_shape</code> \u00b6 <code>list[int] | None</code> <p>List Size of the full image, should be in the form of [height, width]</p> <code>None</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_imantics_annotation(\n    cls,\n    annotation,\n    shift_amount: list[int] | None = [0, 0],\n    full_shape: list[int] | None = None,\n):\n    \"\"\"Creates ObjectAnnotation from imantics.annotation.Annotation.\n\n    Args:\n        annotation: imantics.annotation.Annotation\n        shift_amount: List\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n        full_shape: List\n            Size of the full image, should be in the form of [height, width]\n    \"\"\"\n    return cls(\n        category_id=annotation.category.id,\n        bool_mask=annotation.mask.array,\n        category_name=annotation.category.name,\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre> <code></code> <code>from_shapely_annotation(annotation, full_shape, category_id=None, category_name=None, shift_amount=[0, 0])</code> <code>classmethod</code> \u00b6 <p>Creates ObjectAnnotation from shapely_utils.ShapelyAnnotation.</p> <p>Parameters:</p> Name Type Description Default <code>annotation</code> \u00b6 <code>ShapelyAnnotation</code> <p>shapely_utils.ShapelyAnnotation</p> required <code>category_id</code> \u00b6 <code>int | None</code> <p>int ID of the object category</p> <code>None</code> <code>category_name</code> \u00b6 <code>str | None</code> <p>str Name of the object category</p> <code>None</code> <code>full_shape</code> \u00b6 <code>list[int]</code> <p>List Size of the full image, should be in the form of [height, width]</p> required <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> Source code in <code>sahi/annotation.py</code> <pre><code>@classmethod\ndef from_shapely_annotation(\n    cls,\n    annotation: ShapelyAnnotation,\n    full_shape: list[int],\n    category_id: int | None = None,\n    category_name: str | None = None,\n    shift_amount: list[int] | None = [0, 0],\n):\n    \"\"\"Creates ObjectAnnotation from shapely_utils.ShapelyAnnotation.\n\n    Args:\n        annotation: shapely_utils.ShapelyAnnotation\n        category_id: int\n            ID of the object category\n        category_name: str\n            Name of the object category\n        full_shape: List\n            Size of the full image, should be in the form of [height, width]\n        shift_amount: List\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n    \"\"\"\n    return cls(\n        category_id=category_id,\n        segmentation=annotation.to_coco_segmentation(),\n        category_name=category_name,\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre> <code></code> <code>to_coco_annotation()</code> \u00b6 <p>Returns sahi.utils.coco.CocoAnnotation representation of ObjectAnnotation.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_coco_annotation(self) -&gt; CocoAnnotation:\n    \"\"\"Returns sahi.utils.coco.CocoAnnotation representation of ObjectAnnotation.\"\"\"\n    if self.mask:\n        coco_annotation = CocoAnnotation.from_coco_segmentation(\n            segmentation=self.mask.segmentation,\n            category_id=self.category.id,\n            category_name=self.category.name,\n        )\n    else:\n        coco_annotation = CocoAnnotation.from_coco_bbox(\n            bbox=self.bbox.to_xywh(),\n            category_id=self.category.id,\n            category_name=self.category.name,\n        )\n    return coco_annotation\n</code></pre> <code></code> <code>to_coco_prediction()</code> \u00b6 <p>Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_coco_prediction(self) -&gt; CocoPrediction:\n    \"\"\"Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.\"\"\"\n    if self.mask:\n        coco_prediction = CocoPrediction.from_coco_segmentation(\n            segmentation=self.mask.segmentation,\n            category_id=self.category.id,\n            category_name=self.category.name,\n            score=1,\n        )\n    else:\n        coco_prediction = CocoPrediction.from_coco_bbox(\n            bbox=self.bbox.to_xywh(),\n            category_id=self.category.id,\n            category_name=self.category.name,\n            score=1,\n        )\n    return coco_prediction\n</code></pre> <code></code> <code>to_imantics_annotation()</code> \u00b6 <p>Returns imantics.annotation.Annotation representation of ObjectAnnotation.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_imantics_annotation(self):\n    \"\"\"Returns imantics.annotation.Annotation representation of ObjectAnnotation.\"\"\"\n    try:\n        import imantics\n    except ImportError:\n        raise ImportError('Please run \"pip install -U imantics\" to install imantics first for imantics conversion.')\n\n    imantics_category = imantics.Category(id=self.category.id, name=self.category.name)\n    if self.mask is not None:\n        imantics_mask = imantics.Mask.create(self.mask.bool_mask)\n        imantics_annotation = imantics.annotation.Annotation.from_mask(\n            mask=imantics_mask, category=imantics_category\n        )\n    else:\n        imantics_bbox = imantics.BBox.create(self.bbox.to_xyxy())\n        imantics_annotation = imantics.annotation.Annotation.from_bbox(\n            bbox=imantics_bbox, category=imantics_category\n        )\n    return imantics_annotation\n</code></pre> <code></code> <code>to_shapely_annotation()</code> \u00b6 <p>Returns sahi.utils.shapely.ShapelyAnnotation representation of ObjectAnnotation.</p> Source code in <code>sahi/annotation.py</code> <pre><code>def to_shapely_annotation(self) -&gt; ShapelyAnnotation:\n    \"\"\"Returns sahi.utils.shapely.ShapelyAnnotation representation of ObjectAnnotation.\"\"\"\n    if self.mask:\n        shapely_annotation = ShapelyAnnotation.from_coco_segmentation(\n            segmentation=self.mask.segmentation,\n        )\n    else:\n        shapely_annotation = ShapelyAnnotation.from_coco_bbox(\n            bbox=self.bbox.to_xywh(),\n        )\n    return shapely_annotation\n</code></pre>"},{"location":"api/#sahi.annotation-functions","title":"Functions","text":""},{"location":"api/#sahi.auto_model","title":"<code>auto_model</code>","text":""},{"location":"api/#sahi.auto_model-classes","title":"Classes","text":""},{"location":"api/#sahi.auto_model.AutoDetectionModel","title":"<code>AutoDetectionModel</code>","text":"Source code in <code>sahi/auto_model.py</code> <pre><code>class AutoDetectionModel:\n    @staticmethod\n    def from_pretrained(\n        model_type: str,\n        model_path: str | None = None,\n        model: Any | None = None,\n        config_path: str | None = None,\n        device: str | None = None,\n        mask_threshold: float = 0.5,\n        confidence_threshold: float = 0.3,\n        category_mapping: dict | None = None,\n        category_remapping: dict | None = None,\n        load_at_init: bool = True,\n        image_size: int | None = None,\n        **kwargs,\n    ) -&gt; DetectionModel:\n        \"\"\"Loads a DetectionModel from given path.\n\n        Args:\n            model_type: str\n                Name of the detection framework (example: \"ultralytics\", \"huggingface\", \"torchvision\")\n            model_path: str\n                Path of the detection model (ex. 'model.pt')\n            model: Any\n                A pre-initialized model instance, if available\n            config_path: str\n                Path of the config file (ex. 'mmdet/configs/cascade_rcnn_r50_fpn_1x.py')\n            device: str\n                Device, \"cpu\" or \"cuda:0\"\n            mask_threshold: float\n                Value to threshold mask pixels, should be between 0 and 1\n            confidence_threshold: float\n                All predictions with score &lt; confidence_threshold will be discarded\n            category_mapping: dict: str to str\n                Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n            category_remapping: dict: str to int\n                Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n            load_at_init: bool\n                If True, automatically loads the model at initialization\n            image_size: int\n                Inference input size.\n\n        Returns:\n            Returns an instance of a DetectionModel\n\n        Raises:\n            ImportError: If given {model_type} framework is not installed\n        \"\"\"\n        if model_type in ULTRALYTICS_MODEL_NAMES:\n            model_type = \"ultralytics\"\n        model_class_name = MODEL_TYPE_TO_MODEL_CLASS_NAME[model_type]\n        DetectionModel = import_model_class(model_type, model_class_name)\n\n        return DetectionModel(\n            model_path=model_path,\n            model=model,\n            config_path=config_path,\n            device=device,\n            mask_threshold=mask_threshold,\n            confidence_threshold=confidence_threshold,\n            category_mapping=category_mapping,\n            category_remapping=category_remapping,\n            load_at_init=load_at_init,\n            image_size=image_size,\n            **kwargs,\n        )\n</code></pre> Functions\u00b6 <code></code> <code>from_pretrained(model_type, model_path=None, model=None, config_path=None, device=None, mask_threshold=0.5, confidence_threshold=0.3, category_mapping=None, category_remapping=None, load_at_init=True, image_size=None, **kwargs)</code> <code>staticmethod</code> \u00b6 <p>Loads a DetectionModel from given path.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> \u00b6 <code>str</code> <p>str Name of the detection framework (example: \"ultralytics\", \"huggingface\", \"torchvision\")</p> required <code>model_path</code> \u00b6 <code>str | None</code> <p>str Path of the detection model (ex. 'model.pt')</p> <code>None</code> <code>model</code> \u00b6 <code>Any | None</code> <p>Any A pre-initialized model instance, if available</p> <code>None</code> <code>config_path</code> \u00b6 <code>str | None</code> <p>str Path of the config file (ex. 'mmdet/configs/cascade_rcnn_r50_fpn_1x.py')</p> <code>None</code> <code>device</code> \u00b6 <code>str | None</code> <p>str Device, \"cpu\" or \"cuda:0\"</p> <code>None</code> <code>mask_threshold</code> \u00b6 <code>float</code> <p>float Value to threshold mask pixels, should be between 0 and 1</p> <code>0.5</code> <code>confidence_threshold</code> \u00b6 <code>float</code> <p>float All predictions with score &lt; confidence_threshold will be discarded</p> <code>0.3</code> <code>category_mapping</code> \u00b6 <code>dict | None</code> <p>dict: str to str Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}</p> <code>None</code> <code>category_remapping</code> \u00b6 <code>dict | None</code> <p>dict: str to int Remap category ids based on category names, after performing inference e.g. {\"car\": 3}</p> <code>None</code> <code>load_at_init</code> \u00b6 <code>bool</code> <p>bool If True, automatically loads the model at initialization</p> <code>True</code> <code>image_size</code> \u00b6 <code>int | None</code> <p>int Inference input size.</p> <code>None</code> <p>Returns:</p> Type Description <code>DetectionModel</code> <p>Returns an instance of a DetectionModel</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If given {model_type} framework is not installed</p> Source code in <code>sahi/auto_model.py</code> <pre><code>@staticmethod\ndef from_pretrained(\n    model_type: str,\n    model_path: str | None = None,\n    model: Any | None = None,\n    config_path: str | None = None,\n    device: str | None = None,\n    mask_threshold: float = 0.5,\n    confidence_threshold: float = 0.3,\n    category_mapping: dict | None = None,\n    category_remapping: dict | None = None,\n    load_at_init: bool = True,\n    image_size: int | None = None,\n    **kwargs,\n) -&gt; DetectionModel:\n    \"\"\"Loads a DetectionModel from given path.\n\n    Args:\n        model_type: str\n            Name of the detection framework (example: \"ultralytics\", \"huggingface\", \"torchvision\")\n        model_path: str\n            Path of the detection model (ex. 'model.pt')\n        model: Any\n            A pre-initialized model instance, if available\n        config_path: str\n            Path of the config file (ex. 'mmdet/configs/cascade_rcnn_r50_fpn_1x.py')\n        device: str\n            Device, \"cpu\" or \"cuda:0\"\n        mask_threshold: float\n            Value to threshold mask pixels, should be between 0 and 1\n        confidence_threshold: float\n            All predictions with score &lt; confidence_threshold will be discarded\n        category_mapping: dict: str to str\n            Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n        category_remapping: dict: str to int\n            Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n        load_at_init: bool\n            If True, automatically loads the model at initialization\n        image_size: int\n            Inference input size.\n\n    Returns:\n        Returns an instance of a DetectionModel\n\n    Raises:\n        ImportError: If given {model_type} framework is not installed\n    \"\"\"\n    if model_type in ULTRALYTICS_MODEL_NAMES:\n        model_type = \"ultralytics\"\n    model_class_name = MODEL_TYPE_TO_MODEL_CLASS_NAME[model_type]\n    DetectionModel = import_model_class(model_type, model_class_name)\n\n    return DetectionModel(\n        model_path=model_path,\n        model=model,\n        config_path=config_path,\n        device=device,\n        mask_threshold=mask_threshold,\n        confidence_threshold=confidence_threshold,\n        category_mapping=category_mapping,\n        category_remapping=category_remapping,\n        load_at_init=load_at_init,\n        image_size=image_size,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#sahi.auto_model-functions","title":"Functions","text":""},{"location":"api/#sahi.cli","title":"<code>cli</code>","text":""},{"location":"api/#sahi.cli-functions","title":"Functions","text":""},{"location":"api/#sahi.cli.app","title":"<code>app()</code>","text":"<p>Cli app.</p> Source code in <code>sahi/cli.py</code> <pre><code>def app() -&gt; None:\n    \"\"\"Cli app.\"\"\"\n    fire.Fire(sahi_app)\n</code></pre>"},{"location":"api/#sahi.logger","title":"<code>logger</code>","text":""},{"location":"api/#sahi.logger-classes","title":"Classes","text":""},{"location":"api/#sahi.logger.BaseSahiLogger","title":"<code>BaseSahiLogger</code>","text":"<p>               Bases: <code>Logger</code>, <code>ABC</code></p> Source code in <code>sahi/logger.py</code> <pre><code>class BaseSahiLogger(logging.Logger, ABC):\n    @abstractmethod\n    def pkg_info(self, message: str, *args, **kws) -&gt; None:\n        \"\"\"Log a package info message at PKG_INFO level.\"\"\"\n        raise NotImplementedError\n</code></pre> Functions\u00b6 <code></code> <code>pkg_info(message, *args, **kws)</code> <code>abstractmethod</code> \u00b6 <p>Log a package info message at PKG_INFO level.</p> Source code in <code>sahi/logger.py</code> <pre><code>@abstractmethod\ndef pkg_info(self, message: str, *args, **kws) -&gt; None:\n    \"\"\"Log a package info message at PKG_INFO level.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#sahi.models","title":"<code>models</code>","text":""},{"location":"api/#sahi.models-modules","title":"Modules","text":""},{"location":"api/#sahi.models.base","title":"<code>base</code>","text":"Classes\u00b6 <code>DetectionModel</code> \u00b6 Source code in <code>sahi/models/base.py</code> <pre><code>class DetectionModel:\n    required_packages: list[str] | None = None\n\n    def __init__(\n        self,\n        model_path: str | None = None,\n        model: Any | None = None,\n        config_path: str | None = None,\n        device: str | None = None,\n        mask_threshold: float = 0.5,\n        confidence_threshold: float = 0.3,\n        category_mapping: dict | None = None,\n        category_remapping: dict | None = None,\n        load_at_init: bool = True,\n        image_size: int | None = None,\n    ):\n        \"\"\"Init object detection/instance segmentation model.\n\n        Args:\n            model_path: str\n                Path for the instance segmentation model weight\n            config_path: str\n                Path for the mmdetection instance segmentation model config file\n            device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n            mask_threshold: float\n                Value to threshold mask pixels, should be between 0 and 1\n            confidence_threshold: float\n                All predictions with score &lt; confidence_threshold will be discarded\n            category_mapping: dict: str to str\n                Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n            category_remapping: dict: str to int\n                Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n            load_at_init: bool\n                If True, automatically loads the model at initialization\n            image_size: int\n                Inference input size.\n        \"\"\"\n\n        self.model_path = model_path\n        self.config_path = config_path\n        self.model = None\n        self.mask_threshold = mask_threshold\n        self.confidence_threshold = confidence_threshold\n        self.category_mapping = category_mapping\n        self.category_remapping = category_remapping\n        self.image_size = image_size\n        self._original_predictions = None\n        self._object_prediction_list_per_image = None\n        self.set_device(device)\n\n        # automatically ensure dependencies\n        self.check_dependencies()\n\n        # automatically load model if load_at_init is True\n        if load_at_init:\n            if model:\n                self.set_model(model)\n            else:\n                self.load_model()\n\n    def check_dependencies(self, packages: list[str] | None = None) -&gt; None:\n        \"\"\"Ensures required dependencies are installed.\n\n        If 'packages' is None, uses self.required_packages. Subclasses may still call with a custom list for dynamic\n        needs.\n        \"\"\"\n        pkgs = packages if packages is not None else getattr(self, \"required_packages\", [])\n        if pkgs:\n            check_requirements(pkgs)\n\n    def load_model(self):\n        \"\"\"This function should be implemented in a way that detection model should be initialized and set to\n        self.model.\n\n        (self.model_path, self.config_path, and self.device should be utilized)\n        \"\"\"\n        raise NotImplementedError()\n\n    def set_model(self, model: Any, **kwargs):\n        \"\"\"\n        This function should be implemented to instantiate a DetectionModel out of an already loaded model\n        Args:\n            model: Any\n                Loaded model\n        \"\"\"\n        raise NotImplementedError()\n\n    def set_device(self, device: str | None = None):\n        \"\"\"Sets the device pytorch should use for the model.\n\n        Args:\n            device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n        \"\"\"\n\n        self.device = select_device(device)\n\n    def unload_model(self):\n        \"\"\"Unloads the model from CPU/GPU.\"\"\"\n        self.model = None\n        empty_cuda_cache()\n\n    def perform_inference(self, image: np.ndarray):\n        \"\"\"This function should be implemented in a way that prediction should be performed using self.model and the\n        prediction result should be set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"This function should be implemented in a way that self._original_predictions should be converted to a list of\n        prediction.ObjectPrediction and set to self._object_prediction_list.\n\n        self.mask_threshold can also be utilized.\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        raise NotImplementedError()\n\n    def _apply_category_remapping(self):\n        \"\"\"Applies category remapping based on mapping given in self.category_remapping.\"\"\"\n        # confirm self.category_remapping is not None\n        if self.category_remapping is None:\n            raise ValueError(\"self.category_remapping cannot be None\")\n        # remap categories\n        if not isinstance(self._object_prediction_list_per_image, list):\n            logger.error(\n                f\"Unknown type for self._object_prediction_list_per_image: \"\n                f\"{type(self._object_prediction_list_per_image)}\"\n            )\n            return\n        for object_prediction_list in self._object_prediction_list_per_image:  # type: ignore\n            for object_prediction in object_prediction_list:\n                old_category_id_str = str(object_prediction.category.id)\n                new_category_id_int = self.category_remapping[old_category_id_str]\n                object_prediction.category = Category(id=new_category_id_int, name=object_prediction.category.name)\n\n    def convert_original_predictions(\n        self,\n        shift_amount: list[list[int]] | None = [[0, 0]],\n        full_shape: list[list[int]] | None = None,\n    ):\n        \"\"\"Converts original predictions of the detection model to a list of prediction.ObjectPrediction object.\n\n        Should be called after perform_inference().\n        Args:\n            shift_amount: list\n                To shift the box and mask predictions from sliced image to full sized image,\n                    should be in the form of [shift_x, shift_y]\n            full_shape: list\n                Size of the full image after shifting, should be in the form of [height, width]\n        \"\"\"\n        self._create_object_prediction_list_from_original_predictions(\n            shift_amount_list=shift_amount,\n            full_shape_list=full_shape,\n        )\n        if self.category_remapping:\n            self._apply_category_remapping()\n\n    @property\n    def object_prediction_list(self) -&gt; list[list[ObjectPrediction]]:\n        if self._object_prediction_list_per_image is None:\n            return []\n        if len(self._object_prediction_list_per_image) == 0:\n            return []\n        return self._object_prediction_list_per_image[0]\n\n    @property\n    def object_prediction_list_per_image(self) -&gt; list[list[ObjectPrediction]]:\n        return self._object_prediction_list_per_image or []\n\n    @property\n    def original_predictions(self):\n        return self._original_predictions\n</code></pre> Functions\u00b6 <code></code> <code>__init__(model_path=None, model=None, config_path=None, device=None, mask_threshold=0.5, confidence_threshold=0.3, category_mapping=None, category_remapping=None, load_at_init=True, image_size=None)</code> \u00b6 <p>Init object detection/instance segmentation model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> \u00b6 <code>str | None</code> <p>str Path for the instance segmentation model weight</p> <code>None</code> <code>config_path</code> \u00b6 <code>str | None</code> <p>str Path for the mmdetection instance segmentation model config file</p> <code>None</code> <code>device</code> \u00b6 <code>str | None</code> <p>Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.</p> <code>None</code> <code>mask_threshold</code> \u00b6 <code>float</code> <p>float Value to threshold mask pixels, should be between 0 and 1</p> <code>0.5</code> <code>confidence_threshold</code> \u00b6 <code>float</code> <p>float All predictions with score &lt; confidence_threshold will be discarded</p> <code>0.3</code> <code>category_mapping</code> \u00b6 <code>dict | None</code> <p>dict: str to str Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}</p> <code>None</code> <code>category_remapping</code> \u00b6 <code>dict | None</code> <p>dict: str to int Remap category ids based on category names, after performing inference e.g. {\"car\": 3}</p> <code>None</code> <code>load_at_init</code> \u00b6 <code>bool</code> <p>bool If True, automatically loads the model at initialization</p> <code>True</code> <code>image_size</code> \u00b6 <code>int | None</code> <p>int Inference input size.</p> <code>None</code> Source code in <code>sahi/models/base.py</code> <pre><code>def __init__(\n    self,\n    model_path: str | None = None,\n    model: Any | None = None,\n    config_path: str | None = None,\n    device: str | None = None,\n    mask_threshold: float = 0.5,\n    confidence_threshold: float = 0.3,\n    category_mapping: dict | None = None,\n    category_remapping: dict | None = None,\n    load_at_init: bool = True,\n    image_size: int | None = None,\n):\n    \"\"\"Init object detection/instance segmentation model.\n\n    Args:\n        model_path: str\n            Path for the instance segmentation model weight\n        config_path: str\n            Path for the mmdetection instance segmentation model config file\n        device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n        mask_threshold: float\n            Value to threshold mask pixels, should be between 0 and 1\n        confidence_threshold: float\n            All predictions with score &lt; confidence_threshold will be discarded\n        category_mapping: dict: str to str\n            Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n        category_remapping: dict: str to int\n            Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n        load_at_init: bool\n            If True, automatically loads the model at initialization\n        image_size: int\n            Inference input size.\n    \"\"\"\n\n    self.model_path = model_path\n    self.config_path = config_path\n    self.model = None\n    self.mask_threshold = mask_threshold\n    self.confidence_threshold = confidence_threshold\n    self.category_mapping = category_mapping\n    self.category_remapping = category_remapping\n    self.image_size = image_size\n    self._original_predictions = None\n    self._object_prediction_list_per_image = None\n    self.set_device(device)\n\n    # automatically ensure dependencies\n    self.check_dependencies()\n\n    # automatically load model if load_at_init is True\n    if load_at_init:\n        if model:\n            self.set_model(model)\n        else:\n            self.load_model()\n</code></pre> <code></code> <code>check_dependencies(packages=None)</code> \u00b6 <p>Ensures required dependencies are installed.</p> <p>If 'packages' is None, uses self.required_packages. Subclasses may still call with a custom list for dynamic needs.</p> Source code in <code>sahi/models/base.py</code> <pre><code>def check_dependencies(self, packages: list[str] | None = None) -&gt; None:\n    \"\"\"Ensures required dependencies are installed.\n\n    If 'packages' is None, uses self.required_packages. Subclasses may still call with a custom list for dynamic\n    needs.\n    \"\"\"\n    pkgs = packages if packages is not None else getattr(self, \"required_packages\", [])\n    if pkgs:\n        check_requirements(pkgs)\n</code></pre> <code></code> <code>convert_original_predictions(shift_amount=[[0, 0]], full_shape=None)</code> \u00b6 <p>Converts original predictions of the detection model to a list of prediction.ObjectPrediction object.</p> <p>Should be called after perform_inference(). Args:     shift_amount: list         To shift the box and mask predictions from sliced image to full sized image,             should be in the form of [shift_x, shift_y]     full_shape: list         Size of the full image after shifting, should be in the form of [height, width]</p> Source code in <code>sahi/models/base.py</code> <pre><code>def convert_original_predictions(\n    self,\n    shift_amount: list[list[int]] | None = [[0, 0]],\n    full_shape: list[list[int]] | None = None,\n):\n    \"\"\"Converts original predictions of the detection model to a list of prediction.ObjectPrediction object.\n\n    Should be called after perform_inference().\n    Args:\n        shift_amount: list\n            To shift the box and mask predictions from sliced image to full sized image,\n                should be in the form of [shift_x, shift_y]\n        full_shape: list\n            Size of the full image after shifting, should be in the form of [height, width]\n    \"\"\"\n    self._create_object_prediction_list_from_original_predictions(\n        shift_amount_list=shift_amount,\n        full_shape_list=full_shape,\n    )\n    if self.category_remapping:\n        self._apply_category_remapping()\n</code></pre> <code></code> <code>load_model()</code> \u00b6 <p>This function should be implemented in a way that detection model should be initialized and set to self.model.</p> <p>(self.model_path, self.config_path, and self.device should be utilized)</p> Source code in <code>sahi/models/base.py</code> <pre><code>def load_model(self):\n    \"\"\"This function should be implemented in a way that detection model should be initialized and set to\n    self.model.\n\n    (self.model_path, self.config_path, and self.device should be utilized)\n    \"\"\"\n    raise NotImplementedError()\n</code></pre> <code></code> <code>perform_inference(image)</code> \u00b6 <p>This function should be implemented in a way that prediction should be performed using self.model and the prediction result should be set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted.</p> required Source code in <code>sahi/models/base.py</code> <pre><code>def perform_inference(self, image: np.ndarray):\n    \"\"\"This function should be implemented in a way that prediction should be performed using self.model and the\n    prediction result should be set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre> <code></code> <code>set_device(device=None)</code> \u00b6 <p>Sets the device pytorch should use for the model.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> \u00b6 <code>str | None</code> <p>Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.</p> <code>None</code> Source code in <code>sahi/models/base.py</code> <pre><code>def set_device(self, device: str | None = None):\n    \"\"\"Sets the device pytorch should use for the model.\n\n    Args:\n        device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n    \"\"\"\n\n    self.device = select_device(device)\n</code></pre> <code></code> <code>set_model(model, **kwargs)</code> \u00b6 <p>This function should be implemented to instantiate a DetectionModel out of an already loaded model Args:     model: Any         Loaded model</p> Source code in <code>sahi/models/base.py</code> <pre><code>def set_model(self, model: Any, **kwargs):\n    \"\"\"\n    This function should be implemented to instantiate a DetectionModel out of an already loaded model\n    Args:\n        model: Any\n            Loaded model\n    \"\"\"\n    raise NotImplementedError()\n</code></pre> <code></code> <code>unload_model()</code> \u00b6 <p>Unloads the model from CPU/GPU.</p> Source code in <code>sahi/models/base.py</code> <pre><code>def unload_model(self):\n    \"\"\"Unloads the model from CPU/GPU.\"\"\"\n    self.model = None\n    empty_cuda_cache()\n</code></pre> Functions\u00b6"},{"location":"api/#sahi.models.detectron2","title":"<code>detectron2</code>","text":"Classes\u00b6 <code>Detectron2DetectionModel</code> \u00b6 <p>               Bases: <code>DetectionModel</code></p> Source code in <code>sahi/models/detectron2.py</code> <pre><code>class Detectron2DetectionModel(DetectionModel):\n    def __init__(self, *args, **kwargs):\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"torch\", \"detectron2\"]\n        super().__init__(*args, **kwargs)\n\n    def load_model(self):\n        from detectron2.config import get_cfg\n        from detectron2.data import MetadataCatalog\n        from detectron2.engine import DefaultPredictor\n        from detectron2.model_zoo import model_zoo\n\n        cfg = get_cfg()\n\n        try:  # try to load from model zoo\n            config_file = model_zoo.get_config_file(self.config_path)\n            cfg.set_new_allowed(True)\n            cfg.merge_from_file(config_file)\n            cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(self.config_path)\n        except Exception as e:  # try to load from local\n            print(e)\n            if self.config_path is not None:\n                cfg.set_new_allowed(True)\n                cfg.merge_from_file(self.config_path)\n            cfg.MODEL.WEIGHTS = self.model_path\n\n        # set model device\n        cfg.MODEL.DEVICE = self.device.type\n        # set input image size\n        if self.image_size is not None:\n            cfg.INPUT.MIN_SIZE_TEST = self.image_size\n            cfg.INPUT.MAX_SIZE_TEST = self.image_size\n        # init predictor\n        model = DefaultPredictor(cfg)\n\n        self.model = model\n\n        # detectron2 category mapping\n        if self.category_mapping is None:\n            try:  # try to parse category names from metadata\n                metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n                category_names = metadata.thing_classes\n                self.category_names = category_names\n                self.category_mapping = {\n                    str(ind): category_name for ind, category_name in enumerate(self.category_names)\n                }\n            except Exception as e:\n                logger.warning(e)\n                # https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html#update-the-config-for-new-datasets\n                if cfg.MODEL.META_ARCHITECTURE == \"RetinaNet\":\n                    num_categories = cfg.MODEL.RETINANET.NUM_CLASSES\n                else:  # fasterrcnn/maskrcnn etc\n                    num_categories = cfg.MODEL.ROI_HEADS.NUM_CLASSES\n                self.category_names = [str(category_id) for category_id in range(num_categories)]\n                self.category_mapping = {\n                    str(ind): category_name for ind, category_name in enumerate(self.category_names)\n                }\n        else:\n            self.category_names = list(self.category_mapping.values())\n\n    def perform_inference(self, image: np.ndarray):\n        \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        \"\"\"\n\n        # Confirm model is loaded\n        if self.model is None:\n            raise RuntimeError(\"Model is not loaded, load it by calling .load_model()\")\n\n        if isinstance(image, np.ndarray) and self.model.input_format == \"BGR\":\n            # convert RGB image to BGR format\n            image = image[:, :, ::-1]\n\n        prediction_result = self.model(image)\n\n        self._original_predictions = prediction_result\n\n    @property\n    def num_categories(self):\n        \"\"\"Returns number of categories.\"\"\"\n        num_categories = len(self.category_mapping)\n        return num_categories\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n        self._object_prediction_list_per_image.\n\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n\n        original_predictions = self._original_predictions\n\n        # compatilibty for sahi v0.8.15\n        if isinstance(shift_amount_list[0], int):\n            shift_amount_list = [shift_amount_list]\n        if full_shape_list is not None and isinstance(full_shape_list[0], int):\n            full_shape_list = [full_shape_list]\n\n        # detectron2 DefaultPredictor supports single image\n        shift_amount = shift_amount_list[0]\n        full_shape = None if full_shape_list is None else full_shape_list[0]\n\n        # parse boxes, masks, scores, category_ids from predictions\n        boxes = original_predictions[\"instances\"].pred_boxes.tensor\n        scores = original_predictions[\"instances\"].scores\n        category_ids = original_predictions[\"instances\"].pred_classes\n\n        # check if predictions contain mask\n        try:\n            masks = original_predictions[\"instances\"].pred_masks\n        except AttributeError:\n            masks = None\n\n        # filter predictions with low confidence\n        high_confidence_mask = scores &gt;= self.confidence_threshold\n        boxes = boxes[high_confidence_mask]\n        scores = scores[high_confidence_mask]\n        category_ids = category_ids[high_confidence_mask]\n        if masks is not None:\n            masks = masks[high_confidence_mask]\n        if masks is not None:\n            object_prediction_list = [\n                ObjectPrediction(\n                    bbox=box.tolist() if mask is None else None,\n                    segmentation=(\n                        get_coco_segmentation_from_bool_mask(mask.detach().cpu().numpy()) if mask is not None else None\n                    ),\n                    category_id=category_id.item(),\n                    category_name=self.category_mapping[str(category_id.item())],\n                    shift_amount=shift_amount,\n                    score=score.item(),\n                    full_shape=full_shape,\n                )\n                for box, score, category_id, mask in zip(boxes, scores, category_ids, masks)\n                if mask is None or get_bbox_from_bool_mask(mask.detach().cpu().numpy()) is not None\n            ]\n        else:\n            object_prediction_list = [\n                ObjectPrediction(\n                    bbox=box.tolist(),\n                    segmentation=None,\n                    category_id=category_id.item(),\n                    category_name=self.category_mapping[str(category_id.item())],\n                    shift_amount=shift_amount,\n                    score=score.item(),\n                    full_shape=full_shape,\n                )\n                for box, score, category_id in zip(boxes, scores, category_ids)\n            ]\n\n        # detectron2 DefaultPredictor supports single image\n        object_prediction_list_per_image = [object_prediction_list]\n\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre> Attributes\u00b6 <code></code> <code>num_categories</code> <code>property</code> \u00b6 <p>Returns number of categories.</p> Functions\u00b6 <code></code> <code>perform_inference(image)</code> \u00b6 <p>Prediction is performed using self.model and the prediction result is set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.</p> required Source code in <code>sahi/models/detectron2.py</code> <pre><code>def perform_inference(self, image: np.ndarray):\n    \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n    \"\"\"\n\n    # Confirm model is loaded\n    if self.model is None:\n        raise RuntimeError(\"Model is not loaded, load it by calling .load_model()\")\n\n    if isinstance(image, np.ndarray) and self.model.input_format == \"BGR\":\n        # convert RGB image to BGR format\n        image = image[:, :, ::-1]\n\n    prediction_result = self.model(image)\n\n    self._original_predictions = prediction_result\n</code></pre> Functions\u00b6"},{"location":"api/#sahi.models.huggingface","title":"<code>huggingface</code>","text":"Classes\u00b6 <code>HuggingfaceDetectionModel</code> \u00b6 <p>               Bases: <code>DetectionModel</code></p> Source code in <code>sahi/models/huggingface.py</code> <pre><code>class HuggingfaceDetectionModel(DetectionModel):\n    def __init__(\n        self,\n        model_path: str | None = None,\n        model: Any | None = None,\n        processor: Any | None = None,\n        config_path: str | None = None,\n        device: str | None = None,\n        mask_threshold: float = 0.5,\n        confidence_threshold: float = 0.3,\n        category_mapping: dict | None = None,\n        category_remapping: dict | None = None,\n        load_at_init: bool = True,\n        image_size: int | None = None,\n        token: str | None = None,\n    ):\n        self._processor = processor\n        self._image_shapes: list = []\n        self._token = token\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"torch\", \"transformers\"]\n        ensure_package_minimum_version(\"transformers\", \"4.42.0\")\n        super().__init__(\n            model_path,\n            model,\n            config_path,\n            device,\n            mask_threshold,\n            confidence_threshold,\n            category_mapping,\n            category_remapping,\n            load_at_init,\n            image_size,\n        )\n\n    @property\n    def processor(self):\n        return self._processor\n\n    @property\n    def image_shapes(self):\n        return self._image_shapes\n\n    @property\n    def num_categories(self) -&gt; int:\n        \"\"\"Returns number of categories.\"\"\"\n        return self.model.config.num_labels\n\n    def load_model(self):\n        from transformers import AutoModelForObjectDetection, AutoProcessor\n\n        hf_token = os.getenv(\"HF_TOKEN\", self._token)\n        model = AutoModelForObjectDetection.from_pretrained(self.model_path, token=hf_token)\n        if self.image_size is not None:\n            if model.base_model_prefix == \"rt_detr_v2\":\n                size = {\"height\": self.image_size, \"width\": self.image_size}\n            else:\n                size = {\"shortest_edge\": self.image_size, \"longest_edge\": None}\n            # use_fast=True raises error: AttributeError: 'SizeDict' object has no attribute 'keys'\n            processor = AutoProcessor.from_pretrained(\n                self.model_path, size=size, do_resize=True, use_fast=False, token=hf_token\n            )\n        else:\n            processor = AutoProcessor.from_pretrained(self.model_path, use_fast=False, token=hf_token)\n        self.set_model(model, processor)\n\n    def set_model(self, model: Any, processor: Any = None, **kwargs):\n        processor = processor or self.processor\n        if processor is None:\n            raise ValueError(f\"'processor' is required to be set, got {processor}.\")\n        elif \"ObjectDetection\" not in model.__class__.__name__ or \"ImageProcessor\" not in processor.__class__.__name__:\n            raise ValueError(\n                \"Given 'model' is not an ObjectDetectionModel or 'processor' is not a valid ImageProcessor.\"\n            )\n        self.model = model\n        self.model.to(self.device)\n        self._processor = processor\n        self.category_mapping = self.model.config.id2label\n\n    def perform_inference(self, image: list | np.ndarray):\n        \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        \"\"\"\n        import torch\n\n        # Confirm model is loaded\n        if self.model is None or self.processor is None:\n            raise RuntimeError(\"Model is not loaded, load it by calling .load_model()\")\n\n        with torch.no_grad():\n            inputs = self.processor(images=image, return_tensors=\"pt\")\n            inputs[\"pixel_values\"] = inputs.pixel_values.to(self.device)\n            if hasattr(inputs, \"pixel_mask\"):\n                inputs[\"pixel_mask\"] = inputs.pixel_mask.to(self.device)\n            outputs = self.model(**inputs)\n\n        if isinstance(image, list):\n            self._image_shapes = [img.shape for img in image]\n        else:\n            self._image_shapes = [image.shape]\n        self._original_predictions = outputs\n\n    def get_valid_predictions(self, logits, pred_boxes) -&gt; tuple:\n        \"\"\"\n        Args:\n            logits: torch.Tensor\n            pred_boxes: torch.Tensor\n        Returns:\n            scores: torch.Tensor\n            cat_ids: torch.Tensor\n            boxes: torch.Tensor\n        \"\"\"\n        import torch\n\n        probs = logits.softmax(-1)\n        scores = probs.max(-1).values\n        cat_ids = probs.argmax(-1)\n        valid_detections = torch.where(cat_ids &lt; self.num_categories, 1, 0)\n        valid_confidences = torch.where(scores &gt;= self.confidence_threshold, 1, 0)\n        valid_mask = valid_detections.logical_and(valid_confidences)\n        scores = scores[valid_mask]\n        cat_ids = cat_ids[valid_mask]\n        boxes = pred_boxes[valid_mask]\n        return scores, cat_ids, boxes\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n        self._object_prediction_list_per_image.\n\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        original_predictions = self._original_predictions\n\n        # compatibility for sahi v0.8.15\n        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n        full_shape_list = fix_full_shape_list(full_shape_list)\n\n        n_image = original_predictions.logits.shape[0]\n        object_prediction_list_per_image = []\n        for image_ind in range(n_image):\n            image_height, image_width, _ = self.image_shapes[image_ind]\n            scores, cat_ids, boxes = self.get_valid_predictions(\n                logits=original_predictions.logits[image_ind], pred_boxes=original_predictions.pred_boxes[image_ind]\n            )\n\n            # create object_prediction_list\n            object_prediction_list = []\n\n            shift_amount = shift_amount_list[image_ind]\n            full_shape = None if full_shape_list is None else full_shape_list[image_ind]\n\n            for ind in range(len(boxes)):\n                category_id = cat_ids[ind].item()\n                yolo_bbox = boxes[ind].tolist()\n                bbox = list(\n                    pbf.convert_bbox(\n                        yolo_bbox,\n                        from_type=\"yolo\",\n                        to_type=\"voc\",\n                        image_size=(image_width, image_height),\n                        return_values=True,\n                        strict=False,\n                    )\n                )\n\n                # fix negative box coords\n                bbox[0] = max(0, bbox[0])\n                bbox[1] = max(0, bbox[1])\n                bbox[2] = min(bbox[2], image_width)\n                bbox[3] = min(bbox[3], image_height)\n\n                object_prediction = ObjectPrediction(\n                    bbox=bbox,\n                    segmentation=None,\n                    category_id=category_id,\n                    category_name=self.category_mapping[category_id],\n                    shift_amount=shift_amount,\n                    score=scores[ind].item(),\n                    full_shape=full_shape,\n                )\n                object_prediction_list.append(object_prediction)\n            object_prediction_list_per_image.append(object_prediction_list)\n\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre> Attributes\u00b6 <code></code> <code>num_categories</code> <code>property</code> \u00b6 <p>Returns number of categories.</p> Functions\u00b6 <code></code> <code>get_valid_predictions(logits, pred_boxes)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>logits</code> \u00b6 <p>torch.Tensor</p> required <code>pred_boxes</code> \u00b6 <p>torch.Tensor</p> required <p>Returns:     scores: torch.Tensor     cat_ids: torch.Tensor     boxes: torch.Tensor</p> Source code in <code>sahi/models/huggingface.py</code> <pre><code>def get_valid_predictions(self, logits, pred_boxes) -&gt; tuple:\n    \"\"\"\n    Args:\n        logits: torch.Tensor\n        pred_boxes: torch.Tensor\n    Returns:\n        scores: torch.Tensor\n        cat_ids: torch.Tensor\n        boxes: torch.Tensor\n    \"\"\"\n    import torch\n\n    probs = logits.softmax(-1)\n    scores = probs.max(-1).values\n    cat_ids = probs.argmax(-1)\n    valid_detections = torch.where(cat_ids &lt; self.num_categories, 1, 0)\n    valid_confidences = torch.where(scores &gt;= self.confidence_threshold, 1, 0)\n    valid_mask = valid_detections.logical_and(valid_confidences)\n    scores = scores[valid_mask]\n    cat_ids = cat_ids[valid_mask]\n    boxes = pred_boxes[valid_mask]\n    return scores, cat_ids, boxes\n</code></pre> <code></code> <code>perform_inference(image)</code> \u00b6 <p>Prediction is performed using self.model and the prediction result is set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>list | ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.</p> required Source code in <code>sahi/models/huggingface.py</code> <pre><code>def perform_inference(self, image: list | np.ndarray):\n    \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n    \"\"\"\n    import torch\n\n    # Confirm model is loaded\n    if self.model is None or self.processor is None:\n        raise RuntimeError(\"Model is not loaded, load it by calling .load_model()\")\n\n    with torch.no_grad():\n        inputs = self.processor(images=image, return_tensors=\"pt\")\n        inputs[\"pixel_values\"] = inputs.pixel_values.to(self.device)\n        if hasattr(inputs, \"pixel_mask\"):\n            inputs[\"pixel_mask\"] = inputs.pixel_mask.to(self.device)\n        outputs = self.model(**inputs)\n\n    if isinstance(image, list):\n        self._image_shapes = [img.shape for img in image]\n    else:\n        self._image_shapes = [image.shape]\n    self._original_predictions = outputs\n</code></pre> Functions\u00b6"},{"location":"api/#sahi.models.mmdet","title":"<code>mmdet</code>","text":"Classes\u00b6 <code>DetInferencerWrapper</code> \u00b6 <p>               Bases: <code>DetInferencer</code></p> Source code in <code>sahi/models/mmdet.py</code> <pre><code>class DetInferencerWrapper(DetInferencer):\n    def __init__(\n        self,\n        model: ModelType | str | None = None,\n        weights: str | None = None,\n        device: str | None = None,\n        scope: str | None = \"mmdet\",\n        palette: str = \"none\",\n        image_size: int | None = None,\n    ) -&gt; None:\n        self.image_size = image_size\n        super().__init__(model, weights, device, scope, palette)\n\n    def __call__(self, images: list[np.ndarray], batch_size: int = 1) -&gt; dict:\n        \"\"\"\n        Emulate DetInferencer(images) without progressbar\n        Args:\n            images: list of np.ndarray\n                A list of numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n            batch_size: int\n                Inference batch size. Defaults to 1.\n        \"\"\"\n        inputs = self.preprocess(images, batch_size=batch_size)\n        results_dict = {\"predictions\": [], \"visualization\": []}\n        for _, data in inputs:\n            preds = self.forward(data)\n            results = self.postprocess(\n                preds,\n                visualization=None,\n                return_datasample=False,\n                print_result=False,\n                no_save_pred=True,\n                pred_out_dir=None,\n            )\n            results_dict[\"predictions\"].extend(results[\"predictions\"])\n        return results_dict\n\n    def _init_pipeline(self, cfg: ConfigType) -&gt; Compose:\n        \"\"\"Initialize the test pipeline.\"\"\"\n        pipeline_cfg = cfg.test_dataloader.dataset.pipeline\n\n        # For inference, the key of ``img_id`` is not used.\n        if \"meta_keys\" in pipeline_cfg[-1]:\n            pipeline_cfg[-1][\"meta_keys\"] = tuple(\n                meta_key for meta_key in pipeline_cfg[-1][\"meta_keys\"] if meta_key != \"img_id\"\n            )\n\n        load_img_idx = self._get_transform_idx(pipeline_cfg, \"LoadImageFromFile\")\n        if load_img_idx == -1:\n            raise ValueError(\"LoadImageFromFile is not found in the test pipeline\")\n        pipeline_cfg[load_img_idx][\"type\"] = \"mmdet.InferencerLoader\"\n\n        resize_idx = self._get_transform_idx(pipeline_cfg, \"Resize\")\n        if resize_idx == -1:\n            raise ValueError(\"Resize is not found in the test pipeline\")\n        if self.image_size is not None:\n            pipeline_cfg[resize_idx][\"scale\"] = (self.image_size, self.image_size)\n        return Compose(pipeline_cfg)\n</code></pre> Functions\u00b6 <code></code> <code>__call__(images, batch_size=1)</code> \u00b6 <p>Emulate DetInferencer(images) without progressbar Args:     images: list of np.ndarray         A list of numpy array that contains the image to be predicted. 3 channel image should be in RGB order.     batch_size: int         Inference batch size. Defaults to 1.</p> Source code in <code>sahi/models/mmdet.py</code> <pre><code>def __call__(self, images: list[np.ndarray], batch_size: int = 1) -&gt; dict:\n    \"\"\"\n    Emulate DetInferencer(images) without progressbar\n    Args:\n        images: list of np.ndarray\n            A list of numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        batch_size: int\n            Inference batch size. Defaults to 1.\n    \"\"\"\n    inputs = self.preprocess(images, batch_size=batch_size)\n    results_dict = {\"predictions\": [], \"visualization\": []}\n    for _, data in inputs:\n        preds = self.forward(data)\n        results = self.postprocess(\n            preds,\n            visualization=None,\n            return_datasample=False,\n            print_result=False,\n            no_save_pred=True,\n            pred_out_dir=None,\n        )\n        results_dict[\"predictions\"].extend(results[\"predictions\"])\n    return results_dict\n</code></pre> <code></code> <code>MmdetDetectionModel</code> \u00b6 <p>               Bases: <code>DetectionModel</code></p> Source code in <code>sahi/models/mmdet.py</code> <pre><code>class MmdetDetectionModel(DetectionModel):\n    def __init__(\n        self,\n        model_path: str | None = None,\n        model: Any | None = None,\n        config_path: str | None = None,\n        device: str | None = None,\n        mask_threshold: float = 0.5,\n        confidence_threshold: float = 0.3,\n        category_mapping: dict | None = None,\n        category_remapping: dict | None = None,\n        load_at_init: bool = True,\n        image_size: int | None = None,\n        scope: str = \"mmdet\",\n    ):\n        self.scope = scope\n        self.image_size = image_size\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"mmdet\", \"mmcv\", \"torch\"]\n        super().__init__(\n            model_path,\n            model,\n            config_path,\n            device,\n            mask_threshold,\n            confidence_threshold,\n            category_mapping,\n            category_remapping,\n            load_at_init,\n            image_size,\n        )\n\n    def load_model(self):\n        \"\"\"Detection model is initialized and set to self.model.\"\"\"\n\n        # create model\n        model = DetInferencerWrapper(\n            self.config_path, self.model_path, device=self.device, scope=self.scope, image_size=self.image_size\n        )\n\n        self.set_model(model)\n\n    def set_model(self, model: Any):\n        \"\"\"Sets the underlying MMDetection model.\n\n        Args:\n            model: Any\n                A MMDetection model\n        \"\"\"\n\n        # set self.model\n        self.model = model\n\n        # set category_mapping\n        if not self.category_mapping:\n            category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n            self.category_mapping = category_mapping\n\n    def perform_inference(self, image: np.ndarray):\n        \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        \"\"\"\n\n        # Confirm model is loaded\n        if self.model is None:\n            raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n\n        # Supports only batch of 1\n\n        # perform inference\n        if isinstance(image, np.ndarray):\n            # https://github.com/obss/sahi/issues/265\n            image = image[:, :, ::-1]\n        # compatibility with sahi v0.8.15\n        if not isinstance(image, list):\n            image_list = [image]\n        prediction_result = self.model(image_list)\n\n        self._original_predictions = prediction_result[\"predictions\"]\n\n    @property\n    def num_categories(self):\n        \"\"\"Returns number of categories.\"\"\"\n        return len(self.category_names)\n\n    @property\n    def has_mask(self):\n        \"\"\"Returns if model output contains segmentation mask.\n\n        Considers both single dataset and ConcatDataset scenarios.\n        \"\"\"\n\n        def check_pipeline_for_mask(pipeline):\n            return any(\n                isinstance(item, dict) and any(\"mask\" in key and value is True for key, value in item.items())\n                for item in pipeline\n            )\n\n        # Access the dataset from the configuration\n        dataset_config = self.model.cfg[\"train_dataloader\"][\"dataset\"]\n\n        if dataset_config[\"type\"] == \"ConcatDataset\":\n            # If using ConcatDataset, check each dataset individually\n            datasets = dataset_config[\"datasets\"]\n            for dataset in datasets:\n                if check_pipeline_for_mask(dataset[\"pipeline\"]):\n                    return True\n        else:\n            # Otherwise, assume a single dataset with its own pipeline\n            if check_pipeline_for_mask(dataset_config[\"pipeline\"]):\n                return True\n\n        return False\n\n    @property\n    def category_names(self):\n        classes = self.model.model.dataset_meta[\"classes\"]\n        if isinstance(classes, str):\n            # https://github.com/open-mmlab/mmdetection/pull/4973\n            return (classes,)\n        else:\n            return classes\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n        self._object_prediction_list_per_image.\n\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        try:\n            from pycocotools import mask as mask_utils\n\n            can_decode_rle = True\n        except ImportError:\n            can_decode_rle = False\n        original_predictions = self._original_predictions\n        category_mapping = self.category_mapping\n\n        # compatilibty for sahi v0.8.15\n        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n        full_shape_list = fix_full_shape_list(full_shape_list)\n\n        # parse boxes and masks from predictions\n        object_prediction_list_per_image = []\n        for image_ind, original_prediction in enumerate(original_predictions):\n            shift_amount = shift_amount_list[image_ind]\n            full_shape = None if full_shape_list is None else full_shape_list[image_ind]\n\n            boxes = original_prediction[\"bboxes\"]\n            scores = original_prediction[\"scores\"]\n            labels = original_prediction[\"labels\"]\n            if self.has_mask:\n                masks = original_prediction[\"masks\"]\n\n            object_prediction_list = []\n\n            n_detects = len(labels)\n            # process predictions\n            for i in range(n_detects):\n                if self.has_mask:\n                    mask = masks[i]\n\n                bbox = boxes[i]\n                score = scores[i]\n                category_id = labels[i]\n                category_name = category_mapping[str(category_id)]\n\n                # ignore low scored predictions\n                if score &lt; self.confidence_threshold:\n                    continue\n\n                # parse prediction mask\n                if self.has_mask:\n                    if \"counts\" in mask:\n                        if can_decode_rle:\n                            bool_mask = mask_utils.decode(mask)\n                        else:\n                            raise ValueError(\n                                \"Can not decode rle mask. Please install pycocotools. ex: 'pip install pycocotools'\"\n                            )\n                    else:\n                        bool_mask = mask\n                    # check if mask is valid\n                    # https://github.com/obss/sahi/discussions/696\n                    if get_bbox_from_bool_mask(bool_mask) is None:\n                        continue\n                    segmentation = get_coco_segmentation_from_bool_mask(bool_mask)\n                else:\n                    segmentation = None\n\n                # fix negative box coords\n                bbox[0] = max(0, bbox[0])\n                bbox[1] = max(0, bbox[1])\n                bbox[2] = max(0, bbox[2])\n                bbox[3] = max(0, bbox[3])\n\n                # fix out of image box coords\n                if full_shape is not None:\n                    bbox[0] = min(full_shape[1], bbox[0])\n                    bbox[1] = min(full_shape[0], bbox[1])\n                    bbox[2] = min(full_shape[1], bbox[2])\n                    bbox[3] = min(full_shape[0], bbox[3])\n\n                # ignore invalid predictions\n                if not (bbox[0] &lt; bbox[2]) or not (bbox[1] &lt; bbox[3]):\n                    logger.warning(f\"ignoring invalid prediction with bbox: {bbox}\")\n                    continue\n\n                object_prediction = ObjectPrediction(\n                    bbox=bbox,\n                    category_id=category_id,\n                    score=score,\n                    segmentation=segmentation,\n                    category_name=category_name,\n                    shift_amount=shift_amount,\n                    full_shape=full_shape,\n                )\n                object_prediction_list.append(object_prediction)\n            object_prediction_list_per_image.append(object_prediction_list)\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre> Attributes\u00b6 <code></code> <code>has_mask</code> <code>property</code> \u00b6 <p>Returns if model output contains segmentation mask.</p> <p>Considers both single dataset and ConcatDataset scenarios.</p> <code></code> <code>num_categories</code> <code>property</code> \u00b6 <p>Returns number of categories.</p> Functions\u00b6 <code></code> <code>load_model()</code> \u00b6 <p>Detection model is initialized and set to self.model.</p> Source code in <code>sahi/models/mmdet.py</code> <pre><code>def load_model(self):\n    \"\"\"Detection model is initialized and set to self.model.\"\"\"\n\n    # create model\n    model = DetInferencerWrapper(\n        self.config_path, self.model_path, device=self.device, scope=self.scope, image_size=self.image_size\n    )\n\n    self.set_model(model)\n</code></pre> <code></code> <code>perform_inference(image)</code> \u00b6 <p>Prediction is performed using self.model and the prediction result is set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.</p> required Source code in <code>sahi/models/mmdet.py</code> <pre><code>def perform_inference(self, image: np.ndarray):\n    \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n    \"\"\"\n\n    # Confirm model is loaded\n    if self.model is None:\n        raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n\n    # Supports only batch of 1\n\n    # perform inference\n    if isinstance(image, np.ndarray):\n        # https://github.com/obss/sahi/issues/265\n        image = image[:, :, ::-1]\n    # compatibility with sahi v0.8.15\n    if not isinstance(image, list):\n        image_list = [image]\n    prediction_result = self.model(image_list)\n\n    self._original_predictions = prediction_result[\"predictions\"]\n</code></pre> <code></code> <code>set_model(model)</code> \u00b6 <p>Sets the underlying MMDetection model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> \u00b6 <code>Any</code> <p>Any A MMDetection model</p> required Source code in <code>sahi/models/mmdet.py</code> <pre><code>def set_model(self, model: Any):\n    \"\"\"Sets the underlying MMDetection model.\n\n    Args:\n        model: Any\n            A MMDetection model\n    \"\"\"\n\n    # set self.model\n    self.model = model\n\n    # set category_mapping\n    if not self.category_mapping:\n        category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n        self.category_mapping = category_mapping\n</code></pre> Functions\u00b6"},{"location":"api/#sahi.models.roboflow","title":"<code>roboflow</code>","text":"Classes\u00b6 <code>RoboflowDetectionModel</code> \u00b6 <p>               Bases: <code>DetectionModel</code></p> Source code in <code>sahi/models/roboflow.py</code> <pre><code>class RoboflowDetectionModel(DetectionModel):\n    def __init__(\n        self,\n        model: Any | None = None,\n        model_path: str | None = None,\n        config_path: str | None = None,\n        device: str | None = None,\n        mask_threshold: float = 0.5,\n        confidence_threshold: float = 0.3,\n        category_mapping: dict | None = None,\n        category_remapping: dict | None = None,\n        load_at_init: bool = True,\n        image_size: int | None = None,\n        api_key: str | None = None,\n    ):\n        \"\"\"Initialize the RoboflowDetectionModel with the given parameters.\n\n        Args:\n            model_path: str\n                Path for the instance segmentation model weight\n            config_path: str\n                Path for the mmdetection instance segmentation model config file\n            device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n            mask_threshold: float\n                Value to threshold mask pixels, should be between 0 and 1\n            confidence_threshold: float\n                All predictions with score &lt; confidence_threshold will be discarded\n            category_mapping: dict: str to str\n                Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n            category_remapping: dict: str to int\n                Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n            load_at_init: bool\n                If True, automatically loads the model at initialization\n            image_size: int\n                Inference input size.\n        \"\"\"\n        self._use_universe = model and isinstance(model, str)\n        self._model = model\n        self._device = device\n        self._api_key = api_key\n\n        if self._use_universe:\n            existing_packages = getattr(self, \"required_packages\", None) or []\n            self.required_packages = [*list(existing_packages), \"inference\"]\n        else:\n            existing_packages = getattr(self, \"required_packages\", None) or []\n            self.required_packages = [*list(existing_packages), \"rfdetr\"]\n\n        super().__init__(\n            model=model,\n            model_path=model_path,\n            config_path=config_path,\n            device=device,\n            mask_threshold=mask_threshold,\n            confidence_threshold=confidence_threshold,\n            category_mapping=category_mapping,\n            category_remapping=category_remapping,\n            load_at_init=False,\n            image_size=image_size,\n        )\n\n        if load_at_init:\n            self.load_model()\n\n    def set_model(self, model: Any, **kwargs):\n        \"\"\"\n        This function should be implemented to instantiate a DetectionModel out of an already loaded model\n        Args:\n            model: Any\n                Loaded model\n        \"\"\"\n        self.model = model\n\n    def load_model(self):\n        \"\"\"This function should be implemented in a way that detection model should be initialized and set to\n        self.model.\n\n        (self.model_path, self.config_path, and self.device should be utilized)\n        \"\"\"\n        if self._use_universe:\n            from inference import get_model\n            from inference.core.env import API_KEY\n            from inference.core.exceptions import RoboflowAPINotAuthorizedError\n\n            api_key = self._api_key or API_KEY\n\n            try:\n                model = get_model(self._model, api_key=api_key)\n            except RoboflowAPINotAuthorizedError as e:\n                raise ValueError(\n                    \"Authorization failed. Please pass a valid API key with \"\n                    \"the `api_key` parameter or set the `ROBOFLOW_API_KEY` environment variable.\"\n                ) from e\n\n            assert model.task_type == \"object-detection\", \"Roboflow model must be an object detection model.\"\n\n        else:\n            from rfdetr.detr import RFDETRBase, RFDETRLarge, RFDETRMedium, RFDETRNano, RFDETRSmall\n\n            model, model_path = self._model, self.model_path\n            model_names = (\"RFDETRBase\", \"RFDETRNano\", \"RFDETRSmall\", \"RFDETRMedium\", \"RFDETRLarge\")\n            if hasattr(model, \"__name__\") and model.__name__ in model_names:\n                model_params = dict(\n                    resolution=int(self.image_size) if self.image_size else 560,\n                    device=self._device,\n                    num_classes=len(self.category_mapping.keys()) if self.category_mapping else None,\n                )\n                if model_path:\n                    model_params[\"pretrain_weights\"] = model_path\n\n                model = model(**model_params)\n            elif isinstance(model, (RFDETRBase, RFDETRNano, RFDETRSmall, RFDETRMedium, RFDETRLarge)):\n                model = model\n            else:\n                raise ValueError(\n                    f\"Model must be a Roboflow model string or one of {model_names} models, got {self.model}.\"\n                )\n\n        self.set_model(model)\n\n    def perform_inference(\n        self,\n        image: np.ndarray,\n    ):\n        \"\"\"This function should be implemented in a way that prediction should be performed using self.model and the\n        prediction result should be set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted.\n        \"\"\"\n        if self._use_universe:\n            self._original_predictions = self.model.infer(image, confidence=self.confidence_threshold)\n        else:\n            self._original_predictions = [self.model.predict(image, threshold=self.confidence_threshold)]\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"This function should be implemented in a way that self._original_predictions should be converted to a list of\n        prediction.ObjectPrediction and set to self._object_prediction_list.\n\n        self.mask_threshold can also be utilized.\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        # compatibility for sahi v0.8.15\n        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n        full_shape_list = fix_full_shape_list(full_shape_list)\n\n        object_prediction_list: list[ObjectPrediction] = []\n\n        if self._use_universe:\n            from inference.core.entities.responses.inference import (\n                ObjectDetectionInferenceResponse as InferenceObjectDetectionInferenceResponse,\n            )\n            from inference.core.entities.responses.inference import (\n                ObjectDetectionPrediction as InferenceObjectDetectionPrediction,\n            )\n\n            original_reponses: list[InferenceObjectDetectionInferenceResponse] = self._original_predictions\n\n            assert len(original_reponses) == len(shift_amount_list) == len(full_shape_list), (\n                \"Length mismatch between original responses, shift amounts, and full shapes.\"\n            )\n\n            for original_reponse, shift_amount, full_shape in zip(\n                original_reponses,\n                shift_amount_list,\n                full_shape_list,\n            ):\n                for prediction in original_reponse.predictions:\n                    prediction: InferenceObjectDetectionPrediction\n                    bbox = [\n                        prediction.x - prediction.width / 2,\n                        prediction.y - prediction.height / 2,\n                        prediction.x + prediction.width / 2,\n                        prediction.y + prediction.height / 2,\n                    ]\n                    object_prediction = ObjectPrediction(\n                        bbox=bbox,\n                        category_id=prediction.class_id,\n                        category_name=prediction.class_name,\n                        score=prediction.confidence,\n                        shift_amount=shift_amount,\n                        full_shape=full_shape,\n                    )\n                    object_prediction_list.append(object_prediction)\n\n        else:\n            from supervision.detection.core import Detections\n\n            original_detections: list[Detections] = self._original_predictions\n\n            assert len(original_detections) == len(shift_amount_list) == len(full_shape_list), (\n                \"Length mismatch between original responses, shift amounts, and full shapes.\"\n            )\n\n            for original_detection, shift_amount, full_shape in zip(\n                original_detections,\n                shift_amount_list,\n                full_shape_list,\n            ):\n                for xyxy, confidence, class_id in zip(\n                    original_detection.xyxy,\n                    original_detection.confidence,\n                    original_detection.class_id,\n                ):\n                    object_prediction = ObjectPrediction(\n                        bbox=xyxy,\n                        category_id=int(class_id),\n                        category_name=self.category_mapping.get(int(class_id), None),\n                        score=float(confidence),\n                        shift_amount=shift_amount,\n                        full_shape=full_shape,\n                    )\n                    object_prediction_list.append(object_prediction)\n\n        object_prediction_list_per_image = [object_prediction_list]\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre> Functions\u00b6 <code></code> <code>__init__(model=None, model_path=None, config_path=None, device=None, mask_threshold=0.5, confidence_threshold=0.3, category_mapping=None, category_remapping=None, load_at_init=True, image_size=None, api_key=None)</code> \u00b6 <p>Initialize the RoboflowDetectionModel with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> \u00b6 <code>str | None</code> <p>str Path for the instance segmentation model weight</p> <code>None</code> <code>config_path</code> \u00b6 <code>str | None</code> <p>str Path for the mmdetection instance segmentation model config file</p> <code>None</code> <code>device</code> \u00b6 <code>str | None</code> <p>Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.</p> <code>None</code> <code>mask_threshold</code> \u00b6 <code>float</code> <p>float Value to threshold mask pixels, should be between 0 and 1</p> <code>0.5</code> <code>confidence_threshold</code> \u00b6 <code>float</code> <p>float All predictions with score &lt; confidence_threshold will be discarded</p> <code>0.3</code> <code>category_mapping</code> \u00b6 <code>dict | None</code> <p>dict: str to str Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}</p> <code>None</code> <code>category_remapping</code> \u00b6 <code>dict | None</code> <p>dict: str to int Remap category ids based on category names, after performing inference e.g. {\"car\": 3}</p> <code>None</code> <code>load_at_init</code> \u00b6 <code>bool</code> <p>bool If True, automatically loads the model at initialization</p> <code>True</code> <code>image_size</code> \u00b6 <code>int | None</code> <p>int Inference input size.</p> <code>None</code> Source code in <code>sahi/models/roboflow.py</code> <pre><code>def __init__(\n    self,\n    model: Any | None = None,\n    model_path: str | None = None,\n    config_path: str | None = None,\n    device: str | None = None,\n    mask_threshold: float = 0.5,\n    confidence_threshold: float = 0.3,\n    category_mapping: dict | None = None,\n    category_remapping: dict | None = None,\n    load_at_init: bool = True,\n    image_size: int | None = None,\n    api_key: str | None = None,\n):\n    \"\"\"Initialize the RoboflowDetectionModel with the given parameters.\n\n    Args:\n        model_path: str\n            Path for the instance segmentation model weight\n        config_path: str\n            Path for the mmdetection instance segmentation model config file\n        device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n        mask_threshold: float\n            Value to threshold mask pixels, should be between 0 and 1\n        confidence_threshold: float\n            All predictions with score &lt; confidence_threshold will be discarded\n        category_mapping: dict: str to str\n            Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n        category_remapping: dict: str to int\n            Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n        load_at_init: bool\n            If True, automatically loads the model at initialization\n        image_size: int\n            Inference input size.\n    \"\"\"\n    self._use_universe = model and isinstance(model, str)\n    self._model = model\n    self._device = device\n    self._api_key = api_key\n\n    if self._use_universe:\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"inference\"]\n    else:\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"rfdetr\"]\n\n    super().__init__(\n        model=model,\n        model_path=model_path,\n        config_path=config_path,\n        device=device,\n        mask_threshold=mask_threshold,\n        confidence_threshold=confidence_threshold,\n        category_mapping=category_mapping,\n        category_remapping=category_remapping,\n        load_at_init=False,\n        image_size=image_size,\n    )\n\n    if load_at_init:\n        self.load_model()\n</code></pre> <code></code> <code>load_model()</code> \u00b6 <p>This function should be implemented in a way that detection model should be initialized and set to self.model.</p> <p>(self.model_path, self.config_path, and self.device should be utilized)</p> Source code in <code>sahi/models/roboflow.py</code> <pre><code>def load_model(self):\n    \"\"\"This function should be implemented in a way that detection model should be initialized and set to\n    self.model.\n\n    (self.model_path, self.config_path, and self.device should be utilized)\n    \"\"\"\n    if self._use_universe:\n        from inference import get_model\n        from inference.core.env import API_KEY\n        from inference.core.exceptions import RoboflowAPINotAuthorizedError\n\n        api_key = self._api_key or API_KEY\n\n        try:\n            model = get_model(self._model, api_key=api_key)\n        except RoboflowAPINotAuthorizedError as e:\n            raise ValueError(\n                \"Authorization failed. Please pass a valid API key with \"\n                \"the `api_key` parameter or set the `ROBOFLOW_API_KEY` environment variable.\"\n            ) from e\n\n        assert model.task_type == \"object-detection\", \"Roboflow model must be an object detection model.\"\n\n    else:\n        from rfdetr.detr import RFDETRBase, RFDETRLarge, RFDETRMedium, RFDETRNano, RFDETRSmall\n\n        model, model_path = self._model, self.model_path\n        model_names = (\"RFDETRBase\", \"RFDETRNano\", \"RFDETRSmall\", \"RFDETRMedium\", \"RFDETRLarge\")\n        if hasattr(model, \"__name__\") and model.__name__ in model_names:\n            model_params = dict(\n                resolution=int(self.image_size) if self.image_size else 560,\n                device=self._device,\n                num_classes=len(self.category_mapping.keys()) if self.category_mapping else None,\n            )\n            if model_path:\n                model_params[\"pretrain_weights\"] = model_path\n\n            model = model(**model_params)\n        elif isinstance(model, (RFDETRBase, RFDETRNano, RFDETRSmall, RFDETRMedium, RFDETRLarge)):\n            model = model\n        else:\n            raise ValueError(\n                f\"Model must be a Roboflow model string or one of {model_names} models, got {self.model}.\"\n            )\n\n    self.set_model(model)\n</code></pre> <code></code> <code>perform_inference(image)</code> \u00b6 <p>This function should be implemented in a way that prediction should be performed using self.model and the prediction result should be set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted.</p> required Source code in <code>sahi/models/roboflow.py</code> <pre><code>def perform_inference(\n    self,\n    image: np.ndarray,\n):\n    \"\"\"This function should be implemented in a way that prediction should be performed using self.model and the\n    prediction result should be set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted.\n    \"\"\"\n    if self._use_universe:\n        self._original_predictions = self.model.infer(image, confidence=self.confidence_threshold)\n    else:\n        self._original_predictions = [self.model.predict(image, threshold=self.confidence_threshold)]\n</code></pre> <code></code> <code>set_model(model, **kwargs)</code> \u00b6 <p>This function should be implemented to instantiate a DetectionModel out of an already loaded model Args:     model: Any         Loaded model</p> Source code in <code>sahi/models/roboflow.py</code> <pre><code>def set_model(self, model: Any, **kwargs):\n    \"\"\"\n    This function should be implemented to instantiate a DetectionModel out of an already loaded model\n    Args:\n        model: Any\n            Loaded model\n    \"\"\"\n    self.model = model\n</code></pre>"},{"location":"api/#sahi.models.rtdetr","title":"<code>rtdetr</code>","text":"Classes\u00b6 <code>RTDetrDetectionModel</code> \u00b6 <p>               Bases: <code>UltralyticsDetectionModel</code></p> Source code in <code>sahi/models/rtdetr.py</code> <pre><code>class RTDetrDetectionModel(UltralyticsDetectionModel):\n    def __init__(self, **kwargs):\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"ultralytics\"]\n        super().__init__(**kwargs)\n\n    def load_model(self):\n        \"\"\"Detection model is initialized and set to self.model.\"\"\"\n        from ultralytics import RTDETR\n\n        try:\n            model_source = self.model_path or \"rtdetr-l.pt\"\n            model = RTDETR(model_source)\n            model.to(self.device)\n            self.set_model(model)\n        except Exception as e:\n            raise TypeError(\"model_path is not a valid rtdet model path: \", e)\n</code></pre> Functions\u00b6 <code></code> <code>load_model()</code> \u00b6 <p>Detection model is initialized and set to self.model.</p> Source code in <code>sahi/models/rtdetr.py</code> <pre><code>def load_model(self):\n    \"\"\"Detection model is initialized and set to self.model.\"\"\"\n    from ultralytics import RTDETR\n\n    try:\n        model_source = self.model_path or \"rtdetr-l.pt\"\n        model = RTDETR(model_source)\n        model.to(self.device)\n        self.set_model(model)\n    except Exception as e:\n        raise TypeError(\"model_path is not a valid rtdet model path: \", e)\n</code></pre>"},{"location":"api/#sahi.models.torchvision","title":"<code>torchvision</code>","text":"Classes\u00b6 <code>TorchVisionDetectionModel</code> \u00b6 <p>               Bases: <code>DetectionModel</code></p> Source code in <code>sahi/models/torchvision.py</code> <pre><code>class TorchVisionDetectionModel(DetectionModel):\n    def __init__(self, *args, **kwargs):\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"torch\", \"torchvision\"]\n        super().__init__(*args, **kwargs)\n\n    def load_model(self):\n        import torch\n\n        # read config params\n        model_name = None\n        num_classes = None\n        if self.config_path is not None:\n            with open(self.config_path) as stream:\n                try:\n                    config = yaml.safe_load(stream)\n                except yaml.YAMLError as exc:\n                    raise RuntimeError(exc)\n\n            model_name = config.get(\"model_name\", None)\n            num_classes = config.get(\"num_classes\", None)\n\n        # complete params if not provided in config\n        if not model_name:\n            model_name = \"fasterrcnn_resnet50_fpn\"\n            logger.warning(f\"model_name not provided in config, using default model_type: {model_name}'\")\n        if num_classes is None:\n            logger.warning(\"num_classes not provided in config, using default num_classes: 91\")\n            num_classes = 91\n        if self.model_path is None:\n            logger.warning(\"model_path not provided in config, using pretrained weights and default num_classes: 91.\")\n            weights = \"DEFAULT\"\n            num_classes = 91\n        else:\n            weights = None\n\n        # load model\n        # Note: torchvision &gt;= 0.13 is required for the 'weights' parameter\n        model = MODEL_NAME_TO_CONSTRUCTOR[model_name](num_classes=num_classes, weights=weights)\n        if self.model_path:\n            try:\n                model.load_state_dict(torch.load(self.model_path))\n            except Exception as e:\n                logger.error(f\"Invalid {self.model_path=}\")\n                raise TypeError(\"model_path is not a valid torchvision model path: \", e)\n\n        self.set_model(model)\n\n    def set_model(self, model: Any):\n        \"\"\"Sets the underlying TorchVision model.\n\n        Args:\n            model: Any\n                A TorchVision model\n        \"\"\"\n\n        model.eval()\n        self.model = model.to(self.device)\n\n        # set category_mapping\n\n        if self.category_mapping is None:\n            category_names = {str(i): COCO_CLASSES[i] for i in range(len(COCO_CLASSES))}\n            self.category_mapping = category_names\n\n    def perform_inference(self, image: np.ndarray, image_size: int | None = None):\n        \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n            image_size: int\n                Inference input size.\n        \"\"\"\n        from sahi.utils.torch_utils import to_float_tensor\n\n        # arrange model input size\n        if self.image_size is not None:\n            # get min and max of image height and width\n            min_shape, max_shape = min(image.shape[:2]), max(image.shape[:2])\n            # torchvision resize transform scales the shorter dimension to the target size\n            # we want to scale the longer dimension to the target size\n            image_size = self.image_size * min_shape / max_shape\n            self.model.transform.min_size = (image_size,)  # default is (800,)\n            self.model.transform.max_size = image_size  # default is 1333\n\n        image = to_float_tensor(image)\n        image = image.to(self.device)\n        prediction_result = self.model([image])\n\n        self._original_predictions = prediction_result\n\n    @property\n    def num_categories(self):\n        \"\"\"Returns number of categories.\"\"\"\n        return len(self.category_mapping)\n\n    @property\n    def has_mask(self):\n        \"\"\"Returns if model output contains segmentation mask.\"\"\"\n        return hasattr(self.model, \"roi_heads\") and hasattr(self.model.roi_heads, \"mask_predictor\")\n\n    @property\n    def category_names(self):\n        return list(self.category_mapping.values())\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n        self._object_prediction_list_per_image.\n\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        original_predictions = self._original_predictions\n\n        # compatilibty for sahi v0.8.20\n        if isinstance(shift_amount_list[0], int):\n            shift_amount_list = [shift_amount_list]\n        if full_shape_list is not None and isinstance(full_shape_list[0], int):\n            full_shape_list = [full_shape_list]\n\n        for image_predictions in original_predictions:\n            object_prediction_list_per_image = []\n\n            # get indices of boxes with score &gt; confidence_threshold\n            scores = image_predictions[\"scores\"].cpu().detach().numpy()\n            selected_indices = np.where(scores &gt; self.confidence_threshold)[0]\n\n            # parse boxes, masks, scores, category_ids from predictions\n            category_ids = list(image_predictions[\"labels\"][selected_indices].cpu().detach().numpy())\n            boxes = list(image_predictions[\"boxes\"][selected_indices].cpu().detach().numpy())\n            scores = scores[selected_indices]\n\n            # check if predictions contain mask\n            masks = image_predictions.get(\"masks\", None)\n            if masks is not None:\n                masks = list(\n                    (image_predictions[\"masks\"][selected_indices] &gt; self.mask_threshold).cpu().detach().numpy()\n                )\n            else:\n                masks = None\n\n            # create object_prediction_list\n            object_prediction_list = []\n\n            shift_amount = shift_amount_list[0]\n            full_shape = None if full_shape_list is None else full_shape_list[0]\n\n            for ind in range(len(boxes)):\n                if masks is not None:\n                    segmentation = get_coco_segmentation_from_bool_mask(np.array(masks[ind]))\n                else:\n                    segmentation = None\n\n                object_prediction = ObjectPrediction(\n                    bbox=boxes[ind],\n                    segmentation=segmentation,\n                    category_id=int(category_ids[ind]),\n                    category_name=self.category_mapping[str(int(category_ids[ind]))],\n                    shift_amount=shift_amount,\n                    score=scores[ind],\n                    full_shape=full_shape,\n                )\n                object_prediction_list.append(object_prediction)\n            object_prediction_list_per_image.append(object_prediction_list)\n\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre> Attributes\u00b6 <code></code> <code>has_mask</code> <code>property</code> \u00b6 <p>Returns if model output contains segmentation mask.</p> <code></code> <code>num_categories</code> <code>property</code> \u00b6 <p>Returns number of categories.</p> Functions\u00b6 <code></code> <code>perform_inference(image, image_size=None)</code> \u00b6 <p>Prediction is performed using self.model and the prediction result is set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.</p> required <code>image_size</code> \u00b6 <code>int | None</code> <p>int Inference input size.</p> <code>None</code> Source code in <code>sahi/models/torchvision.py</code> <pre><code>def perform_inference(self, image: np.ndarray, image_size: int | None = None):\n    \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        image_size: int\n            Inference input size.\n    \"\"\"\n    from sahi.utils.torch_utils import to_float_tensor\n\n    # arrange model input size\n    if self.image_size is not None:\n        # get min and max of image height and width\n        min_shape, max_shape = min(image.shape[:2]), max(image.shape[:2])\n        # torchvision resize transform scales the shorter dimension to the target size\n        # we want to scale the longer dimension to the target size\n        image_size = self.image_size * min_shape / max_shape\n        self.model.transform.min_size = (image_size,)  # default is (800,)\n        self.model.transform.max_size = image_size  # default is 1333\n\n    image = to_float_tensor(image)\n    image = image.to(self.device)\n    prediction_result = self.model([image])\n\n    self._original_predictions = prediction_result\n</code></pre> <code></code> <code>set_model(model)</code> \u00b6 <p>Sets the underlying TorchVision model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> \u00b6 <code>Any</code> <p>Any A TorchVision model</p> required Source code in <code>sahi/models/torchvision.py</code> <pre><code>def set_model(self, model: Any):\n    \"\"\"Sets the underlying TorchVision model.\n\n    Args:\n        model: Any\n            A TorchVision model\n    \"\"\"\n\n    model.eval()\n    self.model = model.to(self.device)\n\n    # set category_mapping\n\n    if self.category_mapping is None:\n        category_names = {str(i): COCO_CLASSES[i] for i in range(len(COCO_CLASSES))}\n        self.category_mapping = category_names\n</code></pre> Functions\u00b6"},{"location":"api/#sahi.models.ultralytics","title":"<code>ultralytics</code>","text":"Classes\u00b6 <code>UltralyticsDetectionModel</code> \u00b6 <p>               Bases: <code>DetectionModel</code></p> <p>Detection model for Ultralytics YOLO models.</p> <p>Supports both PyTorch (.pt) and ONNX (.onnx) models.</p> Source code in <code>sahi/models/ultralytics.py</code> <pre><code>class UltralyticsDetectionModel(DetectionModel):\n    \"\"\"Detection model for Ultralytics YOLO models.\n\n    Supports both PyTorch (.pt) and ONNX (.onnx) models.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.fuse: bool = kwargs.pop(\"fuse\", False)\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"ultralytics\"]\n        super().__init__(*args, **kwargs)\n\n    def load_model(self):\n        \"\"\"Detection model is initialized and set to self.model.\n\n        Supports both PyTorch (.pt) and ONNX (.onnx) models.\n        \"\"\"\n\n        from ultralytics import YOLO\n\n        if self.model_path and \".onnx\" in self.model_path:\n            check_requirements([\"onnx\", \"onnxruntime\"])\n\n        try:\n            model = YOLO(self.model_path)\n            # Only call .to(device) for PyTorch models, not ONNX\n            if self.model_path and not self.model_path.endswith(\".onnx\"):\n                model.to(self.device)\n            self.set_model(model)\n            if self.fuse and hasattr(model, \"fuse\"):\n                model.fuse()\n\n        except Exception as e:\n            raise TypeError(\"model_path is not a valid Ultralytics model path: \", e)\n\n    def set_model(self, model: Any, **kwargs):\n        \"\"\"Sets the underlying Ultralytics model.\n\n        Args:\n            model: Any\n                A Ultralytics model\n        \"\"\"\n\n        self.model = model\n        # set category_mapping\n        if not self.category_mapping:\n            category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n            self.category_mapping = category_mapping\n\n    def perform_inference(self, image: np.ndarray):\n        \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        \"\"\"\n\n        # Confirm model is loaded\n\n        import torch\n\n        if self.model is None:\n            raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n\n        kwargs = {\"cfg\": self.config_path, \"verbose\": False, \"conf\": self.confidence_threshold, \"device\": self.device}\n\n        if self.image_size is not None:\n            kwargs = {\"imgsz\": self.image_size, **kwargs}\n\n        prediction_result = self.model(image[:, :, ::-1], **kwargs)  # YOLO expects numpy arrays to have BGR\n\n        # Handle different result types for PyTorch vs ONNX models\n        # ONNX models might return results in a different format\n        if self.has_mask:\n            from ultralytics.engine.results import Masks\n\n            if not prediction_result[0].masks:\n                # Create empty masks if none exist\n                if hasattr(self.model, \"device\"):\n                    device = self.model.device\n                else:\n                    device = \"cpu\"  # Default for ONNX models\n                prediction_result[0].masks = Masks(\n                    torch.tensor([], device=device), prediction_result[0].boxes.orig_shape\n                )\n\n            # We do not filter results again as confidence threshold is already applied above\n            prediction_result = [\n                (\n                    result.boxes.data,\n                    result.masks.data,\n                )\n                for result in prediction_result\n            ]\n        elif self.is_obb:\n            # For OBB task, get OBB points in xyxyxyxy format\n            device = getattr(self.model, \"device\", \"cpu\")\n            prediction_result = [\n                (\n                    # Get OBB data: xyxy, conf, cls\n                    torch.cat(\n                        [\n                            result.obb.xyxy,  # box coordinates\n                            result.obb.conf.unsqueeze(-1),  # confidence scores\n                            result.obb.cls.unsqueeze(-1),  # class ids\n                        ],\n                        dim=1,\n                    )\n                    if result.obb is not None\n                    else torch.empty((0, 6), device=device),\n                    # Get OBB points in (N, 4, 2) format\n                    result.obb.xyxyxyxy if result.obb is not None else torch.empty((0, 4, 2), device=device),\n                )\n                for result in prediction_result\n            ]\n        else:  # If model doesn't do segmentation or OBB then no need to check masks\n            # We do not filter results again as confidence threshold is already applied above\n            prediction_result = [result.boxes.data for result in prediction_result]\n\n        self._original_predictions = prediction_result\n        self._original_shape = image.shape\n\n    @property\n    def category_names(self):\n        # For ONNX models, names might not be available, use category_mapping\n        if hasattr(self.model, \"names\") and self.model.names:\n            return self.model.names.values()\n        elif self.category_mapping:\n            return list(self.category_mapping.values())\n        else:\n            raise ValueError(\"Category names not available. Please provide category_mapping for ONNX models.\")\n\n    @property\n    def num_categories(self):\n        \"\"\"Returns number of categories.\"\"\"\n        if hasattr(self.model, \"names\") and self.model.names:\n            return len(self.model.names)\n        elif self.category_mapping:\n            return len(self.category_mapping)\n        else:\n            raise ValueError(\"Cannot determine number of categories. Please provide category_mapping for ONNX models.\")\n\n    @property\n    def has_mask(self):\n        \"\"\"Returns if model output contains segmentation mask.\"\"\"\n        # Check if model has 'task' attribute (for both .pt and .onnx models)\n        if hasattr(self.model, \"overrides\") and \"task\" in self.model.overrides:\n            return self.model.overrides[\"task\"] == \"segment\"\n        # For ONNX models, task might be stored differently\n        elif hasattr(self.model, \"task\"):\n            return self.model.task == \"segment\"\n        # For ONNX models without task info, check model path\n        elif self.model_path and isinstance(self.model_path, str):\n            return \"seg\" in self.model_path.lower()\n        return False\n\n    @property\n    def is_obb(self):\n        \"\"\"Returns if model output contains oriented bounding boxes.\"\"\"\n        # Check if model has 'task' attribute (for both .pt and .onnx models)\n        if hasattr(self.model, \"overrides\") and \"task\" in self.model.overrides:\n            return self.model.overrides[\"task\"] == \"obb\"\n        # For ONNX models, task might be stored differently\n        elif hasattr(self.model, \"task\"):\n            return self.model.task == \"obb\"\n        # For ONNX models without task info, check model path\n        elif self.model_path and isinstance(self.model_path, str):\n            return \"obb\" in self.model_path.lower()\n        return False\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n        self._object_prediction_list_per_image.\n\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        original_predictions = self._original_predictions\n\n        # compatibility for sahi v0.8.15\n        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n        full_shape_list = fix_full_shape_list(full_shape_list)\n\n        # handle all predictions\n        object_prediction_list_per_image = []\n\n        for image_ind, image_predictions in enumerate(original_predictions):\n            shift_amount = shift_amount_list[image_ind]\n            full_shape = None if full_shape_list is None else full_shape_list[image_ind]\n            object_prediction_list = []\n\n            # Extract boxes and optional masks/obb\n            if self.has_mask or self.is_obb:\n                boxes = image_predictions[0].cpu().detach().numpy()\n                masks_or_points = image_predictions[1].cpu().detach().numpy()\n            else:\n                boxes = image_predictions.data.cpu().detach().numpy()\n                masks_or_points = None\n\n            # Process each prediction\n            for pred_ind, prediction in enumerate(boxes):\n                # Get bbox coordinates\n                bbox = prediction[:4].tolist()\n                score = prediction[4]\n                category_id = int(prediction[5])\n                category_name = self.category_mapping[str(category_id)]\n\n                # Fix box coordinates\n                bbox = [max(0, coord) for coord in bbox]\n                if full_shape is not None:\n                    bbox[0] = min(full_shape[1], bbox[0])\n                    bbox[1] = min(full_shape[0], bbox[1])\n                    bbox[2] = min(full_shape[1], bbox[2])\n                    bbox[3] = min(full_shape[0], bbox[3])\n\n                # Ignore invalid predictions\n                if not (bbox[0] &lt; bbox[2]) or not (bbox[1] &lt; bbox[3]):\n                    logger.warning(f\"ignoring invalid prediction with bbox: {bbox}\")\n                    continue\n\n                # Get segmentation or OBB points\n                segmentation = None\n                if masks_or_points is not None:\n                    if self.has_mask:\n                        bool_mask = masks_or_points[pred_ind]\n                        # Resize mask to original image size\n                        bool_mask = cv2.resize(\n                            bool_mask.astype(np.uint8), (self._original_shape[1], self._original_shape[0])\n                        )\n                        segmentation = get_coco_segmentation_from_bool_mask(bool_mask)\n                    else:  # is_obb\n                        obb_points = masks_or_points[pred_ind]  # Get OBB points for this prediction\n                        segmentation = [obb_points.reshape(-1).tolist()]\n\n                    if len(segmentation) == 0:\n                        continue\n\n                # Create and append object prediction\n                object_prediction = ObjectPrediction(\n                    bbox=bbox,\n                    category_id=category_id,\n                    score=score,\n                    segmentation=segmentation,\n                    category_name=category_name,\n                    shift_amount=shift_amount,\n                    full_shape=self._original_shape[:2] if full_shape is None else full_shape,  # (height, width)\n                )\n                object_prediction_list.append(object_prediction)\n\n            object_prediction_list_per_image.append(object_prediction_list)\n\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre> Attributes\u00b6 <code></code> <code>has_mask</code> <code>property</code> \u00b6 <p>Returns if model output contains segmentation mask.</p> <code></code> <code>is_obb</code> <code>property</code> \u00b6 <p>Returns if model output contains oriented bounding boxes.</p> <code></code> <code>num_categories</code> <code>property</code> \u00b6 <p>Returns number of categories.</p> Functions\u00b6 <code></code> <code>load_model()</code> \u00b6 <p>Detection model is initialized and set to self.model.</p> <p>Supports both PyTorch (.pt) and ONNX (.onnx) models.</p> Source code in <code>sahi/models/ultralytics.py</code> <pre><code>def load_model(self):\n    \"\"\"Detection model is initialized and set to self.model.\n\n    Supports both PyTorch (.pt) and ONNX (.onnx) models.\n    \"\"\"\n\n    from ultralytics import YOLO\n\n    if self.model_path and \".onnx\" in self.model_path:\n        check_requirements([\"onnx\", \"onnxruntime\"])\n\n    try:\n        model = YOLO(self.model_path)\n        # Only call .to(device) for PyTorch models, not ONNX\n        if self.model_path and not self.model_path.endswith(\".onnx\"):\n            model.to(self.device)\n        self.set_model(model)\n        if self.fuse and hasattr(model, \"fuse\"):\n            model.fuse()\n\n    except Exception as e:\n        raise TypeError(\"model_path is not a valid Ultralytics model path: \", e)\n</code></pre> <code></code> <code>perform_inference(image)</code> \u00b6 <p>Prediction is performed using self.model and the prediction result is set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.</p> required Source code in <code>sahi/models/ultralytics.py</code> <pre><code>def perform_inference(self, image: np.ndarray):\n    \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n    \"\"\"\n\n    # Confirm model is loaded\n\n    import torch\n\n    if self.model is None:\n        raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n\n    kwargs = {\"cfg\": self.config_path, \"verbose\": False, \"conf\": self.confidence_threshold, \"device\": self.device}\n\n    if self.image_size is not None:\n        kwargs = {\"imgsz\": self.image_size, **kwargs}\n\n    prediction_result = self.model(image[:, :, ::-1], **kwargs)  # YOLO expects numpy arrays to have BGR\n\n    # Handle different result types for PyTorch vs ONNX models\n    # ONNX models might return results in a different format\n    if self.has_mask:\n        from ultralytics.engine.results import Masks\n\n        if not prediction_result[0].masks:\n            # Create empty masks if none exist\n            if hasattr(self.model, \"device\"):\n                device = self.model.device\n            else:\n                device = \"cpu\"  # Default for ONNX models\n            prediction_result[0].masks = Masks(\n                torch.tensor([], device=device), prediction_result[0].boxes.orig_shape\n            )\n\n        # We do not filter results again as confidence threshold is already applied above\n        prediction_result = [\n            (\n                result.boxes.data,\n                result.masks.data,\n            )\n            for result in prediction_result\n        ]\n    elif self.is_obb:\n        # For OBB task, get OBB points in xyxyxyxy format\n        device = getattr(self.model, \"device\", \"cpu\")\n        prediction_result = [\n            (\n                # Get OBB data: xyxy, conf, cls\n                torch.cat(\n                    [\n                        result.obb.xyxy,  # box coordinates\n                        result.obb.conf.unsqueeze(-1),  # confidence scores\n                        result.obb.cls.unsqueeze(-1),  # class ids\n                    ],\n                    dim=1,\n                )\n                if result.obb is not None\n                else torch.empty((0, 6), device=device),\n                # Get OBB points in (N, 4, 2) format\n                result.obb.xyxyxyxy if result.obb is not None else torch.empty((0, 4, 2), device=device),\n            )\n            for result in prediction_result\n        ]\n    else:  # If model doesn't do segmentation or OBB then no need to check masks\n        # We do not filter results again as confidence threshold is already applied above\n        prediction_result = [result.boxes.data for result in prediction_result]\n\n    self._original_predictions = prediction_result\n    self._original_shape = image.shape\n</code></pre> <code></code> <code>set_model(model, **kwargs)</code> \u00b6 <p>Sets the underlying Ultralytics model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> \u00b6 <code>Any</code> <p>Any A Ultralytics model</p> required Source code in <code>sahi/models/ultralytics.py</code> <pre><code>def set_model(self, model: Any, **kwargs):\n    \"\"\"Sets the underlying Ultralytics model.\n\n    Args:\n        model: Any\n            A Ultralytics model\n    \"\"\"\n\n    self.model = model\n    # set category_mapping\n    if not self.category_mapping:\n        category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n        self.category_mapping = category_mapping\n</code></pre> Functions\u00b6"},{"location":"api/#sahi.models.yolov5","title":"<code>yolov5</code>","text":"Classes\u00b6 <code>Yolov5DetectionModel</code> \u00b6 <p>               Bases: <code>DetectionModel</code></p> Source code in <code>sahi/models/yolov5.py</code> <pre><code>class Yolov5DetectionModel(DetectionModel):\n    def __init__(self, *args, **kwargs):\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"yolov5\", \"torch\"]\n        super().__init__(*args, **kwargs)\n\n    def load_model(self):\n        \"\"\"Detection model is initialized and set to self.model.\"\"\"\n        import yolov5\n\n        try:\n            model = yolov5.load(self.model_path, device=self.device)\n            self.set_model(model)\n        except Exception as e:\n            raise TypeError(\"model_path is not a valid yolov5 model path: \", e)\n\n    def set_model(self, model: Any):\n        \"\"\"Sets the underlying YOLOv5 model.\n\n        Args:\n            model: Any\n                A YOLOv5 model\n        \"\"\"\n\n        if model.__class__.__module__ not in [\"yolov5.models.common\", \"models.common\"]:\n            raise Exception(f\"Not a yolov5 model: {type(model)}\")\n\n        model.conf = self.confidence_threshold\n        self.model = model\n\n        # set category_mapping\n        if not self.category_mapping:\n            category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n            self.category_mapping = category_mapping\n\n    def perform_inference(self, image: np.ndarray):\n        \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        \"\"\"\n\n        # Confirm model is loaded\n        if self.model is None:\n            raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n        if self.image_size is not None:\n            prediction_result = self.model(image, size=self.image_size)\n        else:\n            prediction_result = self.model(image)\n\n        self._original_predictions = prediction_result\n\n    @property\n    def num_categories(self):\n        \"\"\"Returns number of categories.\"\"\"\n        return len(self.model.names)\n\n    @property\n    def has_mask(self):\n        \"\"\"Returns if model output contains segmentation mask.\"\"\"\n\n        return False  # fix when yolov5 supports segmentation models\n\n    @property\n    def category_names(self):\n        if check_package_minimum_version(\"yolov5\", \"6.2.0\"):\n            return list(self.model.names.values())\n        else:\n            return self.model.names\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n        self._object_prediction_list_per_image.\n\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        original_predictions = self._original_predictions\n\n        # compatilibty for sahi v0.8.15\n        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n        full_shape_list = fix_full_shape_list(full_shape_list)\n\n        # handle all predictions\n        object_prediction_list_per_image = []\n        for image_ind, image_predictions_in_xyxy_format in enumerate(original_predictions.xyxy):\n            shift_amount = shift_amount_list[image_ind]\n            full_shape = None if full_shape_list is None else full_shape_list[image_ind]\n            object_prediction_list = []\n\n            # process predictions\n            for prediction in image_predictions_in_xyxy_format.cpu().detach().numpy():\n                x1 = prediction[0]\n                y1 = prediction[1]\n                x2 = prediction[2]\n                y2 = prediction[3]\n                bbox = [x1, y1, x2, y2]\n                score = prediction[4]\n                category_id = int(prediction[5])\n                category_name = self.category_mapping[str(category_id)]\n\n                # fix negative box coords\n                bbox[0] = max(0, bbox[0])\n                bbox[1] = max(0, bbox[1])\n                bbox[2] = max(0, bbox[2])\n                bbox[3] = max(0, bbox[3])\n\n                # fix out of image box coords\n                if full_shape is not None:\n                    bbox[0] = min(full_shape[1], bbox[0])\n                    bbox[1] = min(full_shape[0], bbox[1])\n                    bbox[2] = min(full_shape[1], bbox[2])\n                    bbox[3] = min(full_shape[0], bbox[3])\n\n                # ignore invalid predictions\n                if not (bbox[0] &lt; bbox[2]) or not (bbox[1] &lt; bbox[3]):\n                    logger.warning(f\"ignoring invalid prediction with bbox: {bbox}\")\n                    continue\n\n                object_prediction = ObjectPrediction(\n                    bbox=bbox,\n                    category_id=category_id,\n                    score=score,\n                    segmentation=None,\n                    category_name=category_name,\n                    shift_amount=shift_amount,\n                    full_shape=full_shape,\n                )\n                object_prediction_list.append(object_prediction)\n            object_prediction_list_per_image.append(object_prediction_list)\n\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre> Attributes\u00b6 <code></code> <code>has_mask</code> <code>property</code> \u00b6 <p>Returns if model output contains segmentation mask.</p> <code></code> <code>num_categories</code> <code>property</code> \u00b6 <p>Returns number of categories.</p> Functions\u00b6 <code></code> <code>load_model()</code> \u00b6 <p>Detection model is initialized and set to self.model.</p> Source code in <code>sahi/models/yolov5.py</code> <pre><code>def load_model(self):\n    \"\"\"Detection model is initialized and set to self.model.\"\"\"\n    import yolov5\n\n    try:\n        model = yolov5.load(self.model_path, device=self.device)\n        self.set_model(model)\n    except Exception as e:\n        raise TypeError(\"model_path is not a valid yolov5 model path: \", e)\n</code></pre> <code></code> <code>perform_inference(image)</code> \u00b6 <p>Prediction is performed using self.model and the prediction result is set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.</p> required Source code in <code>sahi/models/yolov5.py</code> <pre><code>def perform_inference(self, image: np.ndarray):\n    \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n    \"\"\"\n\n    # Confirm model is loaded\n    if self.model is None:\n        raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n    if self.image_size is not None:\n        prediction_result = self.model(image, size=self.image_size)\n    else:\n        prediction_result = self.model(image)\n\n    self._original_predictions = prediction_result\n</code></pre> <code></code> <code>set_model(model)</code> \u00b6 <p>Sets the underlying YOLOv5 model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> \u00b6 <code>Any</code> <p>Any A YOLOv5 model</p> required Source code in <code>sahi/models/yolov5.py</code> <pre><code>def set_model(self, model: Any):\n    \"\"\"Sets the underlying YOLOv5 model.\n\n    Args:\n        model: Any\n            A YOLOv5 model\n    \"\"\"\n\n    if model.__class__.__module__ not in [\"yolov5.models.common\", \"models.common\"]:\n        raise Exception(f\"Not a yolov5 model: {type(model)}\")\n\n    model.conf = self.confidence_threshold\n    self.model = model\n\n    # set category_mapping\n    if not self.category_mapping:\n        category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n        self.category_mapping = category_mapping\n</code></pre> Functions\u00b6"},{"location":"api/#sahi.postprocess","title":"<code>postprocess</code>","text":""},{"location":"api/#sahi.postprocess-modules","title":"Modules","text":""},{"location":"api/#sahi.postprocess.combine","title":"<code>combine</code>","text":"Classes\u00b6 <code>PostprocessPredictions</code> \u00b6 <p>Utilities for calculating IOU/IOS based match for given ObjectPredictions.</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>class PostprocessPredictions:\n    \"\"\"Utilities for calculating IOU/IOS based match for given ObjectPredictions.\"\"\"\n\n    def __init__(\n        self,\n        match_threshold: float = 0.5,\n        match_metric: str = \"IOU\",\n        class_agnostic: bool = True,\n    ):\n        self.match_threshold = match_threshold\n        self.class_agnostic = class_agnostic\n        self.match_metric = match_metric\n\n        check_requirements([\"torch\"])\n\n    def __call__(self, predictions: list[ObjectPrediction]):\n        raise NotImplementedError()\n</code></pre> Functions\u00b6 <code></code> <code>batched_greedy_nmm(object_predictions_as_tensor, match_metric='IOU', match_threshold=0.5)</code> \u00b6 <p>Apply greedy version of non-maximum merging per category to avoid detecting too many overlapping bounding boxes for a given object.</p> <p>Parameters:</p> Name Type Description Default <code>object_predictions_as_tensor</code> \u00b6 <code>tensor</code> <p>(tensor) The location preds for the image along with the class predscores, Shape: [num_boxes,5].</p> required <code>match_metric</code> \u00b6 <code>str</code> <p>(str) IOU or IOS</p> <code>'IOU'</code> <code>match_threshold</code> \u00b6 <code>float</code> <p>(float) The overlap thresh for match metric.</p> <code>0.5</code> <p>Returns:     keep_to_merge_list: (Dict[int:List[int]]) mapping from prediction indices     to keep to a list of prediction indices to be merged.</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>def batched_greedy_nmm(\n    object_predictions_as_tensor: torch.tensor,\n    match_metric: str = \"IOU\",\n    match_threshold: float = 0.5,\n):\n    \"\"\"Apply greedy version of non-maximum merging per category to avoid detecting too many overlapping bounding boxes\n    for a given object.\n\n    Args:\n        object_predictions_as_tensor: (tensor) The location preds for the image\n            along with the class predscores, Shape: [num_boxes,5].\n        match_metric: (str) IOU or IOS\n        match_threshold: (float) The overlap thresh for\n            match metric.\n    Returns:\n        keep_to_merge_list: (Dict[int:List[int]]) mapping from prediction indices\n        to keep to a list of prediction indices to be merged.\n    \"\"\"\n    category_ids = object_predictions_as_tensor[:, 5].squeeze()\n    keep_to_merge_list = {}\n    for category_id in torch.unique(category_ids):\n        curr_indices = torch.where(category_ids == category_id)[0]\n        curr_keep_to_merge_list = greedy_nmm(object_predictions_as_tensor[curr_indices], match_metric, match_threshold)\n        curr_indices_list = curr_indices.tolist()\n        for curr_keep, curr_merge_list in curr_keep_to_merge_list.items():\n            keep = curr_indices_list[curr_keep]\n            merge_list = [curr_indices_list[curr_merge_ind] for curr_merge_ind in curr_merge_list]\n            keep_to_merge_list[keep] = merge_list\n    return keep_to_merge_list\n</code></pre> <code></code> <code>batched_nmm(object_predictions_as_tensor, match_metric='IOU', match_threshold=0.5)</code> \u00b6 <p>Apply non-maximum merging per category to avoid detecting too many overlapping bounding boxes for a given object.</p> <p>Parameters:</p> Name Type Description Default <code>object_predictions_as_tensor</code> \u00b6 <code>Tensor</code> <p>(tensor) The location preds for the image along with the class predscores, Shape: [num_boxes,5].</p> required <code>match_metric</code> \u00b6 <code>str</code> <p>(str) IOU or IOS</p> <code>'IOU'</code> <code>match_threshold</code> \u00b6 <code>float</code> <p>(float) The overlap thresh for match metric.</p> <code>0.5</code> <p>Returns:     keep_to_merge_list: (Dict[int:List[int]]) mapping from prediction indices     to keep to a list of prediction indices to be merged.</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>def batched_nmm(\n    object_predictions_as_tensor: torch.Tensor,\n    match_metric: str = \"IOU\",\n    match_threshold: float = 0.5,\n):\n    \"\"\"Apply non-maximum merging per category to avoid detecting too many overlapping bounding boxes for a given object.\n\n    Args:\n        object_predictions_as_tensor: (tensor) The location preds for the image\n            along with the class predscores, Shape: [num_boxes,5].\n        match_metric: (str) IOU or IOS\n        match_threshold: (float) The overlap thresh for\n            match metric.\n    Returns:\n        keep_to_merge_list: (Dict[int:List[int]]) mapping from prediction indices\n        to keep to a list of prediction indices to be merged.\n    \"\"\"\n    category_ids = object_predictions_as_tensor[:, 5].squeeze()\n    keep_to_merge_list = {}\n    for category_id in torch.unique(category_ids):\n        curr_indices = torch.where(category_ids == category_id)[0]\n        curr_keep_to_merge_list = nmm(object_predictions_as_tensor[curr_indices], match_metric, match_threshold)\n        curr_indices_list = curr_indices.tolist()\n        for curr_keep, curr_merge_list in curr_keep_to_merge_list.items():\n            keep = curr_indices_list[curr_keep]\n            merge_list = [curr_indices_list[curr_merge_ind] for curr_merge_ind in curr_merge_list]\n            keep_to_merge_list[keep] = merge_list\n    return keep_to_merge_list\n</code></pre> <code></code> <code>batched_nms(predictions, match_metric='IOU', match_threshold=0.5)</code> \u00b6 <p>Apply non-maximum suppression to avoid detecting too many overlapping bounding boxes for a given object.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> \u00b6 <code>tensor</code> <p>(tensor) The location preds for the image along with the class predscores, Shape: [num_boxes,5].</p> required <code>match_metric</code> \u00b6 <code>str</code> <p>(str) IOU or IOS</p> <code>'IOU'</code> <code>match_threshold</code> \u00b6 <code>float</code> <p>(float) The overlap thresh for match metric.</p> <code>0.5</code> <p>Returns:     A list of filtered indexes, Shape: [ ,]</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>def batched_nms(predictions: torch.tensor, match_metric: str = \"IOU\", match_threshold: float = 0.5):\n    \"\"\"Apply non-maximum suppression to avoid detecting too many overlapping bounding boxes for a given object.\n\n    Args:\n        predictions: (tensor) The location preds for the image\n            along with the class predscores, Shape: [num_boxes,5].\n        match_metric: (str) IOU or IOS\n        match_threshold: (float) The overlap thresh for\n            match metric.\n    Returns:\n        A list of filtered indexes, Shape: [ ,]\n    \"\"\"\n\n    scores = predictions[:, 4].squeeze()\n    category_ids = predictions[:, 5].squeeze()\n    keep_mask = torch.zeros_like(category_ids, dtype=torch.bool)\n    for category_id in torch.unique(category_ids):\n        curr_indices = torch.where(category_ids == category_id)[0]\n        curr_keep_indices = nms(predictions[curr_indices], match_metric, match_threshold)\n        keep_mask[curr_indices[curr_keep_indices]] = True\n    keep_indices = torch.where(keep_mask)[0]\n    # sort selected indices by their scores\n    keep_indices = keep_indices[scores[keep_indices].sort(descending=True)[1]].tolist()\n    return keep_indices\n</code></pre> <code></code> <code>greedy_nmm(object_predictions_as_tensor, match_metric='IOU', match_threshold=0.5)</code> \u00b6 <p>Optimized greedy non-maximum merging for axis-aligned bounding boxes using STRTree.</p> <p>Parameters:</p> Name Type Description Default <code>object_predictions_as_tensor</code> \u00b6 <code>Tensor</code> <p>(tensor) The location preds for the image along with the class predscores, Shape: [num_boxes,5].</p> required <code>match_metric</code> \u00b6 <code>str</code> <p>(str) IOU or IOS</p> <code>'IOU'</code> <code>match_threshold</code> \u00b6 <code>float</code> <p>(float) The overlap thresh for match metric.</p> <code>0.5</code> <p>Returns:     keep_to_merge_list: (dict[int, list[int]]) mapping from prediction indices     to keep to a list of prediction indices to be merged.</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>def greedy_nmm(\n    object_predictions_as_tensor: torch.Tensor,\n    match_metric: str = \"IOU\",\n    match_threshold: float = 0.5,\n):\n    \"\"\"\n    Optimized greedy non-maximum merging for axis-aligned bounding boxes using STRTree.\n\n    Args:\n        object_predictions_as_tensor: (tensor) The location preds for the image\n            along with the class predscores, Shape: [num_boxes,5].\n        match_metric: (str) IOU or IOS\n        match_threshold: (float) The overlap thresh for match metric.\n    Returns:\n        keep_to_merge_list: (dict[int, list[int]]) mapping from prediction indices\n        to keep to a list of prediction indices to be merged.\n    \"\"\"\n    # Extract coordinates and scores as tensors\n    x1 = object_predictions_as_tensor[:, 0]\n    y1 = object_predictions_as_tensor[:, 1]\n    x2 = object_predictions_as_tensor[:, 2]\n    y2 = object_predictions_as_tensor[:, 3]\n    scores = object_predictions_as_tensor[:, 4]\n\n    # Calculate areas as tensor (vectorized operation)\n    areas = (x2 - x1) * (y2 - y1)\n\n    # Create Shapely boxes only once\n    boxes = []\n    for i in range(len(object_predictions_as_tensor)):\n        boxes.append(\n            box(\n                x1[i].item(),  # Convert only individual values\n                y1[i].item(),\n                x2[i].item(),\n                y2[i].item(),\n            )\n        )\n\n    # Sort indices by score (descending) using torch\n    sorted_idxs = torch.argsort(scores, descending=True).tolist()\n\n    # Build STRtree\n    tree = STRtree(boxes)\n\n    keep_to_merge_list = {}\n    suppressed = set()\n\n    for current_idx in sorted_idxs:\n        if current_idx in suppressed:\n            continue\n\n        current_box = boxes[current_idx]\n        current_area = areas[current_idx].item()  # Convert only when needed\n\n        # Query potential intersections using STRtree\n        candidate_idxs = tree.query(current_box)\n\n        merge_list = []\n        for candidate_idx in candidate_idxs:\n            if candidate_idx == current_idx or candidate_idx in suppressed:\n                continue\n\n            # Only consider candidates with lower or equal score\n            if scores[candidate_idx] &gt; scores[current_idx]:\n                continue\n\n            # For equal scores, use deterministic tie-breaking based on box coordinates\n            if scores[candidate_idx] == scores[current_idx]:\n                # Use box coordinates for stable ordering\n                current_coords = (\n                    x1[current_idx].item(),\n                    y1[current_idx].item(),\n                    x2[current_idx].item(),\n                    y2[current_idx].item(),\n                )\n                candidate_coords = (\n                    x1[candidate_idx].item(),\n                    y1[candidate_idx].item(),\n                    x2[candidate_idx].item(),\n                    y2[candidate_idx].item(),\n                )\n\n                # Compare coordinates lexicographically\n                if candidate_coords &gt; current_coords:\n                    continue\n\n            # Calculate intersection area\n            candidate_box = boxes[candidate_idx]\n            intersection = current_box.intersection(candidate_box).area\n\n            # Calculate metric\n            if match_metric == \"IOU\":\n                union = current_area + areas[candidate_idx].item() - intersection\n                metric = intersection / union if union &gt; 0 else 0\n            elif match_metric == \"IOS\":\n                smaller = min(current_area, areas[candidate_idx].item())\n                metric = intersection / smaller if smaller &gt; 0 else 0\n            else:\n                raise ValueError(\"Invalid match_metric\")\n\n            # Add to merge list if overlap exceeds threshold\n            if metric &gt;= match_threshold:\n                merge_list.append(candidate_idx)\n                suppressed.add(candidate_idx)\n\n        keep_to_merge_list[int(current_idx)] = [int(idx) for idx in merge_list]\n\n    return keep_to_merge_list\n</code></pre> <code></code> <code>nmm(object_predictions_as_tensor, match_metric='IOU', match_threshold=0.5)</code> \u00b6 <p>Apply non-maximum merging to avoid detecting too many overlapping bounding boxes for a given object.</p> <p>Parameters:</p> Name Type Description Default <code>object_predictions_as_tensor</code> \u00b6 <code>Tensor</code> <p>(tensor) The location preds for the image along with the class predscores, Shape: [num_boxes,5].</p> required <code>match_metric</code> \u00b6 <code>str</code> <p>(str) IOU or IOS</p> <code>'IOU'</code> <code>match_threshold</code> \u00b6 <code>float</code> <p>(float) The overlap thresh for match metric.</p> <code>0.5</code> <p>Returns:     keep_to_merge_list: (Dict[int:List[int]]) mapping from prediction indices     to keep to a list of prediction indices to be merged.</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>def nmm(\n    object_predictions_as_tensor: torch.Tensor,\n    match_metric: str = \"IOU\",\n    match_threshold: float = 0.5,\n):\n    \"\"\"Apply non-maximum merging to avoid detecting too many overlapping bounding boxes for a given object.\n\n    Args:\n        object_predictions_as_tensor: (tensor) The location preds for the image\n            along with the class predscores, Shape: [num_boxes,5].\n        match_metric: (str) IOU or IOS\n        match_threshold: (float) The overlap thresh for match metric.\n    Returns:\n        keep_to_merge_list: (Dict[int:List[int]]) mapping from prediction indices\n        to keep to a list of prediction indices to be merged.\n    \"\"\"\n    # Extract coordinates and scores as tensors\n    x1 = object_predictions_as_tensor[:, 0]\n    y1 = object_predictions_as_tensor[:, 1]\n    x2 = object_predictions_as_tensor[:, 2]\n    y2 = object_predictions_as_tensor[:, 3]\n    scores = object_predictions_as_tensor[:, 4]\n\n    # Calculate areas as tensor (vectorized operation)\n    areas = (x2 - x1) * (y2 - y1)\n\n    # Create Shapely boxes only once\n    boxes = []\n    for i in range(len(object_predictions_as_tensor)):\n        boxes.append(\n            box(\n                x1[i].item(),  # Convert only individual values\n                y1[i].item(),\n                x2[i].item(),\n                y2[i].item(),\n            )\n        )\n\n    # Sort indices by score (descending) using torch\n    sorted_idxs = torch.argsort(scores, descending=True).tolist()\n\n    # Build STRtree\n    tree = STRtree(boxes)\n\n    keep_to_merge_list = {}\n    merge_to_keep = {}\n\n    for current_idx in sorted_idxs:\n        current_box = boxes[current_idx]\n        current_area = areas[current_idx].item()  # Convert only when needed\n\n        # Query potential intersections using STRtree\n        candidate_idxs = tree.query(current_box)\n\n        matched_box_indices = []\n        for candidate_idx in candidate_idxs:\n            if candidate_idx == current_idx:\n                continue\n\n            # Only consider candidates with lower or equal score\n            if scores[candidate_idx] &gt; scores[current_idx]:\n                continue\n\n            # For equal scores, use deterministic tie-breaking based on box coordinates\n            if scores[candidate_idx] == scores[current_idx]:\n                # Use box coordinates for stable ordering\n                current_coords = (\n                    x1[current_idx].item(),\n                    y1[current_idx].item(),\n                    x2[current_idx].item(),\n                    y2[current_idx].item(),\n                )\n                candidate_coords = (\n                    x1[candidate_idx].item(),\n                    y1[candidate_idx].item(),\n                    x2[candidate_idx].item(),\n                    y2[candidate_idx].item(),\n                )\n\n                # Compare coordinates lexicographically\n                if candidate_coords &gt; current_coords:\n                    continue\n\n            # Calculate intersection area\n            candidate_box = boxes[candidate_idx]\n            intersection = current_box.intersection(candidate_box).area\n\n            # Calculate metric\n            if match_metric == \"IOU\":\n                union = current_area + areas[candidate_idx].item() - intersection\n                metric = intersection / union if union &gt; 0 else 0\n            elif match_metric == \"IOS\":\n                smaller = min(current_area, areas[candidate_idx].item())\n                metric = intersection / smaller if smaller &gt; 0 else 0\n            else:\n                raise ValueError(\"Invalid match_metric\")\n\n            # Add to matched list if overlap exceeds threshold\n            if metric &gt;= match_threshold:\n                matched_box_indices.append(candidate_idx)\n\n        # Convert current_idx to native Python int\n        current_idx_native = int(current_idx)\n\n        # Create keep_ind to merge_ind_list mapping\n        if current_idx_native not in merge_to_keep:\n            keep_to_merge_list[current_idx_native] = []\n\n            for matched_box_idx in matched_box_indices:\n                matched_box_idx_native = int(matched_box_idx)\n                if matched_box_idx_native not in merge_to_keep:\n                    keep_to_merge_list[current_idx_native].append(matched_box_idx_native)\n                    merge_to_keep[matched_box_idx_native] = current_idx_native\n        else:\n            keep_idx = merge_to_keep[current_idx_native]\n            for matched_box_idx in matched_box_indices:\n                matched_box_idx_native = int(matched_box_idx)\n                if (\n                    matched_box_idx_native not in keep_to_merge_list.get(keep_idx, [])\n                    and matched_box_idx_native not in merge_to_keep\n                ):\n                    if keep_idx not in keep_to_merge_list:\n                        keep_to_merge_list[keep_idx] = []\n                    keep_to_merge_list[keep_idx].append(matched_box_idx_native)\n                    merge_to_keep[matched_box_idx_native] = keep_idx\n\n    return keep_to_merge_list\n</code></pre> <code></code> <code>nms(predictions, match_metric='IOU', match_threshold=0.5)</code> \u00b6 <p>Optimized non-maximum suppression for axis-aligned bounding boxes using STRTree.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> \u00b6 <code>Tensor</code> <p>(tensor) The location preds for the image along with the class predscores, Shape: [num_boxes,5].</p> required <code>match_metric</code> \u00b6 <code>str</code> <p>(str) IOU or IOS</p> <code>'IOU'</code> <code>match_threshold</code> \u00b6 <code>float</code> <p>(float) The overlap thresh for match metric.</p> <code>0.5</code> <p>Returns:</p> Type Description <p>A list of filtered indexes, Shape: [ ,]</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>def nms(\n    predictions: torch.Tensor,\n    match_metric: str = \"IOU\",\n    match_threshold: float = 0.5,\n):\n    \"\"\"\n    Optimized non-maximum suppression for axis-aligned bounding boxes using STRTree.\n\n    Args:\n        predictions: (tensor) The location preds for the image along with the class\n            predscores, Shape: [num_boxes,5].\n        match_metric: (str) IOU or IOS\n        match_threshold: (float) The overlap thresh for match metric.\n\n    Returns:\n        A list of filtered indexes, Shape: [ ,]\n    \"\"\"\n\n    # Extract coordinates and scores as tensors\n    x1 = predictions[:, 0]\n    y1 = predictions[:, 1]\n    x2 = predictions[:, 2]\n    y2 = predictions[:, 3]\n    scores = predictions[:, 4]\n\n    # Calculate areas as tensor (vectorized operation)\n    areas = (x2 - x1) * (y2 - y1)\n\n    # Create Shapely boxes only once\n    boxes = []\n    for i in range(len(predictions)):\n        boxes.append(\n            box(\n                x1[i].item(),  # Convert only individual values\n                y1[i].item(),\n                x2[i].item(),\n                y2[i].item(),\n            )\n        )\n\n    # Sort indices by score (descending) using torch\n    sorted_idxs = torch.argsort(scores, descending=True).tolist()\n\n    # Build STRtree\n    tree = STRtree(boxes)\n\n    keep = []\n    suppressed = set()\n\n    for current_idx in sorted_idxs:\n        if current_idx in suppressed:\n            continue\n\n        keep.append(current_idx)\n        current_box = boxes[current_idx]\n        current_area = areas[current_idx].item()  # Convert only when needed\n\n        # Query potential intersections using STRtree\n        candidate_idxs = tree.query(current_box)\n\n        for candidate_idx in candidate_idxs:\n            if candidate_idx == current_idx or candidate_idx in suppressed:\n                continue\n\n            # Skip candidates with higher scores (already processed)\n            if scores[candidate_idx] &gt; scores[current_idx]:\n                continue\n\n            # For equal scores, use deterministic tie-breaking based on box coordinates\n            if scores[candidate_idx] == scores[current_idx]:\n                # Use box coordinates for stable ordering\n                current_coords = (\n                    x1[current_idx].item(),\n                    y1[current_idx].item(),\n                    x2[current_idx].item(),\n                    y2[current_idx].item(),\n                )\n                candidate_coords = (\n                    x1[candidate_idx].item(),\n                    y1[candidate_idx].item(),\n                    x2[candidate_idx].item(),\n                    y2[candidate_idx].item(),\n                )\n\n                # Compare coordinates lexicographically\n                if candidate_coords &gt; current_coords:\n                    continue\n\n            # Calculate intersection area\n            candidate_box = boxes[candidate_idx]\n            intersection = current_box.intersection(candidate_box).area\n\n            # Calculate metric\n            if match_metric == \"IOU\":\n                union = current_area + areas[candidate_idx].item() - intersection\n                metric = intersection / union if union &gt; 0 else 0\n            elif match_metric == \"IOS\":\n                smaller = min(current_area, areas[candidate_idx].item())\n                metric = intersection / smaller if smaller &gt; 0 else 0\n            else:\n                raise ValueError(\"Invalid match_metric\")\n\n            # Suppress if overlap exceeds threshold\n            if metric &gt;= match_threshold:\n                suppressed.add(candidate_idx)\n\n    return keep\n</code></pre>"},{"location":"api/#sahi.postprocess.legacy","title":"<code>legacy</code>","text":"Modules\u00b6 <code>combine</code> \u00b6 Classes\u00b6 <code></code> <code>PostprocessPredictions</code> \u00b6 <p>Utilities for calculating IOU/IOS based match for given ObjectPredictions.</p> Source code in <code>sahi/postprocess/legacy/combine.py</code> <pre><code>class PostprocessPredictions:\n    \"\"\"Utilities for calculating IOU/IOS based match for given ObjectPredictions.\"\"\"\n\n    def __init__(\n        self,\n        match_threshold: float = 0.5,\n        match_metric: str = \"IOU\",\n        class_agnostic: bool = True,\n    ):\n        self.match_threshold = match_threshold\n        self.class_agnostic = class_agnostic\n        if match_metric == \"IOU\":\n            self.calculate_match = self.calculate_bbox_iou\n        elif match_metric == \"IOS\":\n            self.calculate_match = self.calculate_bbox_ios\n        else:\n            raise ValueError(f\"'match_metric' should be one of ['IOU', 'IOS'] but given as {match_metric}\")\n\n    def _has_match(self, pred1: ObjectPrediction, pred2: ObjectPrediction) -&gt; bool:\n        threshold_condition = self.calculate_match(pred1, pred2) &gt; self.match_threshold\n        category_condition = self.has_same_category_id(pred1, pred2) or self.class_agnostic\n        return threshold_condition and category_condition\n\n    @staticmethod\n    def get_score_func(object_prediction: ObjectPrediction):\n        \"\"\"Used for sorting predictions.\"\"\"\n        return object_prediction.score.value\n\n    @staticmethod\n    def has_same_category_id(pred1: ObjectPrediction, pred2: ObjectPrediction) -&gt; bool:\n        return pred1.category.id == pred2.category.id\n\n    @staticmethod\n    def calculate_bbox_iou(pred1: ObjectPrediction, pred2: ObjectPrediction) -&gt; float:\n        \"\"\"Returns the ratio of intersection area to the union.\"\"\"\n        box1 = np.array(pred1.bbox.to_xyxy())\n        box2 = np.array(pred2.bbox.to_xyxy())\n        area1 = calculate_area(box1)\n        area2 = calculate_area(box2)\n        intersect = calculate_intersection_area(box1, box2)\n        return intersect / (area1 + area2 - intersect)\n\n    @staticmethod\n    def calculate_bbox_ios(pred1: ObjectPrediction, pred2: ObjectPrediction) -&gt; float:\n        \"\"\"Returns the ratio of intersection area to the smaller box's area.\"\"\"\n        box1 = np.array(pred1.bbox.to_xyxy())\n        box2 = np.array(pred2.bbox.to_xyxy())\n        area1 = calculate_area(box1)\n        area2 = calculate_area(box2)\n        intersect = calculate_intersection_area(box1, box2)\n        smaller_area = np.minimum(area1, area2)\n        return intersect / smaller_area\n\n    def __call__(self):\n        raise NotImplementedError()\n</code></pre> Functions\u00b6 <code></code> <code>calculate_bbox_ios(pred1, pred2)</code> <code>staticmethod</code> \u00b6 <p>Returns the ratio of intersection area to the smaller box's area.</p> Source code in <code>sahi/postprocess/legacy/combine.py</code> <pre><code>@staticmethod\ndef calculate_bbox_ios(pred1: ObjectPrediction, pred2: ObjectPrediction) -&gt; float:\n    \"\"\"Returns the ratio of intersection area to the smaller box's area.\"\"\"\n    box1 = np.array(pred1.bbox.to_xyxy())\n    box2 = np.array(pred2.bbox.to_xyxy())\n    area1 = calculate_area(box1)\n    area2 = calculate_area(box2)\n    intersect = calculate_intersection_area(box1, box2)\n    smaller_area = np.minimum(area1, area2)\n    return intersect / smaller_area\n</code></pre> <code></code> <code>calculate_bbox_iou(pred1, pred2)</code> <code>staticmethod</code> \u00b6 <p>Returns the ratio of intersection area to the union.</p> Source code in <code>sahi/postprocess/legacy/combine.py</code> <pre><code>@staticmethod\ndef calculate_bbox_iou(pred1: ObjectPrediction, pred2: ObjectPrediction) -&gt; float:\n    \"\"\"Returns the ratio of intersection area to the union.\"\"\"\n    box1 = np.array(pred1.bbox.to_xyxy())\n    box2 = np.array(pred2.bbox.to_xyxy())\n    area1 = calculate_area(box1)\n    area2 = calculate_area(box2)\n    intersect = calculate_intersection_area(box1, box2)\n    return intersect / (area1 + area2 - intersect)\n</code></pre> <code></code> <code>get_score_func(object_prediction)</code> <code>staticmethod</code> \u00b6 <p>Used for sorting predictions.</p> Source code in <code>sahi/postprocess/legacy/combine.py</code> <pre><code>@staticmethod\ndef get_score_func(object_prediction: ObjectPrediction):\n    \"\"\"Used for sorting predictions.\"\"\"\n    return object_prediction.score.value\n</code></pre> Functions\u00b6"},{"location":"api/#sahi.postprocess.utils","title":"<code>utils</code>","text":"Classes\u00b6 Functions\u00b6 <code>calculate_area(box)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>box</code> \u00b6 <code>List[int]</code> <p>[x1, y1, x2, y2]</p> required Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def calculate_area(box: list[int] | np.ndarray) -&gt; float:\n    \"\"\"\n    Args:\n        box (List[int]): [x1, y1, x2, y2]\n    \"\"\"\n    return (box[2] - box[0]) * (box[3] - box[1])\n</code></pre> <code></code> <code>calculate_bbox_ios(pred1, pred2)</code> \u00b6 <p>Returns the ratio of intersection area to the smaller box's area.</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def calculate_bbox_ios(pred1: ObjectPrediction, pred2: ObjectPrediction) -&gt; float:\n    \"\"\"Returns the ratio of intersection area to the smaller box's area.\"\"\"\n    box1 = np.array(pred1.bbox.to_xyxy())\n    box2 = np.array(pred2.bbox.to_xyxy())\n    area1 = calculate_area(box1)\n    area2 = calculate_area(box2)\n    intersect = calculate_intersection_area(box1, box2)\n    smaller_area = np.minimum(area1, area2)\n    return intersect / smaller_area\n</code></pre> <code></code> <code>calculate_bbox_iou(pred1, pred2)</code> \u00b6 <p>Returns the ratio of intersection area to the union.</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def calculate_bbox_iou(pred1: ObjectPrediction, pred2: ObjectPrediction) -&gt; float:\n    \"\"\"Returns the ratio of intersection area to the union.\"\"\"\n    box1 = np.array(pred1.bbox.to_xyxy())\n    box2 = np.array(pred2.bbox.to_xyxy())\n    area1 = calculate_area(box1)\n    area2 = calculate_area(box2)\n    intersect = calculate_intersection_area(box1, box2)\n    return intersect / (area1 + area2 - intersect)\n</code></pre> <code></code> <code>calculate_box_union(box1, box2)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>box1</code> \u00b6 <code>List[int]</code> <p>[x1, y1, x2, y2]</p> required <code>box2</code> \u00b6 <code>List[int]</code> <p>[x1, y1, x2, y2]</p> required Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def calculate_box_union(box1: list[int] | np.ndarray, box2: list[int] | np.ndarray) -&gt; list[int]:\n    \"\"\"\n    Args:\n        box1 (List[int]): [x1, y1, x2, y2]\n        box2 (List[int]): [x1, y1, x2, y2]\n    \"\"\"\n    box1 = np.array(box1)\n    box2 = np.array(box2)\n    left_top = np.minimum(box1[:2], box2[:2])\n    right_bottom = np.maximum(box1[2:], box2[2:])\n    return list(np.concatenate((left_top, right_bottom)))\n</code></pre> <code></code> <code>calculate_intersection_area(box1, box2)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>box1</code> \u00b6 <code>ndarray</code> <p>np.array([x1, y1, x2, y2])</p> required <code>box2</code> \u00b6 <code>ndarray</code> <p>np.array([x1, y1, x2, y2])</p> required Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def calculate_intersection_area(box1: np.ndarray, box2: np.ndarray) -&gt; float:\n    \"\"\"\n    Args:\n        box1 (np.ndarray): np.array([x1, y1, x2, y2])\n        box2 (np.ndarray): np.array([x1, y1, x2, y2])\n    \"\"\"\n    left_top = np.maximum(box1[:2], box2[:2])\n    right_bottom = np.minimum(box1[2:], box2[2:])\n    width_height = (right_bottom - left_top).clip(min=0)\n    return width_height[0] * width_height[1]\n</code></pre> <code></code> <code>coco_segmentation_to_shapely(segmentation)</code> \u00b6 <p>Fix segment data in COCO format :param segmentation: segment data in COCO format :return:</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def coco_segmentation_to_shapely(segmentation: list | list[list]):\n    \"\"\"Fix segment data in COCO format :param segmentation: segment data in COCO format :return:\"\"\"\n    if isinstance(segmentation, list) and all([not isinstance(seg, list) for seg in segmentation]):\n        segmentation = [segmentation]\n    elif isinstance(segmentation, list) and all([isinstance(seg, list) for seg in segmentation]):\n        pass\n    else:\n        raise ValueError(\"segmentation must be List or List[List]\")\n\n    polygon_list = []\n\n    for coco_polygon in segmentation:\n        point_list = list(zip(coco_polygon[::2], coco_polygon[1::2]))\n        shapely_polygon = Polygon(point_list)\n        polygon_list.append(repair_polygon(shapely_polygon))\n\n    shapely_multipolygon = repair_multipolygon(MultiPolygon(polygon_list))\n    return shapely_multipolygon\n</code></pre> <code></code> <code>object_prediction_list_to_numpy(object_prediction_list)</code> \u00b6 <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray of size N x [x1, y1, x2, y2, score, category_id]</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def object_prediction_list_to_numpy(object_prediction_list: ObjectPredictionList) -&gt; np.ndarray:\n    \"\"\"\n    Returns:\n        np.ndarray of size N x [x1, y1, x2, y2, score, category_id]\n    \"\"\"\n    num_predictions = len(object_prediction_list)\n    numpy_predictions = np.zeros([num_predictions, 6], dtype=np.float32)\n    for ind, object_prediction in enumerate(object_prediction_list):\n        numpy_predictions[ind, :4] = np.array(object_prediction.tolist().bbox.to_xyxy(), dtype=np.float32)\n        numpy_predictions[ind, 4] = object_prediction.tolist().score.value\n        numpy_predictions[ind, 5] = object_prediction.tolist().category.id\n    return numpy_predictions\n</code></pre> <code></code> <code>object_prediction_list_to_torch(object_prediction_list)</code> \u00b6 <p>Returns:</p> Type Description <code>tensor</code> <p>torch.tensor of size N x [x1, y1, x2, y2, score, category_id]</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def object_prediction_list_to_torch(object_prediction_list: ObjectPredictionList) -&gt; torch.tensor:\n    \"\"\"\n    Returns:\n        torch.tensor of size N x [x1, y1, x2, y2, score, category_id]\n    \"\"\"\n    num_predictions = len(object_prediction_list)\n    torch_predictions = torch.zeros([num_predictions, 6], dtype=torch.float32)\n    for ind, object_prediction in enumerate(object_prediction_list):\n        torch_predictions[ind, :4] = torch.tensor(object_prediction.tolist().bbox.to_xyxy(), dtype=torch.float32)\n        torch_predictions[ind, 4] = object_prediction.tolist().score.value\n        torch_predictions[ind, 5] = object_prediction.tolist().category.id\n    return torch_predictions\n</code></pre> <code></code> <code>repair_multipolygon(shapely_multipolygon)</code> \u00b6 <p>Fix invalid MultiPolygon objects :param shapely_multipolygon: Imported shapely MultiPolygon object :return:</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def repair_multipolygon(shapely_multipolygon: MultiPolygon) -&gt; MultiPolygon:\n    \"\"\"Fix invalid MultiPolygon objects :param shapely_multipolygon: Imported shapely MultiPolygon object :return:\"\"\"\n    if not shapely_multipolygon.is_valid:\n        fixed_geometry = shapely_multipolygon.buffer(0)\n\n        if fixed_geometry.is_valid:\n            if isinstance(fixed_geometry, MultiPolygon):\n                return fixed_geometry\n            elif isinstance(fixed_geometry, Polygon):\n                return MultiPolygon([fixed_geometry])\n            elif isinstance(fixed_geometry, GeometryCollection):\n                polygons = [geom for geom in fixed_geometry.geoms if isinstance(geom, Polygon)]\n                return MultiPolygon(polygons) if polygons else shapely_multipolygon\n\n    return shapely_multipolygon\n</code></pre> <code></code> <code>repair_polygon(shapely_polygon)</code> \u00b6 <p>Fix polygons :param shapely_polygon: Shapely polygon object :return:</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def repair_polygon(shapely_polygon: Polygon) -&gt; Polygon:\n    \"\"\"Fix polygons :param shapely_polygon: Shapely polygon object :return:\"\"\"\n    if not shapely_polygon.is_valid:\n        fixed_polygon = shapely_polygon.buffer(0)\n        if fixed_polygon.is_valid:\n            if isinstance(fixed_polygon, Polygon):\n                return fixed_polygon\n            elif isinstance(fixed_polygon, MultiPolygon):\n                return max(fixed_polygon.geoms, key=lambda p: p.area)\n            elif isinstance(fixed_polygon, GeometryCollection):\n                polygons = [geom for geom in fixed_polygon.geoms if isinstance(geom, Polygon)]\n                return max(polygons, key=lambda p: p.area) if polygons else shapely_polygon\n\n    return shapely_polygon\n</code></pre>"},{"location":"api/#sahi.predict","title":"<code>predict</code>","text":""},{"location":"api/#sahi.predict-classes","title":"Classes","text":""},{"location":"api/#sahi.predict-functions","title":"Functions","text":""},{"location":"api/#sahi.predict.bbox_sort","title":"<code>bbox_sort(a, b, thresh)</code>","text":"<p>a, b  - function receives two bounding bboxes</p> <p>thresh - the threshold takes into account how far two bounding bboxes differ in Y where thresh is the threshold we set for the minimum allowable difference in height between adjacent bboxes and sorts them by the X coordinate</p> Source code in <code>sahi/predict.py</code> <pre><code>def bbox_sort(a, b, thresh):\n    \"\"\"\n    a, b  - function receives two bounding bboxes\n\n    thresh - the threshold takes into account how far two bounding bboxes differ in\n    Y where thresh is the threshold we set for the\n    minimum allowable difference in height between adjacent bboxes\n    and sorts them by the X coordinate\n    \"\"\"\n\n    bbox_a = a\n    bbox_b = b\n\n    if abs(bbox_a[1] - bbox_b[1]) &lt;= thresh:\n        return bbox_a[0] - bbox_b[0]\n\n    return bbox_a[1] - bbox_b[1]\n</code></pre>"},{"location":"api/#sahi.predict.get_prediction","title":"<code>get_prediction(image, detection_model, shift_amount=[0, 0], full_shape=None, postprocess=None, verbose=0, exclude_classes_by_name=None, exclude_classes_by_id=None)</code>","text":"<p>Function for performing prediction for given image using given detection_model.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <p>str or np.ndarray Location of image or numpy image matrix to slice</p> required <code>detection_model</code> \u00b6 <p>model.DetectionMode</p> required <code>shift_amount</code> \u00b6 <code>list</code> <p>List To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> <code>full_shape</code> \u00b6 <p>List Size of the full image, should be in the form of [height, width]</p> <code>None</code> <code>postprocess</code> \u00b6 <code>PostprocessPredictions | None</code> <p>sahi.postprocess.combine.PostprocessPredictions</p> <code>None</code> <code>verbose</code> \u00b6 <code>int</code> <p>int 0: no print (default) 1: print prediction duration</p> <code>0</code> <code>exclude_classes_by_name</code> \u00b6 <code>list[str] | None</code> <p>Optional[List[str]] None: if no classes are excluded List[str]: set of classes to exclude using its/their class label name/s</p> <code>None</code> <code>exclude_classes_by_id</code> \u00b6 <code>list[int] | None</code> <p>Optional[List[int]] None: if no classes are excluded List[int]: set of classes to exclude using one or more IDs</p> <code>None</code> <p>Returns:     A dict with fields:         object_prediction_list: a list of ObjectPrediction         durations_in_seconds: a dict containing elapsed times for profiling</p> Source code in <code>sahi/predict.py</code> <pre><code>def get_prediction(\n    image,\n    detection_model,\n    shift_amount: list = [0, 0],\n    full_shape=None,\n    postprocess: PostprocessPredictions | None = None,\n    verbose: int = 0,\n    exclude_classes_by_name: list[str] | None = None,\n    exclude_classes_by_id: list[int] | None = None,\n) -&gt; PredictionResult:\n    \"\"\"Function for performing prediction for given image using given detection_model.\n\n    Args:\n        image: str or np.ndarray\n            Location of image or numpy image matrix to slice\n        detection_model: model.DetectionMode\n        shift_amount: List\n            To shift the box and mask predictions from sliced image to full\n            sized image, should be in the form of [shift_x, shift_y]\n        full_shape: List\n            Size of the full image, should be in the form of [height, width]\n        postprocess: sahi.postprocess.combine.PostprocessPredictions\n        verbose: int\n            0: no print (default)\n            1: print prediction duration\n        exclude_classes_by_name: Optional[List[str]]\n            None: if no classes are excluded\n            List[str]: set of classes to exclude using its/their class label name/s\n        exclude_classes_by_id: Optional[List[int]]\n            None: if no classes are excluded\n            List[int]: set of classes to exclude using one or more IDs\n    Returns:\n        A dict with fields:\n            object_prediction_list: a list of ObjectPrediction\n            durations_in_seconds: a dict containing elapsed times for profiling\n    \"\"\"\n    durations_in_seconds = dict()\n\n    # read image as pil\n    image_as_pil = read_image_as_pil(image)\n    # get prediction\n    time_start = time.time()\n    detection_model.perform_inference(np.ascontiguousarray(image_as_pil))\n    time_end = time.time() - time_start\n    durations_in_seconds[\"prediction\"] = time_end\n\n    if full_shape is None:\n        full_shape = [image_as_pil.height, image_as_pil.width]\n\n    # process prediction\n    time_start = time.time()\n    # works only with 1 batch\n    detection_model.convert_original_predictions(\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n    object_prediction_list: list[ObjectPrediction] = detection_model.object_prediction_list\n    object_prediction_list = filter_predictions(object_prediction_list, exclude_classes_by_name, exclude_classes_by_id)\n\n    # postprocess matching predictions\n    if postprocess is not None:\n        object_prediction_list = postprocess(object_prediction_list)\n\n    time_end = time.time() - time_start\n    durations_in_seconds[\"postprocess\"] = time_end\n\n    if verbose == 1:\n        print(\n            \"Prediction performed in\",\n            durations_in_seconds[\"prediction\"],\n            \"seconds.\",\n        )\n\n    return PredictionResult(\n        image=image, object_prediction_list=object_prediction_list, durations_in_seconds=durations_in_seconds\n    )\n</code></pre>"},{"location":"api/#sahi.predict.get_sliced_prediction","title":"<code>get_sliced_prediction(image, detection_model=None, slice_height=None, slice_width=None, overlap_height_ratio=0.2, overlap_width_ratio=0.2, perform_standard_pred=True, postprocess_type='GREEDYNMM', postprocess_match_metric='IOS', postprocess_match_threshold=0.5, postprocess_class_agnostic=False, verbose=1, merge_buffer_length=None, auto_slice_resolution=True, slice_export_prefix=None, slice_dir=None, exclude_classes_by_name=None, exclude_classes_by_id=None, progress_bar=False, progress_callback=None)</code>","text":"<p>Function for slice image + get predicion for each slice + combine predictions in full image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <p>str or np.ndarray Location of image or numpy image matrix to slice</p> required <code>detection_model</code> \u00b6 <p>model.DetectionModel</p> <code>None</code> <code>slice_height</code> \u00b6 <code>int | None</code> <p>int Height of each slice.  Defaults to <code>None</code>.</p> <code>None</code> <code>slice_width</code> \u00b6 <code>int | None</code> <p>int Width of each slice.  Defaults to <code>None</code>.</p> <code>None</code> <code>overlap_height_ratio</code> \u00b6 <code>float</code> <p>float Fractional overlap in height of each window (e.g. an overlap of 0.2 for a window of size 512 yields an overlap of 102 pixels). Default to <code>0.2</code>.</p> <code>0.2</code> <code>overlap_width_ratio</code> \u00b6 <code>float</code> <p>float Fractional overlap in width of each window (e.g. an overlap of 0.2 for a window of size 512 yields an overlap of 102 pixels). Default to <code>0.2</code>.</p> <code>0.2</code> <code>perform_standard_pred</code> \u00b6 <code>bool</code> <p>bool Perform a standard prediction on top of sliced predictions to increase large object detection accuracy. Default: True.</p> <code>True</code> <code>postprocess_type</code> \u00b6 <code>str</code> <p>str Type of the postprocess to be used after sliced inference while merging/eliminating predictions. Options are 'NMM', 'GREEDYNMM' or 'NMS'. Default is 'GREEDYNMM'.</p> <code>'GREEDYNMM'</code> <code>postprocess_match_metric</code> \u00b6 <code>str</code> <p>str Metric to be used during object prediction matching after sliced prediction. 'IOU' for intersection over union, 'IOS' for intersection over smaller area.</p> <code>'IOS'</code> <code>postprocess_match_threshold</code> \u00b6 <code>float</code> <p>float Sliced predictions having higher iou than postprocess_match_threshold will be postprocessed after sliced prediction.</p> <code>0.5</code> <code>postprocess_class_agnostic</code> \u00b6 <code>bool</code> <p>bool If True, postprocess will ignore category ids.</p> <code>False</code> <code>verbose</code> \u00b6 <code>int</code> <p>int 0: no print 1: print number of slices (default) 2: print number of slices and slice/prediction durations</p> <code>1</code> <code>merge_buffer_length</code> \u00b6 <code>int | None</code> <p>int The length of buffer for slices to be used during sliced prediction, which is suitable for low memory. It may affect the AP if it is specified. The higher the amount, the closer results to the non-buffered. scenario. See the discussion.</p> <code>None</code> <code>auto_slice_resolution</code> \u00b6 <code>bool</code> <p>bool if slice parameters (slice_height, slice_width) are not given, it enables automatically calculate these params from image resolution and orientation.</p> <code>True</code> <code>slice_export_prefix</code> \u00b6 <code>str | None</code> <p>str Prefix for the exported slices. Defaults to None.</p> <code>None</code> <code>slice_dir</code> \u00b6 <code>str | None</code> <p>str Directory to save the slices. Defaults to None.</p> <code>None</code> <code>exclude_classes_by_name</code> \u00b6 <code>list[str] | None</code> <p>Optional[List[str]] None: if no classes are excluded List[str]: set of classes to exclude using its/their class label name/s</p> <code>None</code> <code>exclude_classes_by_id</code> \u00b6 <code>list[int] | None</code> <p>Optional[List[int]] None: if no classes are excluded List[int]: set of classes to exclude using one or more IDs</p> <code>None</code> <code>progress_bar</code> \u00b6 <code>bool</code> <p>bool Whether to show progress bar for slice processing. Default: False.</p> <code>False</code> <code>progress_callback</code> \u00b6 <p>callable A callback function that will be called after each slice is processed. The function should accept two arguments: (current_slice, total_slices)</p> <code>None</code> <p>Returns:     A Dict with fields:         object_prediction_list: a list of sahi.prediction.ObjectPrediction         durations_in_seconds: a dict containing elapsed times for profiling</p> Source code in <code>sahi/predict.py</code> <pre><code>def get_sliced_prediction(\n    image,\n    detection_model=None,\n    slice_height: int | None = None,\n    slice_width: int | None = None,\n    overlap_height_ratio: float = 0.2,\n    overlap_width_ratio: float = 0.2,\n    perform_standard_pred: bool = True,\n    postprocess_type: str = \"GREEDYNMM\",\n    postprocess_match_metric: str = \"IOS\",\n    postprocess_match_threshold: float = 0.5,\n    postprocess_class_agnostic: bool = False,\n    verbose: int = 1,\n    merge_buffer_length: int | None = None,\n    auto_slice_resolution: bool = True,\n    slice_export_prefix: str | None = None,\n    slice_dir: str | None = None,\n    exclude_classes_by_name: list[str] | None = None,\n    exclude_classes_by_id: list[int] | None = None,\n    progress_bar: bool = False,\n    progress_callback=None,\n) -&gt; PredictionResult:\n    \"\"\"Function for slice image + get predicion for each slice + combine predictions in full image.\n\n    Args:\n        image: str or np.ndarray\n            Location of image or numpy image matrix to slice\n        detection_model: model.DetectionModel\n        slice_height: int\n            Height of each slice.  Defaults to ``None``.\n        slice_width: int\n            Width of each slice.  Defaults to ``None``.\n        overlap_height_ratio: float\n            Fractional overlap in height of each window (e.g. an overlap of 0.2 for a window\n            of size 512 yields an overlap of 102 pixels).\n            Default to ``0.2``.\n        overlap_width_ratio: float\n            Fractional overlap in width of each window (e.g. an overlap of 0.2 for a window\n            of size 512 yields an overlap of 102 pixels).\n            Default to ``0.2``.\n        perform_standard_pred: bool\n            Perform a standard prediction on top of sliced predictions to increase large object\n            detection accuracy. Default: True.\n        postprocess_type: str\n            Type of the postprocess to be used after sliced inference while merging/eliminating predictions.\n            Options are 'NMM', 'GREEDYNMM' or 'NMS'. Default is 'GREEDYNMM'.\n        postprocess_match_metric: str\n            Metric to be used during object prediction matching after sliced prediction.\n            'IOU' for intersection over union, 'IOS' for intersection over smaller area.\n        postprocess_match_threshold: float\n            Sliced predictions having higher iou than postprocess_match_threshold will be\n            postprocessed after sliced prediction.\n        postprocess_class_agnostic: bool\n            If True, postprocess will ignore category ids.\n        verbose: int\n            0: no print\n            1: print number of slices (default)\n            2: print number of slices and slice/prediction durations\n        merge_buffer_length: int\n            The length of buffer for slices to be used during sliced prediction, which is suitable for low memory.\n            It may affect the AP if it is specified. The higher the amount, the closer results to the non-buffered.\n            scenario. See [the discussion](https://github.com/obss/sahi/pull/445).\n        auto_slice_resolution: bool\n            if slice parameters (slice_height, slice_width) are not given,\n            it enables automatically calculate these params from image resolution and orientation.\n        slice_export_prefix: str\n            Prefix for the exported slices. Defaults to None.\n        slice_dir: str\n            Directory to save the slices. Defaults to None.\n        exclude_classes_by_name: Optional[List[str]]\n            None: if no classes are excluded\n            List[str]: set of classes to exclude using its/their class label name/s\n        exclude_classes_by_id: Optional[List[int]]\n            None: if no classes are excluded\n            List[int]: set of classes to exclude using one or more IDs\n        progress_bar: bool\n            Whether to show progress bar for slice processing. Default: False.\n        progress_callback: callable\n            A callback function that will be called after each slice is processed.\n            The function should accept two arguments: (current_slice, total_slices)\n    Returns:\n        A Dict with fields:\n            object_prediction_list: a list of sahi.prediction.ObjectPrediction\n            durations_in_seconds: a dict containing elapsed times for profiling\n    \"\"\"\n\n    # for profiling\n    durations_in_seconds = dict()\n\n    # currently only 1 batch supported\n    num_batch = 1\n    # create slices from full image\n    time_start = time.time()\n    slice_image_result = slice_image(\n        image=image,\n        output_file_name=slice_export_prefix,\n        output_dir=slice_dir,\n        slice_height=slice_height,\n        slice_width=slice_width,\n        overlap_height_ratio=overlap_height_ratio,\n        overlap_width_ratio=overlap_width_ratio,\n        auto_slice_resolution=auto_slice_resolution,\n    )\n    from sahi.models.ultralytics import UltralyticsDetectionModel\n\n    num_slices = len(slice_image_result)\n    time_end = time.time() - time_start\n    durations_in_seconds[\"slice\"] = time_end\n\n    if isinstance(detection_model, UltralyticsDetectionModel) and detection_model.is_obb:\n        # Only NMS is supported for OBB model outputs\n        postprocess_type = \"NMS\"\n\n    # init match postprocess instance\n    if postprocess_type not in POSTPROCESS_NAME_TO_CLASS.keys():\n        raise ValueError(\n            f\"postprocess_type should be one of {list(POSTPROCESS_NAME_TO_CLASS.keys())} \"\n            f\"but given as {postprocess_type}\"\n        )\n    postprocess_constructor = POSTPROCESS_NAME_TO_CLASS[postprocess_type]\n    postprocess = postprocess_constructor(\n        match_threshold=postprocess_match_threshold,\n        match_metric=postprocess_match_metric,\n        class_agnostic=postprocess_class_agnostic,\n    )\n\n    postprocess_time = 0\n    time_start = time.time()\n    # create prediction input\n    num_group = int(num_slices / num_batch)\n    if verbose == 1 or verbose == 2:\n        tqdm.write(f\"Performing prediction on {num_slices} slices.\")\n\n    if progress_bar:\n        slice_iterator = tqdm(range(num_group), desc=\"Processing slices\", total=num_group)\n    else:\n        slice_iterator = range(num_group)\n\n    object_prediction_list = []\n    # perform sliced prediction\n    for group_ind in slice_iterator:\n        # prepare batch (currently supports only 1 batch)\n        image_list = []\n        shift_amount_list = []\n        for image_ind in range(num_batch):\n            image_list.append(slice_image_result.images[group_ind * num_batch + image_ind])\n            shift_amount_list.append(slice_image_result.starting_pixels[group_ind * num_batch + image_ind])\n        # perform batch prediction\n        prediction_result = get_prediction(\n            image=image_list[0],\n            detection_model=detection_model,\n            shift_amount=shift_amount_list[0],\n            full_shape=[\n                slice_image_result.original_image_height,\n                slice_image_result.original_image_width,\n            ],\n            exclude_classes_by_name=exclude_classes_by_name,\n            exclude_classes_by_id=exclude_classes_by_id,\n        )\n        # convert sliced predictions to full predictions\n        for object_prediction in prediction_result.object_prediction_list:\n            if object_prediction:  # if not empty\n                object_prediction_list.append(object_prediction.get_shifted_object_prediction())\n\n        # merge matching predictions during sliced prediction\n        if merge_buffer_length is not None and len(object_prediction_list) &gt; merge_buffer_length:\n            postprocess_time_start = time.time()\n            object_prediction_list = postprocess(object_prediction_list)\n            postprocess_time += time.time() - postprocess_time_start\n\n        # Call progress callback if provided\n        if progress_callback is not None:\n            progress_callback(group_ind + 1, num_group)\n\n    # perform standard prediction\n    if num_slices &gt; 1 and perform_standard_pred:\n        prediction_result = get_prediction(\n            image=image,\n            detection_model=detection_model,\n            shift_amount=[0, 0],\n            full_shape=[\n                slice_image_result.original_image_height,\n                slice_image_result.original_image_width,\n            ],\n            postprocess=None,\n            exclude_classes_by_name=exclude_classes_by_name,\n            exclude_classes_by_id=exclude_classes_by_id,\n        )\n        object_prediction_list.extend(prediction_result.object_prediction_list)\n\n    # merge matching predictions\n    if len(object_prediction_list) &gt; 1:\n        postprocess_time_start = time.time()\n        object_prediction_list = postprocess(object_prediction_list)\n        postprocess_time += time.time() - postprocess_time_start\n\n    time_end = time.time() - time_start\n    durations_in_seconds[\"prediction\"] = time_end - postprocess_time\n    durations_in_seconds[\"postprocess\"] = postprocess_time\n\n    if verbose == 2:\n        print(\n            \"Slicing performed in\",\n            durations_in_seconds[\"slice\"],\n            \"seconds.\",\n        )\n        print(\n            \"Prediction performed in\",\n            durations_in_seconds[\"prediction\"],\n            \"seconds.\",\n        )\n        print(\n            \"Postprocessing performed in\",\n            durations_in_seconds[\"postprocess\"],\n            \"seconds.\",\n        )\n\n    return PredictionResult(\n        image=image, object_prediction_list=object_prediction_list, durations_in_seconds=durations_in_seconds\n    )\n</code></pre>"},{"location":"api/#sahi.predict.predict","title":"<code>predict(detection_model=None, model_type='ultralytics', model_path=None, model_config_path=None, model_confidence_threshold=0.25, model_device=None, model_category_mapping=None, model_category_remapping=None, source=None, no_standard_prediction=False, no_sliced_prediction=False, image_size=None, slice_height=512, slice_width=512, overlap_height_ratio=0.2, overlap_width_ratio=0.2, postprocess_type='GREEDYNMM', postprocess_match_metric='IOS', postprocess_match_threshold=0.5, postprocess_class_agnostic=False, novisual=False, view_video=False, frame_skip_interval=0, export_pickle=False, export_crop=False, dataset_json_path=None, project='runs/predict', name='exp', visual_bbox_thickness=None, visual_text_size=None, visual_text_thickness=None, visual_hide_labels=False, visual_hide_conf=False, visual_export_format='png', verbose=1, return_dict=False, force_postprocess_type=False, exclude_classes_by_name=None, exclude_classes_by_id=None, progress_bar=False, **kwargs)</code>","text":"<p>Performs prediction for all present images in given folder.</p> <p>Parameters:</p> Name Type Description Default <code>detection_model</code> \u00b6 <code>DetectionModel | None</code> <p>sahi.model.DetectionModel Optionally provide custom DetectionModel to be used for inference. When provided, model_type, model_path, config_path, model_device, model_category_mapping, image_size params will be ignored</p> <code>None</code> <code>model_type</code> \u00b6 <code>str</code> <p>str mmdet for 'MmdetDetectionModel', 'yolov5' for 'Yolov5DetectionModel'.</p> <code>'ultralytics'</code> <code>model_path</code> \u00b6 <code>str | None</code> <p>str Path for the model weight</p> <code>None</code> <code>model_config_path</code> \u00b6 <code>str | None</code> <p>str Path for the detection model config file</p> <code>None</code> <code>model_confidence_threshold</code> \u00b6 <code>float</code> <p>float All predictions with score &lt; model_confidence_threshold will be discarded.</p> <code>0.25</code> <code>model_device</code> \u00b6 <code>str | None</code> <p>str Torch device, \"cpu\" or \"cuda\"</p> <code>None</code> <code>model_category_mapping</code> \u00b6 <code>dict | None</code> <p>dict Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}</p> <code>None</code> <code>model_category_remapping</code> \u00b6 <code>dict | None</code> <p>dict: str to int Remap category ids after performing inference</p> <code>None</code> <code>source</code> \u00b6 <code>str | None</code> <p>str Folder directory that contains images or path of the image to be predicted. Also video to be predicted.</p> <code>None</code> <code>no_standard_prediction</code> \u00b6 <code>bool</code> <p>bool Dont perform standard prediction. Default: False.</p> <code>False</code> <code>no_sliced_prediction</code> \u00b6 <code>bool</code> <p>bool Dont perform sliced prediction. Default: False.</p> <code>False</code> <code>image_size</code> \u00b6 <code>int | None</code> <p>int Input image size for each inference (image is scaled by preserving asp. rat.).</p> <code>None</code> <code>slice_height</code> \u00b6 <code>int</code> <p>int Height of each slice.  Defaults to <code>512</code>.</p> <code>512</code> <code>slice_width</code> \u00b6 <code>int</code> <p>int Width of each slice.  Defaults to <code>512</code>.</p> <code>512</code> <code>overlap_height_ratio</code> \u00b6 <code>float</code> <p>float Fractional overlap in height of each window (e.g. an overlap of 0.2 for a window of size 512 yields an overlap of 102 pixels). Default to <code>0.2</code>.</p> <code>0.2</code> <code>overlap_width_ratio</code> \u00b6 <code>float</code> <p>float Fractional overlap in width of each window (e.g. an overlap of 0.2 for a window of size 512 yields an overlap of 102 pixels). Default to <code>0.2</code>.</p> <code>0.2</code> <code>postprocess_type</code> \u00b6 <code>str</code> <p>str Type of the postprocess to be used after sliced inference while merging/eliminating predictions. Options are 'NMM', 'GREEDYNMM', 'LSNMS' or 'NMS'. Default is 'GREEDYNMM'.</p> <code>'GREEDYNMM'</code> <code>postprocess_match_metric</code> \u00b6 <code>str</code> <p>str Metric to be used during object prediction matching after sliced prediction. 'IOU' for intersection over union, 'IOS' for intersection over smaller area.</p> <code>'IOS'</code> <code>postprocess_match_threshold</code> \u00b6 <code>float</code> <p>float Sliced predictions having higher iou than postprocess_match_threshold will be postprocessed after sliced prediction.</p> <code>0.5</code> <code>postprocess_class_agnostic</code> \u00b6 <code>bool</code> <p>bool If True, postprocess will ignore category ids.</p> <code>False</code> <code>novisual</code> \u00b6 <code>bool</code> <p>bool Dont export predicted video/image visuals.</p> <code>False</code> <code>view_video</code> \u00b6 <code>bool</code> <p>bool View result of prediction during video inference.</p> <code>False</code> <code>frame_skip_interval</code> \u00b6 <code>int</code> <p>int If view_video or export_visual is slow, you can process one frames of 3(for exp: --frame_skip_interval=3).</p> <code>0</code> <code>export_pickle</code> \u00b6 <code>bool</code> <p>bool Export predictions as .pickle</p> <code>False</code> <code>export_crop</code> \u00b6 <code>bool</code> <p>bool Export predictions as cropped images.</p> <code>False</code> <code>dataset_json_path</code> \u00b6 <code>str | None</code> <p>str If coco file path is provided, detection results will be exported in coco json format.</p> <code>None</code> <code>project</code> \u00b6 <code>str</code> <p>str Save results to project/name.</p> <code>'runs/predict'</code> <code>name</code> \u00b6 <code>str</code> <p>str Save results to project/name.</p> <code>'exp'</code> <code>visual_bbox_thickness</code> \u00b6 <code>int | None</code> <p>int, optional Line thickness (in pixels) for bounding boxes in exported visualizations. If None, a default thickness is chosen based on image size.</p> <code>None</code> <code>visual_text_size</code> \u00b6 <code>float | None</code> <p>float, optional Font scale/size for label text in exported visualizations. If None, a sensible default is used.</p> <code>None</code> <code>visual_text_thickness</code> \u00b6 <code>int | None</code> <p>int, optional Thickness of text labels. If None, a sensible default is used.</p> <code>None</code> <code>visual_hide_labels</code> \u00b6 <code>bool</code> <p>bool, optional If True, class label names won't be shown on the exported visuals.</p> <code>False</code> <code>visual_hide_conf</code> \u00b6 <code>bool</code> <p>bool, optional If True, confidence scores won't be shown on the exported visuals.</p> <code>False</code> <code>visual_export_format</code> \u00b6 <code>str</code> <p>str, optional Output image format to use when exporting visuals. Supported values are 'png' (default) and 'jpg'. Note that 'jpg' uses lossy compression and may produce smaller files. This parameter is ignored when <code>novisual</code> is True. Exported visuals are written under the run directory: <code>project/name/visuals</code> (and <code>project/name/visuals_with_gt</code> when ground-truth overlays are created).</p> <code>'png'</code> <code>verbose</code> \u00b6 <code>int</code> <p>int 0: no print 1: print slice/prediction durations, number of slices 2: print model loading/file exporting durations</p> <code>1</code> <code>return_dict</code> \u00b6 <code>bool</code> <p>bool If True, returns a dict with 'export_dir' field.</p> <code>False</code> <code>force_postprocess_type</code> \u00b6 <code>bool</code> <p>bool If True, auto postprocess check will e disabled</p> <code>False</code> <code>exclude_classes_by_name</code> \u00b6 <code>list[str] | None</code> <p>Optional[List[str]] None: if no classes are excluded List[str]: set of classes to exclude using its/their class label name/s</p> <code>None</code> <code>exclude_classes_by_id</code> \u00b6 <code>list[int] | None</code> <p>Optional[List[int]] None: if no classes are excluded List[int]: set of classes to exclude using one or more IDs</p> <code>None</code> <code>progress_bar</code> \u00b6 <code>bool</code> <p>bool Whether to show a progress bar. Default is False.</p> <code>False</code> Source code in <code>sahi/predict.py</code> <pre><code>def predict(\n    detection_model: DetectionModel | None = None,\n    model_type: str = \"ultralytics\",\n    model_path: str | None = None,\n    model_config_path: str | None = None,\n    model_confidence_threshold: float = 0.25,\n    model_device: str | None = None,\n    model_category_mapping: dict | None = None,\n    model_category_remapping: dict | None = None,\n    source: str | None = None,\n    no_standard_prediction: bool = False,\n    no_sliced_prediction: bool = False,\n    image_size: int | None = None,\n    slice_height: int = 512,\n    slice_width: int = 512,\n    overlap_height_ratio: float = 0.2,\n    overlap_width_ratio: float = 0.2,\n    postprocess_type: str = \"GREEDYNMM\",\n    postprocess_match_metric: str = \"IOS\",\n    postprocess_match_threshold: float = 0.5,\n    postprocess_class_agnostic: bool = False,\n    novisual: bool = False,\n    view_video: bool = False,\n    frame_skip_interval: int = 0,\n    export_pickle: bool = False,\n    export_crop: bool = False,\n    dataset_json_path: str | None = None,\n    project: str = \"runs/predict\",\n    name: str = \"exp\",\n    visual_bbox_thickness: int | None = None,\n    visual_text_size: float | None = None,\n    visual_text_thickness: int | None = None,\n    visual_hide_labels: bool = False,\n    visual_hide_conf: bool = False,\n    visual_export_format: str = \"png\",\n    verbose: int = 1,\n    return_dict: bool = False,\n    force_postprocess_type: bool = False,\n    exclude_classes_by_name: list[str] | None = None,\n    exclude_classes_by_id: list[int] | None = None,\n    progress_bar: bool = False,\n    **kwargs,\n):\n    \"\"\"Performs prediction for all present images in given folder.\n\n    Args:\n        detection_model: sahi.model.DetectionModel\n            Optionally provide custom DetectionModel to be used for inference. When provided,\n            model_type, model_path, config_path, model_device, model_category_mapping, image_size\n            params will be ignored\n        model_type: str\n            mmdet for 'MmdetDetectionModel', 'yolov5' for 'Yolov5DetectionModel'.\n        model_path: str\n            Path for the model weight\n        model_config_path: str\n            Path for the detection model config file\n        model_confidence_threshold: float\n            All predictions with score &lt; model_confidence_threshold will be discarded.\n        model_device: str\n            Torch device, \"cpu\" or \"cuda\"\n        model_category_mapping: dict\n            Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n        model_category_remapping: dict: str to int\n            Remap category ids after performing inference\n        source: str\n            Folder directory that contains images or path of the image to be predicted. Also video to be predicted.\n        no_standard_prediction: bool\n            Dont perform standard prediction. Default: False.\n        no_sliced_prediction: bool\n            Dont perform sliced prediction. Default: False.\n        image_size: int\n            Input image size for each inference (image is scaled by preserving asp. rat.).\n        slice_height: int\n            Height of each slice.  Defaults to ``512``.\n        slice_width: int\n            Width of each slice.  Defaults to ``512``.\n        overlap_height_ratio: float\n            Fractional overlap in height of each window (e.g. an overlap of 0.2 for a window\n            of size 512 yields an overlap of 102 pixels).\n            Default to ``0.2``.\n        overlap_width_ratio: float\n            Fractional overlap in width of each window (e.g. an overlap of 0.2 for a window\n            of size 512 yields an overlap of 102 pixels).\n            Default to ``0.2``.\n        postprocess_type: str\n            Type of the postprocess to be used after sliced inference while merging/eliminating predictions.\n            Options are 'NMM', 'GREEDYNMM', 'LSNMS' or 'NMS'. Default is 'GREEDYNMM'.\n        postprocess_match_metric: str\n            Metric to be used during object prediction matching after sliced prediction.\n            'IOU' for intersection over union, 'IOS' for intersection over smaller area.\n        postprocess_match_threshold: float\n            Sliced predictions having higher iou than postprocess_match_threshold will be\n            postprocessed after sliced prediction.\n        postprocess_class_agnostic: bool\n            If True, postprocess will ignore category ids.\n        novisual: bool\n            Dont export predicted video/image visuals.\n        view_video: bool\n            View result of prediction during video inference.\n        frame_skip_interval: int\n            If view_video or export_visual is slow, you can process one frames of 3(for exp: --frame_skip_interval=3).\n        export_pickle: bool\n            Export predictions as .pickle\n        export_crop: bool\n            Export predictions as cropped images.\n        dataset_json_path: str\n            If coco file path is provided, detection results will be exported in coco json format.\n        project: str\n            Save results to project/name.\n        name: str\n            Save results to project/name.\n        visual_bbox_thickness: int, optional\n            Line thickness (in pixels) for bounding boxes in exported visualizations.\n            If None, a default thickness is chosen based on image size.\n        visual_text_size: float, optional\n            Font scale/size for label text in exported visualizations. If None, a\n            sensible default is used.\n        visual_text_thickness: int, optional\n            Thickness of text labels. If None, a sensible default is used.\n        visual_hide_labels: bool, optional\n            If True, class label names won't be shown on the exported visuals.\n        visual_hide_conf: bool, optional\n            If True, confidence scores won't be shown on the exported visuals.\n        visual_export_format: str, optional\n            Output image format to use when exporting visuals. Supported values are\n            'png' (default) and 'jpg'. Note that 'jpg' uses lossy compression and may\n            produce smaller files. This parameter is ignored when `novisual` is True.\n            Exported visuals are written under the run directory: `project/name/visuals`\n            (and `project/name/visuals_with_gt` when ground-truth overlays are created).\n        verbose: int\n            0: no print\n            1: print slice/prediction durations, number of slices\n            2: print model loading/file exporting durations\n        return_dict: bool\n            If True, returns a dict with 'export_dir' field.\n        force_postprocess_type: bool\n            If True, auto postprocess check will e disabled\n        exclude_classes_by_name: Optional[List[str]]\n            None: if no classes are excluded\n            List[str]: set of classes to exclude using its/their class label name/s\n        exclude_classes_by_id: Optional[List[int]]\n            None: if no classes are excluded\n            List[int]: set of classes to exclude using one or more IDs\n        progress_bar: bool\n            Whether to show a progress bar. Default is False.\n    \"\"\"\n    # assert prediction type\n    if no_standard_prediction and no_sliced_prediction:\n        raise ValueError(\"'no_standard_prediction' and 'no_sliced_prediction' cannot be True at the same time.\")\n\n    # auto postprocess type\n    if not force_postprocess_type and model_confidence_threshold &lt; LOW_MODEL_CONFIDENCE and postprocess_type != \"NMS\":\n        logger.warning(\n            f\"Switching postprocess type/metric to NMS/IOU since confidence \"\n            f\"threshold is low ({model_confidence_threshold}).\"\n        )\n        postprocess_type = \"NMS\"\n        postprocess_match_metric = \"IOU\"\n\n    # for profiling\n    durations_in_seconds = dict()\n\n    # Init export directories\n    save_dir = Path(increment_path(Path(project) / name, exist_ok=False))  # increment run\n    crop_dir = save_dir / \"crops\"\n    visual_dir = save_dir / \"visuals\"\n    visual_with_gt_dir = save_dir / \"visuals_with_gt\"\n    pickle_dir = save_dir / \"pickles\"\n    if not novisual or export_pickle or export_crop or dataset_json_path is not None:\n        save_dir.mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Init image iterator\n    # TODO: rewrite this as iterator class as in https://github.com/ultralytics/yolov5/blob/d059d1da03aee9a3c0059895aa4c7c14b7f25a9e/utils/datasets.py#L178\n    source_is_video = False\n    num_frames = None\n    image_iterator: list[str] | Generator[Image.Image]\n    if dataset_json_path and source:\n        coco: Coco = Coco.from_coco_dict_or_path(dataset_json_path)\n        image_iterator = [str(Path(source) / Path(coco_image.file_name)) for coco_image in coco.images]\n        coco_json = []\n    elif source and os.path.isdir(source):\n        image_iterator = list_files(directory=source, contains=IMAGE_EXTENSIONS, verbose=verbose)\n    elif source and Path(source).suffix in VIDEO_EXTENSIONS:\n        source_is_video = True\n        read_video_frame, output_video_writer, video_file_name, num_frames = get_video_reader(\n            source, str(save_dir), frame_skip_interval, not novisual, view_video\n        )\n        image_iterator = read_video_frame\n    elif source:\n        image_iterator = [source]\n    else:\n        logger.error(\"No valid input given to predict function\")\n        return\n\n    # init model instance\n    time_start = time.time()\n    if detection_model is None:\n        detection_model = AutoDetectionModel.from_pretrained(\n            model_type=model_type,\n            model_path=model_path,\n            config_path=model_config_path,\n            confidence_threshold=model_confidence_threshold,\n            device=model_device,\n            category_mapping=model_category_mapping,\n            category_remapping=model_category_remapping,\n            load_at_init=False,\n            image_size=image_size,\n            **kwargs,\n        )\n        detection_model.load_model()\n    time_end = time.time() - time_start\n    durations_in_seconds[\"model_load\"] = time_end\n\n    # iterate over source images\n    durations_in_seconds[\"prediction\"] = 0\n    durations_in_seconds[\"slice\"] = 0\n\n    input_type_str = \"video frames\" if source_is_video else \"images\"\n    for ind, image_path in enumerate(\n        tqdm(image_iterator, f\"Performing inference on {input_type_str}\", total=num_frames)\n    ):\n        # Source is an image: Iterating over Image objects\n        if source and source_is_video:\n            video_name = Path(source).stem\n            relative_filepath = video_name + \"_frame_\" + str(ind)\n        elif isinstance(image_path, Image.Image):\n            raise RuntimeError(\"Source is not a video, but image is still an Image object \")\n        # preserve source folder structure in export\n        elif source and os.path.isdir(source):\n            relative_filepath = str(Path(image_path)).split(str(Path(source)))[-1]\n            relative_filepath = relative_filepath[1:] if relative_filepath[0] == os.sep else relative_filepath\n        else:  # no process if source is single file\n            relative_filepath = Path(image_path).name\n\n        filename_without_extension = Path(relative_filepath).stem\n\n        # load image\n        image_as_pil = read_image_as_pil(image_path)\n\n        # perform prediction\n        if not no_sliced_prediction:\n            # get sliced prediction\n            prediction_result = get_sliced_prediction(\n                image=image_as_pil,\n                detection_model=detection_model,\n                slice_height=slice_height,\n                slice_width=slice_width,\n                overlap_height_ratio=overlap_height_ratio,\n                overlap_width_ratio=overlap_width_ratio,\n                perform_standard_pred=not no_standard_prediction,\n                postprocess_type=postprocess_type,\n                postprocess_match_metric=postprocess_match_metric,\n                postprocess_match_threshold=postprocess_match_threshold,\n                postprocess_class_agnostic=postprocess_class_agnostic,\n                verbose=1 if verbose else 0,\n                exclude_classes_by_name=exclude_classes_by_name,\n                exclude_classes_by_id=exclude_classes_by_id,\n                progress_bar=progress_bar,\n            )\n            object_prediction_list = prediction_result.object_prediction_list\n            if prediction_result.durations_in_seconds:\n                durations_in_seconds[\"slice\"] += prediction_result.durations_in_seconds[\"slice\"]\n        else:\n            # get standard prediction\n            prediction_result = get_prediction(\n                image=image_as_pil,\n                detection_model=detection_model,\n                shift_amount=[0, 0],\n                full_shape=None,\n                postprocess=None,\n                verbose=0,\n                exclude_classes_by_name=exclude_classes_by_name,\n                exclude_classes_by_id=exclude_classes_by_id,\n            )\n            object_prediction_list = prediction_result.object_prediction_list\n\n        durations_in_seconds[\"prediction\"] += prediction_result.durations_in_seconds[\"prediction\"]\n        # Show prediction time\n        if verbose:\n            tqdm.write(\n                \"Prediction time is: {:.2f} ms\".format(prediction_result.durations_in_seconds[\"prediction\"] * 1000)\n            )\n\n        if dataset_json_path:\n            if source_is_video is True:\n                raise NotImplementedError(\"Video input type not supported with coco formatted dataset json\")\n\n            # append predictions in coco format\n            for object_prediction in object_prediction_list:\n                coco_prediction = object_prediction.to_coco_prediction()\n                coco_prediction.image_id = coco.images[ind].id\n                coco_prediction_json = coco_prediction.json\n                if coco_prediction_json[\"bbox\"]:\n                    coco_json.append(coco_prediction_json)\n            if not novisual:\n                # convert ground truth annotations to object_prediction_list\n                coco_image: CocoImage = coco.images[ind]\n                object_prediction_gt_list: list[ObjectPrediction] = []\n                for coco_annotation in coco_image.annotations:\n                    coco_annotation_dict = coco_annotation.json\n                    category_name = coco_annotation.category_name\n                    full_shape = [coco_image.height, coco_image.width]\n                    object_prediction_gt = ObjectPrediction.from_coco_annotation_dict(\n                        annotation_dict=coco_annotation_dict, category_name=category_name, full_shape=full_shape\n                    )\n                    object_prediction_gt_list.append(object_prediction_gt)\n                # export visualizations with ground truths\n                output_dir = str(visual_with_gt_dir / Path(relative_filepath).parent)\n                color = (0, 255, 0)  # original annotations in green\n                result = visualize_object_predictions(\n                    np.ascontiguousarray(image_as_pil),\n                    object_prediction_list=object_prediction_gt_list,\n                    rect_th=visual_bbox_thickness,\n                    text_size=visual_text_size,\n                    text_th=visual_text_thickness,\n                    color=color,\n                    hide_labels=visual_hide_labels,\n                    hide_conf=visual_hide_conf,\n                    output_dir=None,\n                    file_name=None,\n                    export_format=None,\n                )\n                color = (255, 0, 0)  # model predictions in red\n                _ = visualize_object_predictions(\n                    result[\"image\"],\n                    object_prediction_list=object_prediction_list,\n                    rect_th=visual_bbox_thickness,\n                    text_size=visual_text_size,\n                    text_th=visual_text_thickness,\n                    color=color,\n                    hide_labels=visual_hide_labels,\n                    hide_conf=visual_hide_conf,\n                    output_dir=output_dir,\n                    file_name=filename_without_extension,\n                    export_format=visual_export_format,\n                )\n\n        time_start = time.time()\n        # export prediction boxes\n        if export_crop:\n            output_dir = str(crop_dir / Path(relative_filepath).parent)\n            crop_object_predictions(\n                image=np.ascontiguousarray(image_as_pil),\n                object_prediction_list=object_prediction_list,\n                output_dir=output_dir,\n                file_name=filename_without_extension,\n                export_format=visual_export_format,\n            )\n        # export prediction list as pickle\n        if export_pickle:\n            save_path = str(pickle_dir / Path(relative_filepath).parent / (filename_without_extension + \".pickle\"))\n            save_pickle(data=object_prediction_list, save_path=save_path)\n\n        # export visualization\n        if not novisual or view_video:\n            output_dir = str(visual_dir / Path(relative_filepath).parent)\n            result = visualize_object_predictions(\n                np.ascontiguousarray(image_as_pil),\n                object_prediction_list=object_prediction_list,\n                rect_th=visual_bbox_thickness,\n                text_size=visual_text_size,\n                text_th=visual_text_thickness,\n                hide_labels=visual_hide_labels,\n                hide_conf=visual_hide_conf,\n                output_dir=output_dir if not source_is_video else None,\n                file_name=filename_without_extension,\n                export_format=visual_export_format,\n            )\n            if not novisual and source_is_video:  # export video\n                if output_video_writer is None:\n                    raise RuntimeError(\"Output video writer could not be created\")\n                output_video_writer.write(cv2.cvtColor(result[\"image\"], cv2.COLOR_RGB2BGR))\n\n        # render video inference\n        if view_video:\n            cv2.imshow(f\"Prediction of {video_file_name!s}\", result[\"image\"])\n            cv2.waitKey(1)\n\n        time_end = time.time() - time_start\n        durations_in_seconds[\"export_files\"] = time_end\n\n    # export coco results\n    if dataset_json_path:\n        save_path = str(save_dir / \"result.json\")\n        save_json(coco_json, save_path)\n\n    if not novisual or export_pickle or export_crop or dataset_json_path is not None:\n        print(f\"Prediction results are successfully exported to {save_dir}\")\n\n    # print prediction duration\n    if verbose == 2:\n        print(\n            \"Model loaded in\",\n            durations_in_seconds[\"model_load\"],\n            \"seconds.\",\n        )\n        print(\n            \"Slicing performed in\",\n            durations_in_seconds[\"slice\"],\n            \"seconds.\",\n        )\n        print(\n            \"Prediction performed in\",\n            durations_in_seconds[\"prediction\"],\n            \"seconds.\",\n        )\n        if not novisual:\n            print(\n                \"Exporting performed in\",\n                durations_in_seconds[\"export_files\"],\n                \"seconds.\",\n            )\n\n    if return_dict:\n        return {\"export_dir\": save_dir}\n</code></pre>"},{"location":"api/#sahi.predict.predict_fiftyone","title":"<code>predict_fiftyone(model_type='mmdet', model_path=None, model_config_path=None, model_confidence_threshold=0.25, model_device=None, model_category_mapping=None, model_category_remapping=None, dataset_json_path='', image_dir='', no_standard_prediction=False, no_sliced_prediction=False, image_size=None, slice_height=256, slice_width=256, overlap_height_ratio=0.2, overlap_width_ratio=0.2, postprocess_type='GREEDYNMM', postprocess_match_metric='IOS', postprocess_match_threshold=0.5, postprocess_class_agnostic=False, verbose=1, exclude_classes_by_name=None, exclude_classes_by_id=None, progress_bar=False)</code>","text":"<p>Performs prediction for all present images in given folder.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> \u00b6 <code>str</code> <p>str mmdet for 'MmdetDetectionModel', 'yolov5' for 'Yolov5DetectionModel'.</p> <code>'mmdet'</code> <code>model_path</code> \u00b6 <code>str | None</code> <p>str Path for the model weight</p> <code>None</code> <code>model_config_path</code> \u00b6 <code>str | None</code> <p>str Path for the detection model config file</p> <code>None</code> <code>model_confidence_threshold</code> \u00b6 <code>float</code> <p>float All predictions with score &lt; model_confidence_threshold will be discarded.</p> <code>0.25</code> <code>model_device</code> \u00b6 <code>str | None</code> <p>str Torch device, \"cpu\" or \"cuda\"</p> <code>None</code> <code>model_category_mapping</code> \u00b6 <code>dict | None</code> <p>dict Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}</p> <code>None</code> <code>model_category_remapping</code> \u00b6 <code>dict | None</code> <p>dict: str to int Remap category ids after performing inference</p> <code>None</code> <code>dataset_json_path</code> \u00b6 <code>str</code> <p>str If coco file path is provided, detection results will be exported in coco json format.</p> <code>''</code> <code>image_dir</code> \u00b6 <code>str</code> <p>str Folder directory that contains images or path of the image to be predicted.</p> <code>''</code> <code>no_standard_prediction</code> \u00b6 <code>bool</code> <p>bool Dont perform standard prediction. Default: False.</p> <code>False</code> <code>no_sliced_prediction</code> \u00b6 <code>bool</code> <p>bool Dont perform sliced prediction. Default: False.</p> <code>False</code> <code>image_size</code> \u00b6 <code>int | None</code> <p>int Input image size for each inference (image is scaled by preserving asp. rat.).</p> <code>None</code> <code>slice_height</code> \u00b6 <code>int</code> <p>int Height of each slice.  Defaults to <code>256</code>.</p> <code>256</code> <code>slice_width</code> \u00b6 <code>int</code> <p>int Width of each slice.  Defaults to <code>256</code>.</p> <code>256</code> <code>overlap_height_ratio</code> \u00b6 <code>float</code> <p>float Fractional overlap in height of each window (e.g. an overlap of 0.2 for a window of size 256 yields an overlap of 51 pixels). Default to <code>0.2</code>.</p> <code>0.2</code> <code>overlap_width_ratio</code> \u00b6 <code>float</code> <p>float Fractional overlap in width of each window (e.g. an overlap of 0.2 for a window of size 256 yields an overlap of 51 pixels). Default to <code>0.2</code>.</p> <code>0.2</code> <code>postprocess_type</code> \u00b6 <code>str</code> <p>str Type of the postprocess to be used after sliced inference while merging/eliminating predictions. Options are 'NMM', 'GREEDYNMM' or 'NMS'. Default is 'GREEDYNMM'.</p> <code>'GREEDYNMM'</code> <code>postprocess_match_metric</code> \u00b6 <code>str</code> <p>str Metric to be used during object prediction matching after sliced prediction. 'IOU' for intersection over union, 'IOS' for intersection over smaller area.</p> <code>'IOS'</code> <code>postprocess_match_metric</code> \u00b6 <code>str</code> <p>str Metric to be used during object prediction matching after sliced prediction. 'IOU' for intersection over union, 'IOS' for intersection over smaller area.</p> <code>'IOS'</code> <code>postprocess_match_threshold</code> \u00b6 <code>float</code> <p>float Sliced predictions having higher iou than postprocess_match_threshold will be postprocessed after sliced prediction.</p> <code>0.5</code> <code>postprocess_class_agnostic</code> \u00b6 <code>bool</code> <p>bool If True, postprocess will ignore category ids.</p> <code>False</code> <code>verbose</code> \u00b6 <code>int</code> <p>int 0: no print 1: print slice/prediction durations, number of slices, model loading/file exporting durations</p> <code>1</code> <code>exclude_classes_by_name</code> \u00b6 <code>list[str] | None</code> <p>Optional[List[str]] None: if no classes are excluded List[str]: set of classes to exclude using its/their class label name/s</p> <code>None</code> <code>exclude_classes_by_id</code> \u00b6 <code>list[int] | None</code> <p>Optional[List[int]] None: if no classes are excluded List[int]: set of classes to exclude using one or more IDs</p> <code>None</code> <code>progress_bar</code> \u00b6 <code>bool</code> <p>bool Whether to show progress bar for slice processing. Default: False.</p> <code>False</code> Source code in <code>sahi/predict.py</code> <pre><code>def predict_fiftyone(\n    model_type: str = \"mmdet\",\n    model_path: str | None = None,\n    model_config_path: str | None = None,\n    model_confidence_threshold: float = 0.25,\n    model_device: str | None = None,\n    model_category_mapping: dict | None = None,\n    model_category_remapping: dict | None = None,\n    dataset_json_path: str = \"\",\n    image_dir: str = \"\",\n    no_standard_prediction: bool = False,\n    no_sliced_prediction: bool = False,\n    image_size: int | None = None,\n    slice_height: int = 256,\n    slice_width: int = 256,\n    overlap_height_ratio: float = 0.2,\n    overlap_width_ratio: float = 0.2,\n    postprocess_type: str = \"GREEDYNMM\",\n    postprocess_match_metric: str = \"IOS\",\n    postprocess_match_threshold: float = 0.5,\n    postprocess_class_agnostic: bool = False,\n    verbose: int = 1,\n    exclude_classes_by_name: list[str] | None = None,\n    exclude_classes_by_id: list[int] | None = None,\n    progress_bar: bool = False,\n):\n    \"\"\"Performs prediction for all present images in given folder.\n\n    Args:\n        model_type: str\n            mmdet for 'MmdetDetectionModel', 'yolov5' for 'Yolov5DetectionModel'.\n        model_path: str\n            Path for the model weight\n        model_config_path: str\n            Path for the detection model config file\n        model_confidence_threshold: float\n            All predictions with score &lt; model_confidence_threshold will be discarded.\n        model_device: str\n            Torch device, \"cpu\" or \"cuda\"\n        model_category_mapping: dict\n            Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n        model_category_remapping: dict: str to int\n            Remap category ids after performing inference\n        dataset_json_path: str\n            If coco file path is provided, detection results will be exported in coco json format.\n        image_dir: str\n            Folder directory that contains images or path of the image to be predicted.\n        no_standard_prediction: bool\n            Dont perform standard prediction. Default: False.\n        no_sliced_prediction: bool\n            Dont perform sliced prediction. Default: False.\n        image_size: int\n            Input image size for each inference (image is scaled by preserving asp. rat.).\n        slice_height: int\n            Height of each slice.  Defaults to ``256``.\n        slice_width: int\n            Width of each slice.  Defaults to ``256``.\n        overlap_height_ratio: float\n            Fractional overlap in height of each window (e.g. an overlap of 0.2 for a window\n            of size 256 yields an overlap of 51 pixels).\n            Default to ``0.2``.\n        overlap_width_ratio: float\n            Fractional overlap in width of each window (e.g. an overlap of 0.2 for a window\n            of size 256 yields an overlap of 51 pixels).\n            Default to ``0.2``.\n        postprocess_type: str\n            Type of the postprocess to be used after sliced inference while merging/eliminating predictions.\n            Options are 'NMM', 'GREEDYNMM' or 'NMS'. Default is 'GREEDYNMM'.\n        postprocess_match_metric: str\n            Metric to be used during object prediction matching after sliced prediction.\n            'IOU' for intersection over union, 'IOS' for intersection over smaller area.\n        postprocess_match_metric: str\n            Metric to be used during object prediction matching after sliced prediction.\n            'IOU' for intersection over union, 'IOS' for intersection over smaller area.\n        postprocess_match_threshold: float\n            Sliced predictions having higher iou than postprocess_match_threshold will be\n            postprocessed after sliced prediction.\n        postprocess_class_agnostic: bool\n            If True, postprocess will ignore category ids.\n        verbose: int\n            0: no print\n            1: print slice/prediction durations, number of slices, model loading/file exporting durations\n        exclude_classes_by_name: Optional[List[str]]\n            None: if no classes are excluded\n            List[str]: set of classes to exclude using its/their class label name/s\n        exclude_classes_by_id: Optional[List[int]]\n            None: if no classes are excluded\n            List[int]: set of classes to exclude using one or more IDs\n        progress_bar: bool\n            Whether to show progress bar for slice processing. Default: False.\n    \"\"\"\n    check_requirements([\"fiftyone\"])\n\n    from sahi.utils.fiftyone import create_fiftyone_dataset_from_coco_file, fo\n\n    # assert prediction type\n    if no_standard_prediction and no_sliced_prediction:\n        raise ValueError(\"'no_standard_pred' and 'no_sliced_prediction' cannot be True at the same time.\")\n    # for profiling\n    durations_in_seconds = dict()\n\n    dataset = create_fiftyone_dataset_from_coco_file(image_dir, dataset_json_path)\n\n    # init model instance\n    time_start = time.time()\n    detection_model = AutoDetectionModel.from_pretrained(\n        model_type=model_type,\n        model_path=model_path,\n        config_path=model_config_path,\n        confidence_threshold=model_confidence_threshold,\n        device=model_device,\n        category_mapping=model_category_mapping,\n        category_remapping=model_category_remapping,\n        load_at_init=False,\n        image_size=image_size,\n    )\n    detection_model.load_model()\n    time_end = time.time() - time_start\n    durations_in_seconds[\"model_load\"] = time_end\n\n    # iterate over source images\n    durations_in_seconds[\"prediction\"] = 0\n    durations_in_seconds[\"slice\"] = 0\n    # Add predictions to samples\n    with fo.ProgressBar() as pb:\n        for sample in pb(dataset):\n            # perform prediction\n            if not no_sliced_prediction:\n                # get sliced prediction\n                prediction_result = get_sliced_prediction(\n                    image=sample.filepath,\n                    detection_model=detection_model,\n                    slice_height=slice_height,\n                    slice_width=slice_width,\n                    overlap_height_ratio=overlap_height_ratio,\n                    overlap_width_ratio=overlap_width_ratio,\n                    perform_standard_pred=not no_standard_prediction,\n                    postprocess_type=postprocess_type,\n                    postprocess_match_threshold=postprocess_match_threshold,\n                    postprocess_match_metric=postprocess_match_metric,\n                    postprocess_class_agnostic=postprocess_class_agnostic,\n                    verbose=verbose,\n                    exclude_classes_by_name=exclude_classes_by_name,\n                    exclude_classes_by_id=exclude_classes_by_id,\n                    progress_bar=progress_bar,\n                )\n                durations_in_seconds[\"slice\"] += prediction_result.durations_in_seconds[\"slice\"]\n            else:\n                # get standard prediction\n                prediction_result = get_prediction(\n                    image=sample.filepath,\n                    detection_model=detection_model,\n                    shift_amount=[0, 0],\n                    full_shape=None,\n                    postprocess=None,\n                    verbose=0,\n                    exclude_classes_by_name=exclude_classes_by_name,\n                    exclude_classes_by_id=exclude_classes_by_id,\n                )\n                durations_in_seconds[\"prediction\"] += prediction_result.durations_in_seconds[\"prediction\"]\n\n            # Save predictions to dataset\n            sample[model_type] = fo.Detections(detections=prediction_result.to_fiftyone_detections())\n            sample.save()\n\n    # print prediction duration\n    if verbose == 1:\n        print(\n            \"Model loaded in\",\n            durations_in_seconds[\"model_load\"],\n            \"seconds.\",\n        )\n        print(\n            \"Slicing performed in\",\n            durations_in_seconds[\"slice\"],\n            \"seconds.\",\n        )\n        print(\n            \"Prediction performed in\",\n            durations_in_seconds[\"prediction\"],\n            \"seconds.\",\n        )\n\n    # visualize results\n    session = fo.launch_app()  # pyright: ignore[reportArgumentType]\n    session.dataset = dataset\n    # Evaluate the predictions\n    results = dataset.evaluate_detections(\n        model_type,\n        gt_field=\"ground_truth\",\n        eval_key=\"eval\",\n        iou=postprocess_match_threshold,\n        compute_mAP=True,\n    )\n    # Get the 10 most common classes in the dataset\n    counts = dataset.count_values(\"ground_truth.detections.label\")\n    classes_top10 = sorted(counts, key=counts.get, reverse=True)[:10]\n    # Print a classification report for the top-10 classes\n    results.print_report(classes=classes_top10)\n    # Load the view on which we ran the `eval` evaluation\n    eval_view = dataset.load_evaluation_view(\"eval\")\n    # Show samples with most false positives\n    session.view = eval_view.sort_by(\"eval_fp\", reverse=True)\n    while 1:\n        time.sleep(3)\n</code></pre>"},{"location":"api/#sahi.prediction","title":"<code>prediction</code>","text":""},{"location":"api/#sahi.prediction-classes","title":"Classes","text":""},{"location":"api/#sahi.prediction.ObjectPrediction","title":"<code>ObjectPrediction</code>","text":"<p>               Bases: <code>ObjectAnnotation</code></p> <p>Class for handling detection model predictions.</p> Source code in <code>sahi/prediction.py</code> <pre><code>class ObjectPrediction(ObjectAnnotation):\n    \"\"\"Class for handling detection model predictions.\"\"\"\n\n    def __init__(\n        self,\n        bbox: list[int] | None = None,\n        category_id: int | None = None,\n        category_name: str | None = None,\n        segmentation: list[list[float]] | None = None,\n        score: float = 0.0,\n        shift_amount: list[int] | None = [0, 0],\n        full_shape: list[int] | None = None,\n    ):\n        \"\"\"Creates ObjectPrediction from bbox, score, category_id, category_name, segmentation.\n\n        Args:\n            bbox: list\n                [minx, miny, maxx, maxy]\n            score: float\n                Prediction score between 0 and 1\n            category_id: int\n                ID of the object category\n            category_name: str\n                Name of the object category\n            segmentation: List[List]\n                [\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    ...\n                ]\n            shift_amount: list\n                To shift the box and mask predictions from sliced image\n                to full sized image, should be in the form of [shift_x, shift_y]\n            full_shape: list\n                Size of the full image after shifting, should be in\n                the form of [height, width]\n        \"\"\"\n        self.score = PredictionScore(score)\n        super().__init__(\n            bbox=bbox,\n            category_id=category_id,\n            segmentation=segmentation,\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    def get_shifted_object_prediction(self):\n        \"\"\"Returns shifted version ObjectPrediction.\n\n        Shifts bbox and mask coords. Used for mapping sliced predictions over full image.\n        \"\"\"\n        if self.mask:\n            shifted_mask = self.mask.get_shifted_mask()\n            return ObjectPrediction(\n                bbox=self.bbox.get_shifted_box().to_xyxy(),\n                category_id=self.category.id,\n                score=self.score.value,\n                segmentation=shifted_mask.segmentation,\n                category_name=self.category.name,\n                shift_amount=[0, 0],\n                full_shape=shifted_mask.full_shape,\n            )\n        else:\n            return ObjectPrediction(\n                bbox=self.bbox.get_shifted_box().to_xyxy(),\n                category_id=self.category.id,\n                score=self.score.value,\n                segmentation=None,\n                category_name=self.category.name,\n                shift_amount=[0, 0],\n                full_shape=None,\n            )\n\n    def to_coco_prediction(self, image_id=None):\n        \"\"\"Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.\"\"\"\n        if self.mask:\n            coco_prediction = CocoPrediction.from_coco_segmentation(\n                segmentation=self.mask.segmentation,\n                category_id=self.category.id,\n                category_name=self.category.name,\n                score=self.score.value,\n                image_id=image_id,\n            )\n        else:\n            coco_prediction = CocoPrediction.from_coco_bbox(\n                bbox=self.bbox.to_xywh(),\n                category_id=self.category.id,\n                category_name=self.category.name,\n                score=self.score.value,\n                image_id=image_id,\n            )\n        return coco_prediction\n\n    def to_fiftyone_detection(self, image_height: int, image_width: int):\n        \"\"\"Returns fiftyone.Detection representation of ObjectPrediction.\"\"\"\n        try:\n            import fiftyone as fo\n        except ImportError:\n            raise ImportError('Please run \"pip install -U fiftyone\" to install fiftyone first for fiftyone conversion.')\n\n        x1, y1, x2, y2 = self.bbox.to_xyxy()\n        rel_box = [x1 / image_width, y1 / image_height, (x2 - x1) / image_width, (y2 - y1) / image_height]\n        fiftyone_detection = fo.Detection(label=self.category.name, bounding_box=rel_box, confidence=self.score.value)\n        return fiftyone_detection\n\n    def __repr__(self):\n        return f\"\"\"ObjectPrediction&lt;\n    bbox: {self.bbox},\n    mask: {self.mask},\n    score: {self.score},\n    category: {self.category}&gt;\"\"\"\n</code></pre> Functions\u00b6 <code></code> <code>__init__(bbox=None, category_id=None, category_name=None, segmentation=None, score=0.0, shift_amount=[0, 0], full_shape=None)</code> \u00b6 <p>Creates ObjectPrediction from bbox, score, category_id, category_name, segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> \u00b6 <code>list[int] | None</code> <p>list [minx, miny, maxx, maxy]</p> <code>None</code> <code>score</code> \u00b6 <code>float</code> <p>float Prediction score between 0 and 1</p> <code>0.0</code> <code>category_id</code> \u00b6 <code>int | None</code> <p>int ID of the object category</p> <code>None</code> <code>category_name</code> \u00b6 <code>str | None</code> <p>str Name of the object category</p> <code>None</code> <code>segmentation</code> \u00b6 <code>list[list[float]] | None</code> <p>List[List] [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> <code>None</code> <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>list To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> <code>full_shape</code> \u00b6 <code>list[int] | None</code> <p>list Size of the full image after shifting, should be in the form of [height, width]</p> <code>None</code> Source code in <code>sahi/prediction.py</code> <pre><code>def __init__(\n    self,\n    bbox: list[int] | None = None,\n    category_id: int | None = None,\n    category_name: str | None = None,\n    segmentation: list[list[float]] | None = None,\n    score: float = 0.0,\n    shift_amount: list[int] | None = [0, 0],\n    full_shape: list[int] | None = None,\n):\n    \"\"\"Creates ObjectPrediction from bbox, score, category_id, category_name, segmentation.\n\n    Args:\n        bbox: list\n            [minx, miny, maxx, maxy]\n        score: float\n            Prediction score between 0 and 1\n        category_id: int\n            ID of the object category\n        category_name: str\n            Name of the object category\n        segmentation: List[List]\n            [\n                [x1, y1, x2, y2, x3, y3, ...],\n                [x1, y1, x2, y2, x3, y3, ...],\n                ...\n            ]\n        shift_amount: list\n            To shift the box and mask predictions from sliced image\n            to full sized image, should be in the form of [shift_x, shift_y]\n        full_shape: list\n            Size of the full image after shifting, should be in\n            the form of [height, width]\n    \"\"\"\n    self.score = PredictionScore(score)\n    super().__init__(\n        bbox=bbox,\n        category_id=category_id,\n        segmentation=segmentation,\n        category_name=category_name,\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre> <code></code> <code>get_shifted_object_prediction()</code> \u00b6 <p>Returns shifted version ObjectPrediction.</p> <p>Shifts bbox and mask coords. Used for mapping sliced predictions over full image.</p> Source code in <code>sahi/prediction.py</code> <pre><code>def get_shifted_object_prediction(self):\n    \"\"\"Returns shifted version ObjectPrediction.\n\n    Shifts bbox and mask coords. Used for mapping sliced predictions over full image.\n    \"\"\"\n    if self.mask:\n        shifted_mask = self.mask.get_shifted_mask()\n        return ObjectPrediction(\n            bbox=self.bbox.get_shifted_box().to_xyxy(),\n            category_id=self.category.id,\n            score=self.score.value,\n            segmentation=shifted_mask.segmentation,\n            category_name=self.category.name,\n            shift_amount=[0, 0],\n            full_shape=shifted_mask.full_shape,\n        )\n    else:\n        return ObjectPrediction(\n            bbox=self.bbox.get_shifted_box().to_xyxy(),\n            category_id=self.category.id,\n            score=self.score.value,\n            segmentation=None,\n            category_name=self.category.name,\n            shift_amount=[0, 0],\n            full_shape=None,\n        )\n</code></pre> <code></code> <code>to_coco_prediction(image_id=None)</code> \u00b6 <p>Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.</p> Source code in <code>sahi/prediction.py</code> <pre><code>def to_coco_prediction(self, image_id=None):\n    \"\"\"Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.\"\"\"\n    if self.mask:\n        coco_prediction = CocoPrediction.from_coco_segmentation(\n            segmentation=self.mask.segmentation,\n            category_id=self.category.id,\n            category_name=self.category.name,\n            score=self.score.value,\n            image_id=image_id,\n        )\n    else:\n        coco_prediction = CocoPrediction.from_coco_bbox(\n            bbox=self.bbox.to_xywh(),\n            category_id=self.category.id,\n            category_name=self.category.name,\n            score=self.score.value,\n            image_id=image_id,\n        )\n    return coco_prediction\n</code></pre> <code></code> <code>to_fiftyone_detection(image_height, image_width)</code> \u00b6 <p>Returns fiftyone.Detection representation of ObjectPrediction.</p> Source code in <code>sahi/prediction.py</code> <pre><code>def to_fiftyone_detection(self, image_height: int, image_width: int):\n    \"\"\"Returns fiftyone.Detection representation of ObjectPrediction.\"\"\"\n    try:\n        import fiftyone as fo\n    except ImportError:\n        raise ImportError('Please run \"pip install -U fiftyone\" to install fiftyone first for fiftyone conversion.')\n\n    x1, y1, x2, y2 = self.bbox.to_xyxy()\n    rel_box = [x1 / image_width, y1 / image_height, (x2 - x1) / image_width, (y2 - y1) / image_height]\n    fiftyone_detection = fo.Detection(label=self.category.name, bounding_box=rel_box, confidence=self.score.value)\n    return fiftyone_detection\n</code></pre>"},{"location":"api/#sahi.prediction.PredictionResult","title":"<code>PredictionResult</code>","text":"Source code in <code>sahi/prediction.py</code> <pre><code>class PredictionResult:\n    def __init__(\n        self,\n        object_prediction_list: list[ObjectPrediction],\n        image: Image.Image | str | np.ndarray,\n        durations_in_seconds: dict[str, Any] = dict(),\n    ):\n        self.image: Image.Image = read_image_as_pil(image)\n        self.image_width, self.image_height = self.image.size\n        self.object_prediction_list: list[ObjectPrediction] = object_prediction_list\n        self.durations_in_seconds = durations_in_seconds\n\n    def export_visuals(\n        self,\n        export_dir: str,\n        text_size: float | None = None,\n        rect_th: int | None = None,\n        hide_labels: bool = False,\n        hide_conf: bool = False,\n        file_name: str = \"prediction_visual\",\n    ):\n        \"\"\"\n\n        Args:\n            export_dir: directory for resulting visualization to be exported\n            text_size: size of the category name over box\n            rect_th: rectangle thickness\n            hide_labels: hide labels\n            hide_conf: hide confidence\n            file_name: saving name\n        Returns:\n\n        \"\"\"\n        Path(export_dir).mkdir(parents=True, exist_ok=True)\n        visualize_object_predictions(\n            image=np.ascontiguousarray(self.image),\n            object_prediction_list=self.object_prediction_list,\n            rect_th=rect_th,\n            text_size=text_size,\n            text_th=None,\n            color=None,\n            hide_labels=hide_labels,\n            hide_conf=hide_conf,\n            output_dir=export_dir,\n            file_name=file_name,\n            export_format=\"png\",\n        )\n\n    def to_coco_annotations(self):\n        coco_annotation_list = []\n        for object_prediction in self.object_prediction_list:\n            coco_annotation_list.append(object_prediction.to_coco_prediction().json)\n        return coco_annotation_list\n\n    def to_coco_predictions(self, image_id: int | None = None):\n        coco_prediction_list = []\n        for object_prediction in self.object_prediction_list:\n            coco_prediction_list.append(object_prediction.to_coco_prediction(image_id=image_id).json)\n        return coco_prediction_list\n\n    def to_imantics_annotations(self):\n        imantics_annotation_list = []\n        for object_prediction in self.object_prediction_list:\n            imantics_annotation_list.append(object_prediction.to_imantics_annotation())\n        return imantics_annotation_list\n\n    def to_fiftyone_detections(self):\n        try:\n            import fiftyone as fo\n        except ImportError:\n            raise ImportError('Please run \"uv pip install -U fiftyone\" to install fiftyone for conversion.')\n\n        fiftyone_detection_list: list[fo.Detection] = []\n        for object_prediction in self.object_prediction_list:\n            fiftyone_detection_list.append(\n                object_prediction.to_fiftyone_detection(image_height=self.image_height, image_width=self.image_width)\n            )\n        return fiftyone_detection_list\n</code></pre> Functions\u00b6 <code></code> <code>export_visuals(export_dir, text_size=None, rect_th=None, hide_labels=False, hide_conf=False, file_name='prediction_visual')</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>export_dir</code> \u00b6 <code>str</code> <p>directory for resulting visualization to be exported</p> required <code>text_size</code> \u00b6 <code>float | None</code> <p>size of the category name over box</p> <code>None</code> <code>rect_th</code> \u00b6 <code>int | None</code> <p>rectangle thickness</p> <code>None</code> <code>hide_labels</code> \u00b6 <code>bool</code> <p>hide labels</p> <code>False</code> <code>hide_conf</code> \u00b6 <code>bool</code> <p>hide confidence</p> <code>False</code> <code>file_name</code> \u00b6 <code>str</code> <p>saving name</p> <code>'prediction_visual'</code> <p>Returns:</p> Source code in <code>sahi/prediction.py</code> <pre><code>def export_visuals(\n    self,\n    export_dir: str,\n    text_size: float | None = None,\n    rect_th: int | None = None,\n    hide_labels: bool = False,\n    hide_conf: bool = False,\n    file_name: str = \"prediction_visual\",\n):\n    \"\"\"\n\n    Args:\n        export_dir: directory for resulting visualization to be exported\n        text_size: size of the category name over box\n        rect_th: rectangle thickness\n        hide_labels: hide labels\n        hide_conf: hide confidence\n        file_name: saving name\n    Returns:\n\n    \"\"\"\n    Path(export_dir).mkdir(parents=True, exist_ok=True)\n    visualize_object_predictions(\n        image=np.ascontiguousarray(self.image),\n        object_prediction_list=self.object_prediction_list,\n        rect_th=rect_th,\n        text_size=text_size,\n        text_th=None,\n        color=None,\n        hide_labels=hide_labels,\n        hide_conf=hide_conf,\n        output_dir=export_dir,\n        file_name=file_name,\n        export_format=\"png\",\n    )\n</code></pre>"},{"location":"api/#sahi.prediction.PredictionScore","title":"<code>PredictionScore</code>","text":"Source code in <code>sahi/prediction.py</code> <pre><code>class PredictionScore:\n    def __init__(self, value: float | np.ndarray):\n        \"\"\"\n        Args:\n            score: prediction score between 0 and 1\n        \"\"\"\n        # if score is a numpy object, convert it to python variable\n        if type(value).__module__ == \"numpy\":\n            value = copy.deepcopy(value).tolist()\n        # set score\n        self.value = value\n\n    def is_greater_than_threshold(self, threshold):\n        \"\"\"Check if score is greater than threshold.\"\"\"\n        return self.value &gt; threshold\n\n    def __eq__(self, threshold):\n        return self.value == threshold\n\n    def __gt__(self, threshold):\n        return self.value &gt; threshold\n\n    def __lt__(self, threshold):\n        return self.value &lt; threshold\n\n    def __repr__(self):\n        return f\"PredictionScore: &lt;value: {self.value}&gt;\"\n</code></pre> Functions\u00b6 <code></code> <code>__init__(value)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>score</code> \u00b6 <p>prediction score between 0 and 1</p> required Source code in <code>sahi/prediction.py</code> <pre><code>def __init__(self, value: float | np.ndarray):\n    \"\"\"\n    Args:\n        score: prediction score between 0 and 1\n    \"\"\"\n    # if score is a numpy object, convert it to python variable\n    if type(value).__module__ == \"numpy\":\n        value = copy.deepcopy(value).tolist()\n    # set score\n    self.value = value\n</code></pre> <code></code> <code>is_greater_than_threshold(threshold)</code> \u00b6 <p>Check if score is greater than threshold.</p> Source code in <code>sahi/prediction.py</code> <pre><code>def is_greater_than_threshold(self, threshold):\n    \"\"\"Check if score is greater than threshold.\"\"\"\n    return self.value &gt; threshold\n</code></pre>"},{"location":"api/#sahi.prediction-functions","title":"Functions","text":""},{"location":"api/#sahi.scripts","title":"<code>scripts</code>","text":""},{"location":"api/#sahi.scripts-modules","title":"Modules","text":""},{"location":"api/#sahi.scripts.coco2fiftyone","title":"<code>coco2fiftyone</code>","text":"Functions\u00b6 <code>main(image_dir, dataset_json_path, *result_json_paths, iou_thresh=0.5)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>image_dir</code> \u00b6 <code>str</code> <p>directory for coco images</p> required <code>dataset_json_path</code> \u00b6 <code>str</code> <p>file path for the coco dataset json file</p> required <code>result_json_paths</code> \u00b6 <code>str</code> <p>one or more paths for the coco result json file</p> <code>()</code> <code>iou_thresh</code> \u00b6 <code>float</code> <p>iou threshold for coco evaluation</p> <code>0.5</code> Source code in <code>sahi/scripts/coco2fiftyone.py</code> <pre><code>def main(\n    image_dir: str,\n    dataset_json_path: str,\n    *result_json_paths,\n    iou_thresh: float = 0.5,\n):\n    \"\"\"\n    Args:\n        image_dir (str): directory for coco images\n        dataset_json_path (str): file path for the coco dataset json file\n        result_json_paths (str): one or more paths for the coco result json file\n        iou_thresh (float): iou threshold for coco evaluation\n    \"\"\"\n\n    from fiftyone.utils.coco import add_coco_labels\n\n    from sahi.utils.fiftyone import create_fiftyone_dataset_from_coco_file, fo\n\n    coco_result_list = []\n    result_name_list = []\n    if result_json_paths:\n        for result_json_path in result_json_paths:\n            coco_result = load_json(result_json_path)\n            coco_result_list.append(coco_result)\n\n            # use file names as fiftyone name, create unique names if duplicate\n            result_name_temp = Path(result_json_path).stem\n            result_name = result_name_temp\n            name_increment = 2\n            while result_name in result_name_list:\n                result_name = result_name_temp + \"_\" + str(name_increment)\n                name_increment += 1\n            result_name_list.append(result_name)\n\n    dataset = create_fiftyone_dataset_from_coco_file(image_dir, dataset_json_path)\n\n    # submit detections if coco result is given\n    if result_json_paths:\n        for result_name, coco_result in zip(result_name_list, coco_result_list):\n            add_coco_labels(dataset, result_name, coco_result, coco_id_field=\"gt_coco_id\")\n\n    # visualize results\n    session = fo.launch_app()  # pyright: ignore[reportArgumentType]\n    session.dataset = dataset\n\n    # order by false positives if any coco result is given\n    if result_json_paths:\n        # Evaluate the predictions\n        first_coco_result_name = result_name_list[0]\n        _ = dataset.evaluate_detections(\n            first_coco_result_name,\n            gt_field=\"gt_detections\",\n            eval_key=f\"{first_coco_result_name}_eval\",\n            iou=iou_thresh,\n            compute_mAP=False,\n        )\n        # Get the 10 most common classes in the dataset\n        # counts = dataset.count_values(\"gt_detections.detections.label\")\n        # classes_top10 = sorted(counts, key=counts.get, reverse=True)[:10]\n        # Print a classification report for the top-10 classes\n        # results.print_report(classes=classes_top10)\n        # Load the view on which we ran the `eval` evaluation\n        eval_view = dataset.load_evaluation_view(f\"{first_coco_result_name}_eval\")\n        # Show samples with most false positives\n        session.view = eval_view.sort_by(f\"{first_coco_result_name}_eval_fp\", reverse=True)\n\n        print(f\"SAHI has successfully launched a Fiftyone app at http://localhost:{fo.config.default_app_port}\")\n    while 1:\n        time.sleep(3)\n</code></pre>"},{"location":"api/#sahi.scripts.coco2yolo","title":"<code>coco2yolo</code>","text":"Classes\u00b6 Functions\u00b6 <code>main(image_dir, dataset_json_path, train_split=0.9, project='runs/coco2yolo', name='exp', seed=1, disable_symlink=False)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>images_dir</code> \u00b6 <code>str</code> <p>directory for coco images</p> required <code>dataset_json_path</code> \u00b6 <code>str</code> <p>file path for the coco json file to be converted</p> required <code>train_split</code> \u00b6 <code>float or int</code> <p>set the training split ratio</p> <code>0.9</code> <code>project</code> \u00b6 <code>str</code> <p>save results to project/name</p> <code>'runs/coco2yolo'</code> <code>name</code> \u00b6 <code>str</code> <p>save results to project/name\"</p> <code>'exp'</code> <code>seed</code> \u00b6 <code>int</code> <p>fix the seed for reproducibility</p> <code>1</code> <code>disable_symlink</code> \u00b6 <code>bool</code> <p>required in google colab env</p> <code>False</code> Source code in <code>sahi/scripts/coco2yolo.py</code> <pre><code>def main(\n    image_dir: str,\n    dataset_json_path: str,\n    train_split: int | float = 0.9,\n    project: str = \"runs/coco2yolo\",\n    name: str = \"exp\",\n    seed: int = 1,\n    disable_symlink=False,\n):\n    \"\"\"\n    Args:\n        images_dir (str): directory for coco images\n        dataset_json_path (str): file path for the coco json file to be converted\n        train_split (float or int): set the training split ratio\n        project (str): save results to project/name\n        name (str): save results to project/name\"\n        seed (int): fix the seed for reproducibility\n        disable_symlink (bool): required in google colab env\n    \"\"\"\n\n    # increment run\n    save_dir = Path(increment_path(Path(project) / name, exist_ok=False))\n    # load coco dict\n    coco = Coco.from_coco_dict_or_path(\n        coco_dict_or_path=dataset_json_path,\n        image_dir=image_dir,\n    )\n    # export as YOLO\n    coco.export_as_yolo(\n        output_dir=str(save_dir),\n        train_split_rate=train_split,\n        numpy_seed=seed,\n        disable_symlink=disable_symlink,\n    )\n\n    print(f\"COCO to YOLO conversion results are successfully exported to {save_dir}\")\n</code></pre>"},{"location":"api/#sahi.scripts.coco_error_analysis","title":"<code>coco_error_analysis</code>","text":"Functions\u00b6 <code>analyse(dataset_json_path, result_json_path, out_dir=None, type='bbox', no_extraplots=False, areas=[1024, 9216, 10000000000], max_detections=500, return_dict=False)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>dataset_json_path</code> \u00b6 <code>str</code> <p>file path for the coco dataset json file</p> required <code>result_json_paths</code> \u00b6 <code>str</code> <p>file path for the coco result json file</p> required <code>out_dir</code> \u00b6 <code>str</code> <p>dir to save analyse result images</p> <code>None</code> <code>no_extraplots</code> \u00b6 <code>bool</code> <p>dont export export extra bar/stat plots</p> <code>False</code> <code>type</code> \u00b6 <code>str</code> <p>'bbox' or 'mask'</p> <code>'bbox'</code> <code>areas</code> \u00b6 <code>List[int]</code> <p>area regions for coco evaluation calculations</p> <code>[1024, 9216, 10000000000]</code> <code>max_detections</code> \u00b6 <code>int</code> <p>Maximum number of detections to consider for AP alculation. Default: 500</p> <code>500</code> <code>return_dict</code> \u00b6 <code>bool</code> <p>If True, returns a dict export paths.</p> <code>False</code> Source code in <code>sahi/scripts/coco_error_analysis.py</code> <pre><code>def analyse(\n    dataset_json_path: str,\n    result_json_path: str,\n    out_dir: str | None = None,\n    type: str = \"bbox\",\n    no_extraplots: bool = False,\n    areas: list[int] = [1024, 9216, 10000000000],\n    max_detections: int = 500,\n    return_dict: bool = False,\n) -&gt; dict | None:\n    \"\"\"\n    Args:\n        dataset_json_path (str): file path for the coco dataset json file\n        result_json_paths (str): file path for the coco result json file\n        out_dir (str): dir to save analyse result images\n        no_extraplots (bool): dont export export extra bar/stat plots\n        type (str): 'bbox' or 'mask'\n        areas (List[int]): area regions for coco evaluation calculations\n        max_detections (int): Maximum number of detections to consider for AP alculation. Default: 500\n        return_dict (bool): If True, returns a dict export paths.\n    \"\"\"\n    if not has_matplotlib:\n        logger.error(\"Please run 'uv pip install -U matplotlib' first for visualization.\")\n        raise ModuleNotFoundError(\"matplotlib not installed\")\n    if not has_pycocotools:\n        logger.error(\"Please run 'uv pip install -U pycocotools' first for Coco analysis.\")\n        raise ModuleNotFoundError(\"pycocotools not installed\")\n\n    result = _analyse_results(\n        result_json_path,\n        dataset_json_path,\n        res_types=[type],\n        out_dir=out_dir,\n        extraplots=not no_extraplots,\n        areas=areas,\n        max_detections=max_detections,\n    )\n    if return_dict:\n        return result\n</code></pre>"},{"location":"api/#sahi.scripts.coco_evaluation","title":"<code>coco_evaluation</code>","text":"Functions\u00b6 <code>evaluate(dataset_json_path, result_json_path, out_dir=None, type='bbox', classwise=False, max_detections=500, iou_thrs=None, areas=[1024, 9216, 10000000000], return_dict=False)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>dataset_json_path</code> \u00b6 <code>str</code> <p>file path for the coco dataset json file</p> required <code>result_json_path</code> \u00b6 <code>str</code> <p>file path for the coco result json file</p> required <code>out_dir</code> \u00b6 <code>str</code> <p>dir to save eval result</p> <code>None</code> <code>type</code> \u00b6 <code>bool</code> <p>'bbox' or 'segm'</p> <code>'bbox'</code> <code>classwise</code> \u00b6 <code>bool</code> <p>whether to evaluate the AP for each class</p> <code>False</code> <code>max_detections</code> \u00b6 <code>int</code> <p>Maximum number of detections to consider for AP alculation. Default: 500</p> <code>500</code> <code>iou_thrs</code> \u00b6 <code>float</code> <p>IoU threshold used for evaluating recalls/mAPs</p> <code>None</code> <code>areas</code> \u00b6 <code>List[int]</code> <p>area regions for coco evaluation calculations</p> <code>[1024, 9216, 10000000000]</code> <code>return_dict</code> \u00b6 <code>bool</code> <p>If True, returns a dict with 'eval_results' 'export_path' fields.</p> <code>False</code> Source code in <code>sahi/scripts/coco_evaluation.py</code> <pre><code>def evaluate(\n    dataset_json_path: str,\n    result_json_path: str,\n    out_dir: str | None = None,\n    type: Literal[\"bbox\", \"segm\"] = \"bbox\",\n    classwise: bool = False,\n    max_detections: int = 500,\n    iou_thrs: list[float] | float | None = None,\n    areas: list[int] = [1024, 9216, 10000000000],\n    return_dict: bool = False,\n):\n    \"\"\"\n    Args:\n        dataset_json_path (str): file path for the coco dataset json file\n        result_json_path (str): file path for the coco result json file\n        out_dir (str): dir to save eval result\n        type (bool): 'bbox' or 'segm'\n        classwise (bool): whether to evaluate the AP for each class\n        max_detections (int): Maximum number of detections to consider for AP alculation. Default: 500\n        iou_thrs (float): IoU threshold used for evaluating recalls/mAPs\n        areas (List[int]): area regions for coco evaluation calculations\n        return_dict (bool): If True, returns a dict with 'eval_results' 'export_path' fields.\n    \"\"\"\n    try:\n        from pycocotools.coco import COCO\n        from pycocotools.cocoeval import COCOeval\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            'Please run \"pip install -U pycocotools\" to install pycocotools first for coco evaluation.'\n        )\n\n    # perform coco eval\n    result = evaluate_core(\n        dataset_path=dataset_json_path,\n        result_path=result_json_path,\n        metric=type,\n        classwise=classwise,\n        max_detections=max_detections,\n        iou_thrs=iou_thrs,\n        out_dir=out_dir,\n        areas=areas,\n        COCO=COCO,\n        COCOeval=COCOeval,\n    )\n    if return_dict:\n        return result\n</code></pre> <code></code> <code>evaluate_core(dataset_path, result_path, COCO, COCOeval, metric='bbox', classwise=False, max_detections=500, iou_thrs=None, metric_items=None, out_dir=None, areas=[1024, 9216, 10000000000])</code> \u00b6 <p>Evaluation in COCO protocol.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> \u00b6 <code>str</code> <p>COCO dataset json path.</p> required <code>result_path</code> \u00b6 <code>str</code> <p>COCO result json path.</p> required <code>COCO, COCOeval</code> \u00b6 <p>Pass COCO and COCOeval class after safely imported</p> required <code>metric</code> \u00b6 <code>str | list[str]</code> <p>Metrics to be evaluated. Options are 'bbox', 'segm', 'proposal'.</p> <code>'bbox'</code> <code>classwise</code> \u00b6 <code>bool</code> <p>Whether to evaluating the AP for each class.</p> <code>False</code> <code>max_detections</code> \u00b6 <code>int</code> <p>Maximum number of detections to consider for AP calculation. Default: 500</p> <code>500</code> <code>iou_thrs</code> \u00b6 <code>List[float]</code> <p>IoU threshold used for evaluating recalls/mAPs. If set to a list, the average of all IoUs will also be computed. If not specified, [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used. Default: None.</p> <code>None</code> <code>metric_items</code> \u00b6 <code>list[str] | str</code> <p>Metric items that will be returned. If not specified, <code>['AR@10', 'AR@100', 'AR@500', 'AR_s@500', 'AR_m@500', 'AR_l@500' ]</code> will be used when <code>metric=='proposal'</code>, <code>['mAP', 'mAP50', 'mAP75', 'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']</code> will be used when <code>metric=='bbox' or metric=='segm'</code>.</p> <code>None</code> <code>out_dir</code> \u00b6 <code>str</code> <p>Directory to save evaluation result json.</p> <code>None</code> <code>areas</code> \u00b6 <code>List[int]</code> <p>area regions for coco evaluation calculations</p> <code>[1024, 9216, 10000000000]</code> <p>Returns:     dict:         eval_results (dict[str, float]): COCO style evaluation metric.         export_path (str): Path for the exported eval result json.</p> Source code in <code>sahi/scripts/coco_evaluation.py</code> <pre><code>def evaluate_core(\n    dataset_path: str,\n    result_path: str,\n    COCO: type,\n    COCOeval: type,\n    metric: str = \"bbox\",\n    classwise: bool = False,\n    max_detections: int = 500,\n    iou_thrs=None,\n    metric_items=None,\n    out_dir: str | Path | None = None,\n    areas: list[int] = [1024, 9216, 10000000000],\n):\n    \"\"\"Evaluation in COCO protocol.\n\n    Args:\n        dataset_path (str): COCO dataset json path.\n        result_path (str): COCO result json path.\n        COCO, COCOeval: Pass COCO and COCOeval class after safely imported\n        metric (str | list[str]): Metrics to be evaluated. Options are\n            'bbox', 'segm', 'proposal'.\n        classwise (bool): Whether to evaluating the AP for each class.\n        max_detections (int): Maximum number of detections to consider for AP\n            calculation.\n            Default: 500\n        iou_thrs (List[float], optional): IoU threshold used for\n            evaluating recalls/mAPs. If set to a list, the average of all\n            IoUs will also be computed. If not specified, [0.50, 0.55,\n            0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used.\n            Default: None.\n        metric_items (list[str] | str, optional): Metric items that will\n            be returned. If not specified, ``['AR@10', 'AR@100',\n            'AR@500', 'AR_s@500', 'AR_m@500', 'AR_l@500' ]`` will be\n            used when ``metric=='proposal'``, ``['mAP', 'mAP50', 'mAP75',\n            'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']``\n            will be used when ``metric=='bbox' or metric=='segm'``.\n        out_dir (str): Directory to save evaluation result json.\n        areas (List[int]): area regions for coco evaluation calculations\n    Returns:\n        dict:\n            eval_results (dict[str, float]): COCO style evaluation metric.\n            export_path (str): Path for the exported eval result json.\n    \"\"\"\n\n    metrics = metric if isinstance(metric, list) else [metric]\n    allowed_metrics = [\"bbox\", \"segm\"]\n    for metric in metrics:\n        if metric not in allowed_metrics:\n            raise KeyError(f\"metric {metric} is not supported\")\n    if iou_thrs is None:\n        iou_thrs = np.linspace(0.5, 0.95, int(np.round((0.95 - 0.5) / 0.05)) + 1, endpoint=True)\n    if metric_items is not None:\n        if not isinstance(metric_items, list):\n            metric_items = [metric_items]\n    if areas is not None:\n        if len(areas) != 3:\n            raise ValueError(\"3 integers should be specified as areas, representing 3 area regions\")\n    eval_results = OrderedDict()\n\n    # Load dataset json and add empty 'info' field if missing\n    with open(dataset_path) as f:\n        dataset_dict = json.load(f)\n    if \"info\" not in dataset_dict:\n        dataset_dict[\"info\"] = {}\n\n    # Create temporary file with updated dataset\n    import tempfile\n\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".json\", delete=False) as tmp_file:\n        json.dump(dataset_dict, tmp_file)\n        temp_dataset_path = tmp_file.name\n\n    try:\n        cocoGt = COCO(temp_dataset_path)\n        cat_ids = list(cocoGt.cats.keys())\n        for metric in metrics:\n            msg = f\"Evaluating {metric}...\"\n            msg = \"\\n\" + msg\n            print(msg)\n\n            iou_type = metric\n            with open(result_path) as json_file:\n                results = json.load(json_file)\n            try:\n                cocoDt = cocoGt.loadRes(results)\n            except IndexError:\n                print(\"The testing results of the whole dataset is empty.\")\n                break\n\n            cocoEval = COCOeval(cocoGt, cocoDt, iou_type)\n            if areas is not None:\n                cocoEval.params.areaRng = [\n                    [0**2, areas[2]],\n                    [0**2, areas[0]],\n                    [areas[0], areas[1]],\n                    [areas[1], areas[2]],\n                ]\n            cocoEval.params.catIds = cat_ids\n            cocoEval.params.maxDets = [max_detections]\n            cocoEval.params.iouThrs = (\n                [iou_thrs] if not isinstance(iou_thrs, list) and not isinstance(iou_thrs, np.ndarray) else iou_thrs\n            )\n            # mapping of cocoEval.stats\n            coco_metric_names = {\n                \"mAP\": 0,\n                \"mAP75\": 1,\n                \"mAP50\": 2,\n                \"mAP_s\": 3,\n                \"mAP_m\": 4,\n                \"mAP_l\": 5,\n                \"mAP50_s\": 6,\n                \"mAP50_m\": 7,\n                \"mAP50_l\": 8,\n                \"AR_s\": 9,\n                \"AR_m\": 10,\n                \"AR_l\": 11,\n            }\n            if metric_items is not None:\n                for metric_item in metric_items:\n                    if metric_item not in coco_metric_names:\n                        raise KeyError(f\"metric item {metric_item} is not supported\")\n\n            cocoEval.evaluate()\n            cocoEval.accumulate()\n            # calculate mAP50_s/m/l\n            mAP = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng=\"all\", maxDets=max_detections)\n            mAP50 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng=\"all\", maxDets=max_detections)\n            mAP75 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.75, areaRng=\"all\", maxDets=max_detections)\n            mAP50_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng=\"small\", maxDets=max_detections)\n            mAP50_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng=\"medium\", maxDets=max_detections)\n            mAP50_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng=\"large\", maxDets=max_detections)\n            mAP_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng=\"small\", maxDets=max_detections)\n            mAP_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng=\"medium\", maxDets=max_detections)\n            mAP_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng=\"large\", maxDets=max_detections)\n            AR_s = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng=\"small\", maxDets=max_detections)\n            AR_m = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng=\"medium\", maxDets=max_detections)\n            AR_l = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng=\"large\", maxDets=max_detections)\n            cocoEval.stats = np.append(\n                [mAP, mAP75, mAP50, mAP_s, mAP_m, mAP_l, mAP50_s, mAP50_m, mAP50_l, AR_s, AR_m, AR_l], 0\n            )\n\n            if classwise:  # Compute per-category AP\n                # Compute per-category AP\n                # from https://github.com/facebookresearch/detectron2/\n                precisions = cocoEval.eval[\"precision\"]\n                # precision: (iou, recall, cls, area range, max dets)\n                if len(cat_ids) != precisions.shape[2]:\n                    raise ValueError(\n                        f\"The number of categories {len(cat_ids)} is not equal \"\n                        f\"to the number of precisions {precisions.shape[2]}\"\n                    )\n                max_cat_name_len = 0\n                for idx, catId in enumerate(cat_ids):\n                    nm = cocoGt.loadCats(catId)[0]\n                    cat_name_len = len(nm[\"name\"])\n                    max_cat_name_len = cat_name_len if cat_name_len &gt; max_cat_name_len else max_cat_name_len\n\n                results_per_category = []\n                for idx, catId in enumerate(cat_ids):\n                    # skip if no image with this category\n                    image_ids = cocoGt.getImgIds(catIds=[catId])\n                    if len(image_ids) == 0:\n                        continue\n                    # area range index 0: all area ranges\n                    # max dets index -1: typically 100 per image\n                    nm = cocoGt.loadCats(catId)[0]\n                    ap = _cocoeval_summarize(\n                        cocoEval,\n                        ap=1,\n                        catIdx=idx,\n                        areaRng=\"all\",\n                        maxDets=max_detections,\n                        catName=nm[\"name\"],\n                        nameStrLen=max_cat_name_len,\n                    )\n                    ap_s = _cocoeval_summarize(\n                        cocoEval,\n                        ap=1,\n                        catIdx=idx,\n                        areaRng=\"small\",\n                        maxDets=max_detections,\n                        catName=nm[\"name\"],\n                        nameStrLen=max_cat_name_len,\n                    )\n                    ap_m = _cocoeval_summarize(\n                        cocoEval,\n                        ap=1,\n                        catIdx=idx,\n                        areaRng=\"medium\",\n                        maxDets=max_detections,\n                        catName=nm[\"name\"],\n                        nameStrLen=max_cat_name_len,\n                    )\n                    ap_l = _cocoeval_summarize(\n                        cocoEval,\n                        ap=1,\n                        catIdx=idx,\n                        areaRng=\"large\",\n                        maxDets=max_detections,\n                        catName=nm[\"name\"],\n                        nameStrLen=max_cat_name_len,\n                    )\n                    ap50 = _cocoeval_summarize(\n                        cocoEval,\n                        ap=1,\n                        iouThr=0.5,\n                        catIdx=idx,\n                        areaRng=\"all\",\n                        maxDets=max_detections,\n                        catName=nm[\"name\"],\n                        nameStrLen=max_cat_name_len,\n                    )\n                    ap50_s = _cocoeval_summarize(\n                        cocoEval,\n                        ap=1,\n                        iouThr=0.5,\n                        catIdx=idx,\n                        areaRng=\"small\",\n                        maxDets=max_detections,\n                        catName=nm[\"name\"],\n                        nameStrLen=max_cat_name_len,\n                    )\n                    ap50_m = _cocoeval_summarize(\n                        cocoEval,\n                        ap=1,\n                        iouThr=0.5,\n                        catIdx=idx,\n                        areaRng=\"medium\",\n                        maxDets=max_detections,\n                        catName=nm[\"name\"],\n                        nameStrLen=max_cat_name_len,\n                    )\n                    ap50_l = _cocoeval_summarize(\n                        cocoEval,\n                        ap=1,\n                        iouThr=0.5,\n                        catIdx=idx,\n                        areaRng=\"large\",\n                        maxDets=max_detections,\n                        catName=nm[\"name\"],\n                        nameStrLen=max_cat_name_len,\n                    )\n                    results_per_category.append((f\"{metric}_{nm['name']}_mAP\", f\"{float(ap):0.3f}\"))\n                    results_per_category.append((f\"{metric}_{nm['name']}_mAP_s\", f\"{float(ap_s):0.3f}\"))\n                    results_per_category.append((f\"{metric}_{nm['name']}_mAP_m\", f\"{float(ap_m):0.3f}\"))\n                    results_per_category.append((f\"{metric}_{nm['name']}_mAP_l\", f\"{float(ap_l):0.3f}\"))\n                    results_per_category.append((f\"{metric}_{nm['name']}_mAP50\", f\"{float(ap50):0.3f}\"))\n                    results_per_category.append((f\"{metric}_{nm['name']}_mAP50_s\", f\"{float(ap50_s):0.3f}\"))\n                    results_per_category.append((f\"{metric}_{nm['name']}_mAP50_m\", f\"{float(ap50_m):0.3f}\"))\n                    results_per_category.append((f\"{metric}_{nm['name']}_mAP50_l\", f\"{float(ap50_l):0.3f}\"))\n\n                num_columns = min(6, len(results_per_category) * 2)\n                results_flatten = list(itertools.chain(*results_per_category))\n                headers = [\"category\", \"AP\"] * (num_columns // 2)\n                results_2d = itertools.zip_longest(*[results_flatten[i::num_columns] for i in range(num_columns)])\n                table_data = [headers]\n                table_data += [result for result in results_2d]\n                table = AsciiTable(table_data)\n                print(\"\\n\" + table.table)\n\n            if metric_items is None:\n                metric_items = [\"mAP\", \"mAP50\", \"mAP75\", \"mAP_s\", \"mAP_m\", \"mAP_l\", \"mAP50_s\", \"mAP50_m\", \"mAP50_l\"]\n\n            for metric_item in metric_items:\n                key = f\"{metric}_{metric_item}\"\n                val = float(f\"{cocoEval.stats[coco_metric_names[metric_item]]:.3f}\")\n                eval_results[key] = val\n            ap = cocoEval.stats\n            eval_results[f\"{metric}_mAP_copypaste\"] = (\n                f\"{ap[0]:.3f} {ap[1]:.3f} {ap[2]:.3f} {ap[3]:.3f} \"\n                f\"{ap[4]:.3f} {ap[5]:.3f} {ap[6]:.3f} {ap[7]:.3f} \"\n                f\"{ap[8]:.3f}\"\n            )\n            if classwise:\n                eval_results[\"results_per_category\"] = {key: value for key, value in results_per_category}\n    finally:\n        # Clean up temporary file\n        os.unlink(temp_dataset_path)\n\n    # set save path\n    if not out_dir:\n        out_dir = Path(result_path).parent\n    Path(out_dir).mkdir(parents=True, exist_ok=True)\n    export_path = str(Path(out_dir) / \"eval.json\")\n    # export as json\n    with open(export_path, \"w\", encoding=\"utf-8\") as outfile:\n        json.dump(eval_results, outfile, indent=4, separators=(\",\", \":\"))\n    print(f\"COCO evaluation results are successfully exported to {export_path}\")\n    return {\"eval_results\": eval_results, \"export_path\": export_path}\n</code></pre>"},{"location":"api/#sahi.scripts.slice_coco","title":"<code>slice_coco</code>","text":"Functions\u00b6 <code>slicer(image_dir, dataset_json_path, slice_size=512, overlap_ratio=0.2, ignore_negative_samples=False, output_dir='runs/slice_coco', min_area_ratio=0.1)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>image_dir</code> \u00b6 <code>str</code> <p>directory for coco images</p> required <code>dataset_json_path</code> \u00b6 <code>str</code> <p>file path for the coco dataset json file</p> required <code>overlap_ratio</code> \u00b6 <code>float</code> <p>slice overlap ratio</p> <code>0.2</code> <code>ignore_negative_samples</code> \u00b6 <code>bool</code> <p>ignore images without annotation</p> <code>False</code> <code>output_dir</code> \u00b6 <code>str</code> <p>output export dir</p> <code>'runs/slice_coco'</code> <code>min_area_ratio</code> \u00b6 <code>float</code> <p>If the cropped annotation area to original annotation ratio is smaller than this value, the annotation is filtered out. Default 0.1.</p> <code>0.1</code> Source code in <code>sahi/scripts/slice_coco.py</code> <pre><code>def slicer(\n    image_dir: str,\n    dataset_json_path: str,\n    slice_size: int = 512,\n    overlap_ratio: float = 0.2,\n    ignore_negative_samples: bool = False,\n    output_dir: str = \"runs/slice_coco\",\n    min_area_ratio: float = 0.1,\n):\n    \"\"\"\n    Args:\n        image_dir (str): directory for coco images\n        dataset_json_path (str): file path for the coco dataset json file\n        slice_size (int)\n        overlap_ratio (float): slice overlap ratio\n        ignore_negative_samples (bool): ignore images without annotation\n        output_dir (str): output export dir\n        min_area_ratio (float): If the cropped annotation area to original\n            annotation ratio is smaller than this value, the annotation\n            is filtered out. Default 0.1.\n    \"\"\"\n\n    # assure slice_size is list\n    slice_size_list = slice_size\n    if isinstance(slice_size_list, (int, float)):\n        slice_size_list = [slice_size_list]\n\n    # slice coco dataset images and annotations\n    print(\"Slicing step is starting...\")\n    for slice_size in slice_size_list:\n        # in format: train_images_512_01\n        output_images_folder_name = (\n            Path(dataset_json_path).stem + f\"_images_{slice_size!s}_{str(overlap_ratio).replace('.', '')}\"\n        )\n        output_images_dir = str(Path(output_dir) / output_images_folder_name)\n        sliced_coco_name = Path(dataset_json_path).name.replace(\n            \".json\", f\"_{slice_size!s}_{str(overlap_ratio).replace('.', '')}\"\n        )\n        coco_dict, _ = slice_coco(\n            coco_annotation_file_path=dataset_json_path,\n            image_dir=image_dir,\n            output_coco_annotation_file_name=\"\",\n            output_dir=output_images_dir,\n            ignore_negative_samples=ignore_negative_samples,\n            slice_height=slice_size,\n            slice_width=slice_size,\n            min_area_ratio=min_area_ratio,\n            overlap_height_ratio=overlap_ratio,\n            overlap_width_ratio=overlap_ratio,\n            out_ext=\".jpg\",\n            verbose=False,\n        )\n        output_coco_annotation_file_path = os.path.join(output_dir, sliced_coco_name + \".json\")\n        save_json(coco_dict, output_coco_annotation_file_path)\n        print(f\"Sliced dataset for 'slice_size: {slice_size}' is exported to {output_dir}\")\n</code></pre>"},{"location":"api/#sahi.slicing","title":"<code>slicing</code>","text":""},{"location":"api/#sahi.slicing-classes","title":"Classes","text":""},{"location":"api/#sahi.slicing.SliceImageResult","title":"<code>SliceImageResult</code>","text":"Source code in <code>sahi/slicing.py</code> <pre><code>class SliceImageResult:\n    def __init__(self, original_image_size: list[int], image_dir: str | None = None):\n        \"\"\"\n        image_dir: str\n            Directory of the sliced image exports.\n        original_image_size: list of int\n            Size of the unsliced original image in [height, width]\n        \"\"\"\n        self.original_image_height = original_image_size[0]\n        self.original_image_width = original_image_size[1]\n        self.image_dir = image_dir\n\n        self._sliced_image_list: list[SlicedImage] = []\n\n    def add_sliced_image(self, sliced_image: SlicedImage):\n        if not isinstance(sliced_image, SlicedImage):\n            raise TypeError(\"sliced_image must be a SlicedImage instance\")\n\n        self._sliced_image_list.append(sliced_image)\n\n    @property\n    def sliced_image_list(self):\n        return self._sliced_image_list\n\n    @property\n    def images(self):\n        \"\"\"Returns sliced images.\n\n        Returns:\n            images: a list of np.array\n        \"\"\"\n        images = []\n        for sliced_image in self._sliced_image_list:\n            images.append(sliced_image.image)\n        return images\n\n    @property\n    def coco_images(self) -&gt; list[CocoImage]:\n        \"\"\"Returns CocoImage representation of SliceImageResult.\n\n        Returns:\n            coco_images: a list of CocoImage\n        \"\"\"\n        coco_images: list = []\n        for sliced_image in self._sliced_image_list:\n            coco_images.append(sliced_image.coco_image)\n        return coco_images\n\n    @property\n    def starting_pixels(self) -&gt; list[int]:\n        \"\"\"Returns a list of starting pixels for each slice.\n\n        Returns:\n            starting_pixels: a list of starting pixel coords [x,y]\n        \"\"\"\n        starting_pixels = []\n        for sliced_image in self._sliced_image_list:\n            starting_pixels.append(sliced_image.starting_pixel)\n        return starting_pixels\n\n    @property\n    def filenames(self) -&gt; list[int]:\n        \"\"\"Returns a list of filenames for each slice.\n\n        Returns:\n            filenames: a list of filenames as str\n        \"\"\"\n        filenames = []\n        for sliced_image in self._sliced_image_list:\n            filenames.append(sliced_image.coco_image.file_name)\n        return filenames\n\n    def __getitem__(self, i):\n        def _prepare_ith_dict(i):\n            return {\n                \"image\": self.images[i],\n                \"coco_image\": self.coco_images[i],\n                \"starting_pixel\": self.starting_pixels[i],\n                \"filename\": self.filenames[i],\n            }\n\n        if isinstance(i, np.ndarray):\n            i = i.tolist()\n\n        if isinstance(i, int):\n            return _prepare_ith_dict(i)\n        elif isinstance(i, slice):\n            start, stop, step = i.indices(len(self))\n            return [_prepare_ith_dict(i) for i in range(start, stop, step)]\n        elif isinstance(i, (tuple, list)):\n            accessed_mapping = map(_prepare_ith_dict, i)\n            return list(accessed_mapping)\n        else:\n            raise NotImplementedError(f\"{type(i)}\")\n\n    def __len__(self):\n        return len(self._sliced_image_list)\n</code></pre> Attributes\u00b6 <code></code> <code>coco_images</code> <code>property</code> \u00b6 <p>Returns CocoImage representation of SliceImageResult.</p> <p>Returns:</p> Name Type Description <code>coco_images</code> <code>list[CocoImage]</code> <p>a list of CocoImage</p> <code></code> <code>filenames</code> <code>property</code> \u00b6 <p>Returns a list of filenames for each slice.</p> <p>Returns:</p> Name Type Description <code>filenames</code> <code>list[int]</code> <p>a list of filenames as str</p> <code></code> <code>images</code> <code>property</code> \u00b6 <p>Returns sliced images.</p> <p>Returns:</p> Name Type Description <code>images</code> <p>a list of np.array</p> <code></code> <code>starting_pixels</code> <code>property</code> \u00b6 <p>Returns a list of starting pixels for each slice.</p> <p>Returns:</p> Name Type Description <code>starting_pixels</code> <code>list[int]</code> <p>a list of starting pixel coords [x,y]</p> Functions\u00b6 <code></code> <code>__init__(original_image_size, image_dir=None)</code> \u00b6 str <p>Directory of the sliced image exports.</p> <p>original_image_size: list of int     Size of the unsliced original image in [height, width]</p> Source code in <code>sahi/slicing.py</code> <pre><code>def __init__(self, original_image_size: list[int], image_dir: str | None = None):\n    \"\"\"\n    image_dir: str\n        Directory of the sliced image exports.\n    original_image_size: list of int\n        Size of the unsliced original image in [height, width]\n    \"\"\"\n    self.original_image_height = original_image_size[0]\n    self.original_image_width = original_image_size[1]\n    self.image_dir = image_dir\n\n    self._sliced_image_list: list[SlicedImage] = []\n</code></pre>"},{"location":"api/#sahi.slicing.SlicedImage","title":"<code>SlicedImage</code>","text":"Source code in <code>sahi/slicing.py</code> <pre><code>class SlicedImage:\n    def __init__(self, image, coco_image, starting_pixel):\n        \"\"\"\n        image: np.array\n            Sliced image.\n        coco_image: CocoImage\n            Coco styled image object that belong to sliced image.\n        starting_pixel: list of list of int\n            Starting pixel coordinates of the sliced image.\n        \"\"\"\n        self.image = image\n        self.coco_image = coco_image\n        self.starting_pixel = starting_pixel\n</code></pre> Functions\u00b6 <code></code> <code>__init__(image, coco_image, starting_pixel)</code> \u00b6 np.array <p>Sliced image.</p> <p>coco_image: CocoImage     Coco styled image object that belong to sliced image. starting_pixel: list of list of int     Starting pixel coordinates of the sliced image.</p> Source code in <code>sahi/slicing.py</code> <pre><code>def __init__(self, image, coco_image, starting_pixel):\n    \"\"\"\n    image: np.array\n        Sliced image.\n    coco_image: CocoImage\n        Coco styled image object that belong to sliced image.\n    starting_pixel: list of list of int\n        Starting pixel coordinates of the sliced image.\n    \"\"\"\n    self.image = image\n    self.coco_image = coco_image\n    self.starting_pixel = starting_pixel\n</code></pre>"},{"location":"api/#sahi.slicing-functions","title":"Functions","text":""},{"location":"api/#sahi.slicing.annotation_inside_slice","title":"<code>annotation_inside_slice(annotation, slice_bbox)</code>","text":"<p>Check whether annotation coordinates lie inside slice coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>annotation</code> \u00b6 <code>dict</code> <p>Single annotation entry in COCO format.</p> required <code>slice_bbox</code> \u00b6 <code>List[int]</code> <p>Generated from <code>get_slice_bboxes</code>. Format for each slice bbox: [x_min, y_min, x_max, y_max].</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if any annotation coordinate lies inside slice.</p> Source code in <code>sahi/slicing.py</code> <pre><code>def annotation_inside_slice(annotation: dict, slice_bbox: list[int]) -&gt; bool:\n    \"\"\"Check whether annotation coordinates lie inside slice coordinates.\n\n    Args:\n        annotation (dict): Single annotation entry in COCO format.\n        slice_bbox (List[int]): Generated from `get_slice_bboxes`.\n            Format for each slice bbox: [x_min, y_min, x_max, y_max].\n\n    Returns:\n        (bool): True if any annotation coordinate lies inside slice.\n    \"\"\"\n    left, top, width, height = annotation[\"bbox\"]\n\n    right = left + width\n    bottom = top + height\n\n    if left &gt;= slice_bbox[2]:\n        return False\n    if top &gt;= slice_bbox[3]:\n        return False\n    if right &lt;= slice_bbox[0]:\n        return False\n    if bottom &lt;= slice_bbox[1]:\n        return False\n\n    return True\n</code></pre>"},{"location":"api/#sahi.slicing.calc_aspect_ratio_orientation","title":"<code>calc_aspect_ratio_orientation(width, height)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>width</code> \u00b6 <code>int</code> required <code>height</code> \u00b6 <code>int</code> required <p>Returns:</p> Type Description <code>str</code> <p>image capture orientation</p> Source code in <code>sahi/slicing.py</code> <pre><code>def calc_aspect_ratio_orientation(width: int, height: int) -&gt; str:\n    \"\"\"\n\n    Args:\n        width:\n        height:\n\n    Returns:\n        image capture orientation\n    \"\"\"\n\n    if width &lt; height:\n        return \"vertical\"\n    elif width &gt; height:\n        return \"horizontal\"\n    else:\n        return \"square\"\n</code></pre>"},{"location":"api/#sahi.slicing.calc_ratio_and_slice","title":"<code>calc_ratio_and_slice(orientation, slide=1, ratio=0.1)</code>","text":"<p>According to image resolution calculation overlap params Args:     orientation: image capture angle     slide: sliding window     ratio: buffer value</p> <p>Returns:</p> Type Description <p>overlap params</p> Source code in <code>sahi/slicing.py</code> <pre><code>def calc_ratio_and_slice(orientation: Literal[\"vertical\", \"horizontal\", \"square\"], slide: int = 1, ratio: float = 0.1):\n    \"\"\"\n    According to image resolution calculation overlap params\n    Args:\n        orientation: image capture angle\n        slide: sliding window\n        ratio: buffer value\n\n    Returns:\n        overlap params\n    \"\"\"\n    if orientation == \"vertical\":\n        slice_row, slice_col, overlap_height_ratio, overlap_width_ratio = slide, slide * 2, ratio, ratio\n    elif orientation == \"horizontal\":\n        slice_row, slice_col, overlap_height_ratio, overlap_width_ratio = slide * 2, slide, ratio, ratio\n    elif orientation == \"square\":\n        slice_row, slice_col, overlap_height_ratio, overlap_width_ratio = slide, slide, ratio, ratio\n    else:\n        raise ValueError(f\"Invalid orientation: {orientation}. Must be one of 'vertical', 'horizontal', or 'square'.\")\n\n    return slice_row, slice_col, overlap_height_ratio, overlap_width_ratio\n</code></pre>"},{"location":"api/#sahi.slicing.calc_resolution_factor","title":"<code>calc_resolution_factor(resolution)</code>","text":"<p>According to image resolution calculate power(2,n) and return the closest smaller <code>n</code>. Args:     resolution: the width and height of the image multiplied. such as 1024x720 = 737280</p> <p>Returns:</p> Source code in <code>sahi/slicing.py</code> <pre><code>def calc_resolution_factor(resolution: int) -&gt; int:\n    \"\"\"\n    According to image resolution calculate power(2,n) and return the closest smaller `n`.\n    Args:\n        resolution: the width and height of the image multiplied. such as 1024x720 = 737280\n\n    Returns:\n\n    \"\"\"\n    expo = 0\n    while np.power(2, expo) &lt; resolution:\n        expo += 1\n\n    return expo - 1\n</code></pre>"},{"location":"api/#sahi.slicing.calc_slice_and_overlap_params","title":"<code>calc_slice_and_overlap_params(resolution, height, width, orientation)</code>","text":"<p>This function calculate according to image resolution slice and overlap params. Args:     resolution: str     height: int     width: int     orientation: str</p> <p>Returns:</p> Type Description <code>tuple[int, int, int, int]</code> <p>x_overlap, y_overlap, slice_width, slice_height</p> Source code in <code>sahi/slicing.py</code> <pre><code>def calc_slice_and_overlap_params(\n    resolution: str, height: int, width: int, orientation: str\n) -&gt; tuple[int, int, int, int]:\n    \"\"\"\n    This function calculate according to image resolution slice and overlap params.\n    Args:\n        resolution: str\n        height: int\n        width: int\n        orientation: str\n\n    Returns:\n        x_overlap, y_overlap, slice_width, slice_height\n    \"\"\"\n\n    if resolution == \"medium\":\n        split_row, split_col, overlap_height_ratio, overlap_width_ratio = calc_ratio_and_slice(\n            orientation, slide=1, ratio=0.8\n        )\n\n    elif resolution == \"high\":\n        split_row, split_col, overlap_height_ratio, overlap_width_ratio = calc_ratio_and_slice(\n            orientation, slide=2, ratio=0.4\n        )\n\n    elif resolution == \"ultra-high\":\n        split_row, split_col, overlap_height_ratio, overlap_width_ratio = calc_ratio_and_slice(\n            orientation, slide=4, ratio=0.4\n        )\n    else:  # low condition\n        split_col = 1\n        split_row = 1\n        overlap_width_ratio = 1\n        overlap_height_ratio = 1\n\n    slice_height = height // split_col\n    slice_width = width // split_row\n\n    x_overlap = int(slice_width * overlap_width_ratio)\n    y_overlap = int(slice_height * overlap_height_ratio)\n\n    return x_overlap, y_overlap, slice_width, slice_height\n</code></pre>"},{"location":"api/#sahi.slicing.get_auto_slice_params","title":"<code>get_auto_slice_params(height, width)</code>","text":"<p>According to Image HxW calculate overlap sliding window and buffer params factor is the power value of 2 closest to the image resolution.     factor &lt;= 18: low resolution image such as 300x300, 640x640     18 &lt; factor &lt;= 21: medium resolution image such as 1024x1024, 1336x960     21 &lt; factor &lt;= 24: high resolution image such as 2048x2048, 2048x4096, 4096x4096     factor &gt; 24: ultra-high resolution image such as 6380x6380, 4096x8192 Args:     height:     width:</p> <p>Returns:</p> Type Description <code>tuple[int, int, int, int]</code> <p>slicing overlap params x_overlap, y_overlap, slice_width, slice_height</p> Source code in <code>sahi/slicing.py</code> <pre><code>def get_auto_slice_params(height: int, width: int) -&gt; tuple[int, int, int, int]:\n    \"\"\"\n    According to Image HxW calculate overlap sliding window and buffer params\n    factor is the power value of 2 closest to the image resolution.\n        factor &lt;= 18: low resolution image such as 300x300, 640x640\n        18 &lt; factor &lt;= 21: medium resolution image such as 1024x1024, 1336x960\n        21 &lt; factor &lt;= 24: high resolution image such as 2048x2048, 2048x4096, 4096x4096\n        factor &gt; 24: ultra-high resolution image such as 6380x6380, 4096x8192\n    Args:\n        height:\n        width:\n\n    Returns:\n        slicing overlap params x_overlap, y_overlap, slice_width, slice_height\n    \"\"\"\n    resolution = height * width\n    factor = calc_resolution_factor(resolution)\n    if factor &lt;= 18:\n        return get_resolution_selector(\"low\", height=height, width=width)\n    elif 18 &lt;= factor &lt; 21:\n        return get_resolution_selector(\"medium\", height=height, width=width)\n    elif 21 &lt;= factor &lt; 24:\n        return get_resolution_selector(\"high\", height=height, width=width)\n    else:\n        return get_resolution_selector(\"ultra-high\", height=height, width=width)\n</code></pre>"},{"location":"api/#sahi.slicing.get_resolution_selector","title":"<code>get_resolution_selector(res, height, width)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>res</code> \u00b6 <code>str</code> <p>resolution of image such as low, medium</p> required <code>height</code> \u00b6 <code>int</code> required <code>width</code> \u00b6 <code>int</code> required <p>Returns:</p> Type Description <code>tuple[int, int, int, int]</code> <p>trigger slicing params function and return overlap params</p> Source code in <code>sahi/slicing.py</code> <pre><code>def get_resolution_selector(res: str, height: int, width: int) -&gt; tuple[int, int, int, int]:\n    \"\"\"\n\n    Args:\n        res: resolution of image such as low, medium\n        height:\n        width:\n\n    Returns:\n        trigger slicing params function and return overlap params\n    \"\"\"\n    orientation = calc_aspect_ratio_orientation(width=width, height=height)\n    x_overlap, y_overlap, slice_width, slice_height = calc_slice_and_overlap_params(\n        resolution=res, height=height, width=width, orientation=orientation\n    )\n\n    return x_overlap, y_overlap, slice_width, slice_height\n</code></pre>"},{"location":"api/#sahi.slicing.get_slice_bboxes","title":"<code>get_slice_bboxes(image_height, image_width, slice_height=None, slice_width=None, auto_slice_resolution=True, overlap_height_ratio=0.2, overlap_width_ratio=0.2)</code>","text":"<p>Generate bounding boxes for slicing an image into crops.</p> <p>The function calculates the coordinates for each slice based on the provided image dimensions, slice size, and overlap ratios. If slice size is not provided and auto_slice_resolution is True, the function will automatically determine appropriate slice parameters.</p> <p>Parameters:</p> Name Type Description Default <code>image_height</code> \u00b6 <code>int</code> <p>Height of the original image.</p> required <code>image_width</code> \u00b6 <code>int</code> <p>Width of the original image.</p> required <code>slice_height</code> \u00b6 <code>int</code> <p>Height of each slice. Default None.</p> <code>None</code> <code>slice_width</code> \u00b6 <code>int</code> <p>Width of each slice. Default None.</p> <code>None</code> <code>overlap_height_ratio</code> \u00b6 <code>float</code> <p>Fractional overlap in height of each slice (e.g. an overlap of 0.2 for a slice of size 100 yields an overlap of 20 pixels). Default 0.2.</p> <code>0.2</code> <code>overlap_width_ratio</code> \u00b6 <code>float</code> <p>Fractional overlap in width of each slice (e.g. an overlap of 0.2 for a slice of size 100 yields an overlap of 20 pixels). Default 0.2.</p> <code>0.2</code> <code>auto_slice_resolution</code> \u00b6 <code>bool</code> <p>if not set slice parameters such as slice_height and slice_width, it enables automatically calculate these parameters from image resolution and orientation.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[list[int]]</code> <p>List[List[int]]: List of 4 corner coordinates for each N slices. [     [slice_0_left, slice_0_top, slice_0_right, slice_0_bottom],     ...     [slice_N_left, slice_N_top, slice_N_right, slice_N_bottom] ]</p> Source code in <code>sahi/slicing.py</code> <pre><code>def get_slice_bboxes(\n    image_height: int,\n    image_width: int,\n    slice_height: int | None = None,\n    slice_width: int | None = None,\n    auto_slice_resolution: bool | None = True,\n    overlap_height_ratio: float | None = 0.2,\n    overlap_width_ratio: float | None = 0.2,\n) -&gt; list[list[int]]:\n    \"\"\"Generate bounding boxes for slicing an image into crops.\n\n    The function calculates the coordinates for each slice based on the provided\n    image dimensions, slice size, and overlap ratios. If slice size is not provided\n    and auto_slice_resolution is True, the function will automatically determine\n    appropriate slice parameters.\n\n    Args:\n        image_height (int): Height of the original image.\n        image_width (int): Width of the original image.\n        slice_height (int, optional): Height of each slice. Default None.\n        slice_width (int, optional): Width of each slice. Default None.\n        overlap_height_ratio (float, optional): Fractional overlap in height of each\n            slice (e.g. an overlap of 0.2 for a slice of size 100 yields an\n            overlap of 20 pixels). Default 0.2.\n        overlap_width_ratio(float, optional): Fractional overlap in width of each\n            slice (e.g. an overlap of 0.2 for a slice of size 100 yields an\n            overlap of 20 pixels). Default 0.2.\n        auto_slice_resolution (bool, optional): if not set slice parameters such as slice_height and slice_width,\n            it enables automatically calculate these parameters from image resolution and orientation.\n\n    Returns:\n        List[List[int]]: List of 4 corner coordinates for each N slices.\n            [\n                [slice_0_left, slice_0_top, slice_0_right, slice_0_bottom],\n                ...\n                [slice_N_left, slice_N_top, slice_N_right, slice_N_bottom]\n            ]\n    \"\"\"\n    slice_bboxes = []\n    y_max = y_min = 0\n\n    if slice_height and slice_width:\n        y_overlap = int(overlap_height_ratio * slice_height)\n        x_overlap = int(overlap_width_ratio * slice_width)\n    elif auto_slice_resolution:\n        x_overlap, y_overlap, slice_width, slice_height = get_auto_slice_params(height=image_height, width=image_width)\n    else:\n        raise ValueError(\"Compute type is not auto and slice width and height are not provided.\")\n\n    while y_max &lt; image_height:\n        x_min = x_max = 0\n        y_max = y_min + slice_height\n        while x_max &lt; image_width:\n            x_max = x_min + slice_width\n            if y_max &gt; image_height or x_max &gt; image_width:\n                xmax = min(image_width, x_max)\n                ymax = min(image_height, y_max)\n                xmin = max(0, xmax - slice_width)\n                ymin = max(0, ymax - slice_height)\n                slice_bboxes.append([xmin, ymin, xmax, ymax])\n            else:\n                slice_bboxes.append([x_min, y_min, x_max, y_max])\n            x_min = x_max - x_overlap\n        y_min = y_max - y_overlap\n    return slice_bboxes\n</code></pre>"},{"location":"api/#sahi.slicing.process_coco_annotations","title":"<code>process_coco_annotations(coco_annotation_list, slice_bbox, min_area_ratio)</code>","text":"<p>Slices and filters given list of CocoAnnotation objects with given 'slice_bbox' and 'min_area_ratio'.</p> <p>Parameters:</p> Name Type Description Default <code>slice_bbox</code> \u00b6 <code>List[int]</code> <p>Generated from <code>get_slice_bboxes</code>. Format for each slice bbox: [x_min, y_min, x_max, y_max].</p> required <code>min_area_ratio</code> \u00b6 <code>float</code> <p>If the cropped annotation area to original annotation ratio is smaller than this value, the annotation is filtered out. Default 0.1.</p> required <p>Returns:</p> Type Description <code>List[CocoAnnotation]</code> <p>Sliced annotations.</p> Source code in <code>sahi/slicing.py</code> <pre><code>def process_coco_annotations(\n    coco_annotation_list: list[CocoAnnotation], slice_bbox: list[int], min_area_ratio\n) -&gt; list[CocoAnnotation]:\n    \"\"\"Slices and filters given list of CocoAnnotation objects with given 'slice_bbox' and 'min_area_ratio'.\n\n    Args:\n        coco_annotation_list (List[CocoAnnotation])\n        slice_bbox (List[int]): Generated from `get_slice_bboxes`.\n            Format for each slice bbox: [x_min, y_min, x_max, y_max].\n        min_area_ratio (float): If the cropped annotation area to original\n            annotation ratio is smaller than this value, the annotation is\n            filtered out. Default 0.1.\n\n    Returns:\n        (List[CocoAnnotation]): Sliced annotations.\n    \"\"\"\n\n    sliced_coco_annotation_list: list[CocoAnnotation] = []\n    for coco_annotation in coco_annotation_list:\n        if annotation_inside_slice(coco_annotation.json, slice_bbox):\n            sliced_coco_annotation = coco_annotation.get_sliced_coco_annotation(slice_bbox)\n            if sliced_coco_annotation.area / coco_annotation.area &gt;= min_area_ratio:\n                sliced_coco_annotation_list.append(sliced_coco_annotation)\n    return sliced_coco_annotation_list\n</code></pre>"},{"location":"api/#sahi.slicing.shift_bboxes","title":"<code>shift_bboxes(bboxes, offset)</code>","text":"<p>Shift bboxes w.r.t offset.</p> <p>Suppo</p> <p>Parameters:</p> Name Type Description Default <code>bboxes</code> \u00b6 <code>(Tensor, ndarray, list)</code> <p>The bboxes need to be translated. Its shape can be (n, 4), which means (x, y, x, y).</p> required <code>offset</code> \u00b6 <code>Sequence[int]</code> <p>The translation offsets with shape of (2, ).</p> required <p>Returns:     Tensor, np.ndarray, list: Shifted bboxes.</p> Source code in <code>sahi/slicing.py</code> <pre><code>def shift_bboxes(bboxes, offset: Sequence[int]):\n    \"\"\"Shift bboxes w.r.t offset.\n\n    Suppo\n\n    Args:\n        bboxes (Tensor, np.ndarray, list): The bboxes need to be translated. Its shape can\n            be (n, 4), which means (x, y, x, y).\n        offset (Sequence[int]): The translation offsets with shape of (2, ).\n    Returns:\n        Tensor, np.ndarray, list: Shifted bboxes.\n    \"\"\"\n    shifted_bboxes = []\n\n    if type(bboxes).__module__ == \"torch\":\n        bboxes_is_torch_tensor = True\n    else:\n        bboxes_is_torch_tensor = False\n\n    for bbox in bboxes:\n        if bboxes_is_torch_tensor or isinstance(bbox, np.ndarray):\n            bbox = bbox.tolist()\n        bbox = BoundingBox(bbox, shift_amount=offset)\n        bbox = bbox.get_shifted_box()\n        shifted_bboxes.append(bbox.to_xyxy())\n\n    if isinstance(bboxes, np.ndarray):\n        return np.stack(shifted_bboxes, axis=0)\n    elif bboxes_is_torch_tensor:\n        return bboxes.new_tensor(shifted_bboxes)\n    else:\n        return shifted_bboxes\n</code></pre>"},{"location":"api/#sahi.slicing.shift_masks","title":"<code>shift_masks(masks, offset, full_shape)</code>","text":"<p>Shift masks to the original image.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> \u00b6 <code>ndarray</code> <p>masks that need to be shifted.</p> required <code>offset</code> \u00b6 <code>Sequence[int]</code> <p>The offset to translate with shape of (2, ).</p> required <code>full_shape</code> \u00b6 <code>Sequence[int]</code> <p>A (height, width) tuple of the huge image's shape.</p> required <p>Returns:     np.ndarray: Shifted masks.</p> Source code in <code>sahi/slicing.py</code> <pre><code>def shift_masks(masks: np.ndarray, offset: Sequence[int], full_shape: Sequence[int]) -&gt; np.ndarray:\n    \"\"\"Shift masks to the original image.\n\n    Args:\n        masks (np.ndarray): masks that need to be shifted.\n        offset (Sequence[int]): The offset to translate with shape of (2, ).\n        full_shape (Sequence[int]): A (height, width) tuple of the huge image's shape.\n    Returns:\n        np.ndarray: Shifted masks.\n    \"\"\"\n    # empty masks\n    if masks is None:\n        return masks\n\n    shifted_masks = []\n    for mask in masks:\n        mask = Mask(segmentation=mask, shift_amount=offset, full_shape=full_shape)\n        mask = mask.get_shifted_mask()\n        shifted_masks.append(mask.bool_mask)\n\n    return np.stack(shifted_masks, axis=0)\n</code></pre>"},{"location":"api/#sahi.slicing.slice_coco","title":"<code>slice_coco(coco_annotation_file_path, image_dir, output_coco_annotation_file_name, output_dir=None, ignore_negative_samples=False, slice_height=512, slice_width=512, overlap_height_ratio=0.2, overlap_width_ratio=0.2, min_area_ratio=0.1, out_ext=None, verbose=False, exif_fix=True)</code>","text":"<p>Slice large images given in a directory, into smaller windows. If output_dir is given, export sliced images and coco file.</p> <p>Parameters:</p> Name Type Description Default <code>coco_annotation_file_path</code> \u00b6 <code>str</code> <p>Location of the coco annotation file</p> required <code>image_dir</code> \u00b6 <code>str</code> <p>Base directory for the images</p> required <code>output_coco_annotation_file_name</code> \u00b6 <code>str</code> <p>File name of the exported coco dataset json.</p> required <code>output_dir</code> \u00b6 <code>str</code> <p>Output directory</p> <code>None</code> <code>ignore_negative_samples</code> \u00b6 <code>bool</code> <p>If True, images without annotations are ignored. Defaults to False.</p> <code>False</code> <code>slice_height</code> \u00b6 <code>int</code> <p>Height of each slice. Default 512.</p> <code>512</code> <code>slice_width</code> \u00b6 <code>int</code> <p>Width of each slice. Default 512.</p> <code>512</code> <code>overlap_height_ratio</code> \u00b6 <code>float</code> <p>Fractional overlap in height of each slice (e.g. an overlap of 0.2 for a slice of size 100 yields an overlap of 20 pixels). Default 0.2.</p> <code>0.2</code> <code>overlap_width_ratio</code> \u00b6 <code>float</code> <p>Fractional overlap in width of each slice (e.g. an overlap of 0.2 for a slice of size 100 yields an overlap of 20 pixels). Default 0.2.</p> <code>0.2</code> <code>min_area_ratio</code> \u00b6 <code>float</code> <p>If the cropped annotation area to original annotation ratio is smaller than this value, the annotation is filtered out. Default 0.1.</p> <code>0.1</code> <code>out_ext</code> \u00b6 <code>str</code> <p>Extension of saved images. Default is the original suffix.</p> <code>None</code> <code>verbose</code> \u00b6 <code>bool</code> <p>Switch to print relevant values to screen.</p> <code>False</code> <code>exif_fix</code> \u00b6 <code>bool</code> <p>Whether to apply an EXIF fix to the image.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>coco_dict</code> <code>list[dict | str]</code> <p>dict COCO dict for sliced images and annotations</p> <code>save_path</code> <code>list[dict | str]</code> <p>str Path to the saved coco file</p> Source code in <code>sahi/slicing.py</code> <pre><code>def slice_coco(\n    coco_annotation_file_path: str,\n    image_dir: str,\n    output_coco_annotation_file_name: str,\n    output_dir: str | None = None,\n    ignore_negative_samples: bool | None = False,\n    slice_height: int | None = 512,\n    slice_width: int | None = 512,\n    overlap_height_ratio: float | None = 0.2,\n    overlap_width_ratio: float | None = 0.2,\n    min_area_ratio: float | None = 0.1,\n    out_ext: str | None = None,\n    verbose: bool | None = False,\n    exif_fix: bool = True,\n) -&gt; list[dict | str]:\n    \"\"\"Slice large images given in a directory, into smaller windows. If output_dir is given, export sliced images and\n    coco file.\n\n    Args:\n        coco_annotation_file_path (str): Location of the coco annotation file\n        image_dir (str): Base directory for the images\n        output_coco_annotation_file_name (str): File name of the exported coco\n            dataset json.\n        output_dir (str, optional): Output directory\n        ignore_negative_samples (bool, optional): If True, images without annotations\n            are ignored. Defaults to False.\n        slice_height (int, optional): Height of each slice. Default 512.\n        slice_width (int, optional): Width of each slice. Default 512.\n        overlap_height_ratio (float, optional): Fractional overlap in height of each\n            slice (e.g. an overlap of 0.2 for a slice of size 100 yields an\n            overlap of 20 pixels). Default 0.2.\n        overlap_width_ratio (float, optional): Fractional overlap in width of each\n            slice (e.g. an overlap of 0.2 for a slice of size 100 yields an\n            overlap of 20 pixels). Default 0.2.\n        min_area_ratio (float): If the cropped annotation area to original annotation\n            ratio is smaller than this value, the annotation is filtered out. Default 0.1.\n        out_ext (str, optional): Extension of saved images. Default is the\n            original suffix.\n        verbose (bool, optional): Switch to print relevant values to screen.\n        exif_fix (bool, optional): Whether to apply an EXIF fix to the image.\n\n    Returns:\n        coco_dict: dict\n            COCO dict for sliced images and annotations\n        save_path: str\n            Path to the saved coco file\n    \"\"\"\n\n    # read coco file\n    coco_dict: dict = load_json(coco_annotation_file_path)\n    # create image_id_to_annotation_list mapping\n    coco = Coco.from_coco_dict_or_path(coco_dict)\n    # init sliced coco_utils.CocoImage list\n    sliced_coco_images: list = []\n\n    # iterate over images and slice\n    for idx, coco_image in enumerate(tqdm(coco.images)):\n        # get image path\n        image_path: str = os.path.join(image_dir, coco_image.file_name)\n        # get annotation json list corresponding to selected coco image\n        # slice image\n        try:\n            slice_image_result = slice_image(\n                image=image_path,\n                coco_annotation_list=coco_image.annotations,\n                output_file_name=f\"{Path(coco_image.file_name).stem}_{idx}\",\n                output_dir=output_dir,\n                slice_height=slice_height,\n                slice_width=slice_width,\n                overlap_height_ratio=overlap_height_ratio,\n                overlap_width_ratio=overlap_width_ratio,\n                min_area_ratio=min_area_ratio,\n                out_ext=out_ext,\n                verbose=verbose,\n                exif_fix=exif_fix,\n            )\n            # append slice outputs\n            sliced_coco_images.extend(slice_image_result.coco_images)\n        except TopologicalError:\n            logger.warning(f\"Invalid annotation found, skipping this image: {image_path}\")\n\n    # create and save coco dict\n    coco_dict = create_coco_dict(\n        sliced_coco_images, coco_dict[\"categories\"], ignore_negative_samples=ignore_negative_samples\n    )\n    save_path = \"\"\n    if output_coco_annotation_file_name and output_dir:\n        save_path = Path(output_dir) / (output_coco_annotation_file_name + \"_coco.json\")\n        save_json(coco_dict, save_path)\n\n    return coco_dict, save_path\n</code></pre>"},{"location":"api/#sahi.slicing.slice_image","title":"<code>slice_image(image, coco_annotation_list=None, output_file_name=None, output_dir=None, slice_height=None, slice_width=None, overlap_height_ratio=0.2, overlap_width_ratio=0.2, auto_slice_resolution=True, min_area_ratio=0.1, out_ext=None, verbose=False, exif_fix=True)</code>","text":"<p>Slice a large image into smaller windows. If output_file_name and output_dir is given, export sliced images.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>str or Image</code> <p>File path of image or Pillow Image to be sliced.</p> required <code>coco_annotation_list</code> \u00b6 <code>List[CocoAnnotation]</code> <p>List of CocoAnnotation objects.</p> <code>None</code> <code>output_file_name</code> \u00b6 <code>str</code> <p>Root name of output files (coordinates will be appended to this)</p> <code>None</code> <code>output_dir</code> \u00b6 <code>str</code> <p>Output directory</p> <code>None</code> <code>slice_height</code> \u00b6 <code>int</code> <p>Height of each slice. Default None.</p> <code>None</code> <code>slice_width</code> \u00b6 <code>int</code> <p>Width of each slice. Default None.</p> <code>None</code> <code>overlap_height_ratio</code> \u00b6 <code>float</code> <p>Fractional overlap in height of each slice (e.g. an overlap of 0.2 for a slice of size 100 yields an overlap of 20 pixels). Default 0.2.</p> <code>0.2</code> <code>overlap_width_ratio</code> \u00b6 <code>float</code> <p>Fractional overlap in width of each slice (e.g. an overlap of 0.2 for a slice of size 100 yields an overlap of 20 pixels). Default 0.2.</p> <code>0.2</code> <code>auto_slice_resolution</code> \u00b6 <code>bool</code> <p>if not set slice parameters such as slice_height and slice_width, it enables automatically calculate these params from image resolution and orientation.</p> <code>True</code> <code>min_area_ratio</code> \u00b6 <code>float</code> <p>If the cropped annotation area to original annotation ratio is smaller than this value, the annotation is filtered out. Default 0.1.</p> <code>0.1</code> <code>out_ext</code> \u00b6 <code>str</code> <p>Extension of saved images. Default is the original suffix for lossless image formats and png for lossy formats ('.jpg','.jpeg').</p> <code>None</code> <code>verbose</code> \u00b6 <code>bool</code> <p>Switch to print relevant values to screen. Default 'False'.</p> <code>False</code> <code>exif_fix</code> \u00b6 <code>bool</code> <p>Whether to apply an EXIF fix to the image.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>sliced_image_result</code> <code>SliceImageResult</code> <p>SliceImageResult:                     sliced_image_list: list of SlicedImage                     image_dir: str                         Directory of the sliced image exports.                     original_image_size: list of int                         Size of the unsliced original image in [height, width]</p> Source code in <code>sahi/slicing.py</code> <pre><code>def slice_image(\n    image: str | Image.Image,\n    coco_annotation_list: list[CocoAnnotation] | None = None,\n    output_file_name: str | None = None,\n    output_dir: str | None = None,\n    slice_height: int | None = None,\n    slice_width: int | None = None,\n    overlap_height_ratio: float | None = 0.2,\n    overlap_width_ratio: float | None = 0.2,\n    auto_slice_resolution: bool | None = True,\n    min_area_ratio: float | None = 0.1,\n    out_ext: str | None = None,\n    verbose: bool | None = False,\n    exif_fix: bool = True,\n) -&gt; SliceImageResult:\n    \"\"\"Slice a large image into smaller windows. If output_file_name and output_dir is given, export sliced images.\n\n    Args:\n        image (str or PIL.Image): File path of image or Pillow Image to be sliced.\n        coco_annotation_list (List[CocoAnnotation], optional): List of CocoAnnotation objects.\n        output_file_name (str, optional): Root name of output files (coordinates will\n            be appended to this)\n        output_dir (str, optional): Output directory\n        slice_height (int, optional): Height of each slice. Default None.\n        slice_width (int, optional): Width of each slice. Default None.\n        overlap_height_ratio (float, optional): Fractional overlap in height of each\n            slice (e.g. an overlap of 0.2 for a slice of size 100 yields an\n            overlap of 20 pixels). Default 0.2.\n        overlap_width_ratio (float, optional): Fractional overlap in width of each\n            slice (e.g. an overlap of 0.2 for a slice of size 100 yields an\n            overlap of 20 pixels). Default 0.2.\n        auto_slice_resolution (bool, optional): if not set slice parameters such as slice_height and slice_width,\n            it enables automatically calculate these params from image resolution and orientation.\n        min_area_ratio (float, optional): If the cropped annotation area to original annotation\n            ratio is smaller than this value, the annotation is filtered out. Default 0.1.\n        out_ext (str, optional): Extension of saved images. Default is the\n            original suffix for lossless image formats and png for lossy formats ('.jpg','.jpeg').\n        verbose (bool, optional): Switch to print relevant values to screen.\n            Default 'False'.\n        exif_fix (bool): Whether to apply an EXIF fix to the image.\n\n    Returns:\n        sliced_image_result: SliceImageResult:\n                                sliced_image_list: list of SlicedImage\n                                image_dir: str\n                                    Directory of the sliced image exports.\n                                original_image_size: list of int\n                                    Size of the unsliced original image in [height, width]\n    \"\"\"\n\n    # define verboseprint\n    verboselog = logger.info if verbose else lambda *a, **k: None\n\n    def _export_single_slice(image: np.ndarray, output_dir: str, slice_file_name: str):\n        image_pil = read_image_as_pil(image, exif_fix=exif_fix)\n        slice_file_path = str(Path(output_dir) / slice_file_name)\n        # export sliced image\n        image_pil.save(slice_file_path)\n        image_pil.close()  # to fix https://github.com/obss/sahi/issues/565\n        verboselog(\"sliced image path: \" + slice_file_path)\n\n    # create outdir if not present\n    if output_dir is not None:\n        Path(output_dir).mkdir(parents=True, exist_ok=True)\n\n    # read image\n    image_pil = read_image_as_pil(image, exif_fix=exif_fix)\n    verboselog(\"image.shape: \" + str(image_pil.size))\n\n    image_width, image_height = image_pil.size\n    if not (image_width != 0 and image_height != 0):\n        raise RuntimeError(f\"invalid image size: {image_pil.size} for 'slice_image'.\")\n    slice_bboxes = get_slice_bboxes(\n        image_height=image_height,\n        image_width=image_width,\n        auto_slice_resolution=auto_slice_resolution,\n        slice_height=slice_height,\n        slice_width=slice_width,\n        overlap_height_ratio=overlap_height_ratio,\n        overlap_width_ratio=overlap_width_ratio,\n    )\n\n    n_ims = 0\n\n    # init images and annotations lists\n    sliced_image_result = SliceImageResult(original_image_size=[image_height, image_width], image_dir=output_dir)\n\n    image_pil_arr = np.asarray(image_pil)\n    # iterate over slices\n    for slice_bbox in slice_bboxes:\n        n_ims += 1\n\n        # extract image\n        tlx = slice_bbox[0]\n        tly = slice_bbox[1]\n        brx = slice_bbox[2]\n        bry = slice_bbox[3]\n        image_pil_slice = image_pil_arr[tly:bry, tlx:brx]\n\n        # set image file suffixes\n        slice_suffixes = \"_\".join(map(str, slice_bbox))\n        if out_ext:\n            suffix = out_ext\n        elif hasattr(image_pil, \"filename\"):\n            suffix = Path(getattr(image_pil, \"filename\")).suffix\n            if suffix in IMAGE_EXTENSIONS_LOSSY:\n                suffix = \".png\"\n            elif suffix in IMAGE_EXTENSIONS_LOSSLESS:\n                suffix = Path(image_pil.filename).suffix\n        else:\n            suffix = \".png\"\n\n        # set image file name and path\n        slice_file_name = f\"{output_file_name}_{slice_suffixes}{suffix}\"\n\n        # create coco image\n        slice_width = slice_bbox[2] - slice_bbox[0]\n        slice_height = slice_bbox[3] - slice_bbox[1]\n        coco_image = CocoImage(file_name=slice_file_name, height=slice_height, width=slice_width)\n\n        # append coco annotations (if present) to coco image\n        if coco_annotation_list is not None:\n            for sliced_coco_annotation in process_coco_annotations(coco_annotation_list, slice_bbox, min_area_ratio):\n                coco_image.add_annotation(sliced_coco_annotation)\n\n        # create sliced image and append to sliced_image_result\n        sliced_image = SlicedImage(\n            image=image_pil_slice, coco_image=coco_image, starting_pixel=[slice_bbox[0], slice_bbox[1]]\n        )\n        sliced_image_result.add_sliced_image(sliced_image)\n\n    # export slices if output directory is provided\n    if output_file_name and output_dir:\n        conc_exec = concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS)\n        conc_exec.map(\n            _export_single_slice,\n            sliced_image_result.images,\n            [output_dir] * len(sliced_image_result),\n            sliced_image_result.filenames,\n        )\n\n    verboselog(\n        \"Num slices: \" + str(n_ims) + \" slice_height: \" + str(slice_height) + \" slice_width: \" + str(slice_width)\n    )\n\n    return sliced_image_result\n</code></pre>"},{"location":"api/#sahi.utils","title":"<code>utils</code>","text":""},{"location":"api/#sahi.utils-modules","title":"Modules","text":""},{"location":"api/#sahi.utils.coco","title":"<code>coco</code>","text":"Classes\u00b6 <code>Coco</code> \u00b6 Source code in <code>sahi/utils/coco.py</code> <pre><code>class Coco:\n    def __init__(\n        self,\n        name: str | None = None,\n        image_dir: str | None = None,\n        remapping_dict: dict[int, int] | None = None,\n        ignore_negative_samples: bool = False,\n        clip_bboxes_to_img_dims: bool = False,\n        image_id_setting: Literal[\"auto\", \"manual\"] = \"auto\",\n    ):\n        \"\"\"Creates Coco object.\n\n        Args:\n            name: str\n                Name of the Coco dataset, it determines exported json name.\n            image_dir: str\n                Base file directory that contains dataset images. Required for dataset merging.\n            remapping_dict: dict\n                {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1\n            ignore_negative_samples: bool\n                If True ignores images without annotations in all operations.\n            image_id_setting: str\n                how to assign image ids while exporting can be\n                auto -&gt; will assign id from scratch (&lt;CocoImage&gt;.id will be ignored)\n                manual -&gt; you will need to provide image ids in &lt;CocoImage&gt; instances (&lt;CocoImage&gt;.id can not be None)\n        \"\"\"\n        if image_id_setting not in [\"auto\", \"manual\"]:\n            raise ValueError(\"image_id_setting must be either 'auto' or 'manual'\")\n        self.name: str | None = name\n        self.image_dir: str | None = image_dir\n        self.remapping_dict: dict[int, int] | None = remapping_dict\n        self.ignore_negative_samples = ignore_negative_samples\n        self.categories: list[CocoCategory] = []\n        self.images = []\n        self._stats = None\n        self.clip_bboxes_to_img_dims = clip_bboxes_to_img_dims\n        self.image_id_setting = image_id_setting\n\n    def add_categories_from_coco_category_list(self, coco_category_list):\n        \"\"\"Creates CocoCategory object using coco category list.\n\n        Args:\n            coco_category_list: List[Dict]\n                [\n                    {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},\n                    {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"}\n                ]\n        \"\"\"\n\n        for coco_category in coco_category_list:\n            if self.remapping_dict is not None:\n                for source_id in self.remapping_dict.keys():\n                    if coco_category[\"id\"] == source_id:\n                        target_id = self.remapping_dict[source_id]\n                        coco_category[\"id\"] = target_id\n\n            self.add_category(CocoCategory.from_coco_category(coco_category))\n\n    def add_category(self, category):\n        \"\"\"Adds category to this Coco instance.\n\n        Args:\n            category: CocoCategory\n        \"\"\"\n\n        # assert type(category) == CocoCategory, \"category must be a CocoCategory instance\"\n        if not isinstance(category, CocoCategory):\n            raise TypeError(\"category must be a CocoCategory instance\")\n        self.categories.append(category)\n\n    def add_image(self, image):\n        \"\"\"Adds image to this Coco instance.\n\n        Args:\n            image: CocoImage\n        \"\"\"\n\n        if self.image_id_setting == \"manual\" and image.id is None:\n            raise ValueError(\"image id should be manually set for image_id_setting='manual'\")\n        self.images.append(image)\n\n    def update_categories(self, desired_name2id: dict[str, int], update_image_filenames: bool = False):\n        \"\"\"Rearranges category mapping of given COCO object based on given desired_name2id. Can also be used to filter\n        some of the categories.\n\n        Args:\n            desired_name2id: dict\n                {\"big_vehicle\": 1, \"car\": 2, \"human\": 3}\n            update_image_filenames: bool\n                If True, updates coco image file_names with absolute file paths.\n        \"\"\"\n        # init vars\n        currentid2desiredid_mapping: dict[int, int | None] = {}\n        updated_coco = Coco(\n            name=self.name,\n            image_dir=self.image_dir,\n            remapping_dict=self.remapping_dict,\n            ignore_negative_samples=self.ignore_negative_samples,\n        )\n        # create category id mapping (currentid2desiredid_mapping)\n        for coco_category in self.categories:\n            current_category_id = coco_category.id\n            current_category_name = coco_category.name\n            if not current_category_name:\n                logger.warning(\"no category name provided to update categories\")\n                continue\n            if current_category_name in desired_name2id.keys():\n                currentid2desiredid_mapping[current_category_id] = desired_name2id[current_category_name]\n            else:\n                # ignore categories that are not included in desired_name2id\n                currentid2desiredid_mapping[current_category_id] = None\n\n        # add updated categories\n        for name in desired_name2id.keys():\n            updated_coco_category = CocoCategory(id=desired_name2id[name], name=name, supercategory=name)\n            updated_coco.add_category(updated_coco_category)\n\n        # add updated images &amp; annotations\n        for coco_image in copy.deepcopy(self.images):\n            updated_coco_image = CocoImage.from_coco_image_dict(coco_image.json)\n            # update filename to abspath\n            file_name_is_abspath = True if os.path.abspath(coco_image.file_name) == coco_image.file_name else False\n            if update_image_filenames and not file_name_is_abspath:\n                if not self.image_dir:\n                    logger.error(\"image directory not set\")\n                else:\n                    updated_coco_image.file_name = str(Path(os.path.abspath(self.image_dir)) / coco_image.file_name)\n            # update annotations\n            for coco_annotation in coco_image.annotations:\n                current_category_id = coco_annotation.category_id\n                desired_category_id = currentid2desiredid_mapping[current_category_id]\n                # append annotations with category id present in desired_name2id\n                if desired_category_id is not None:\n                    # update cetegory id\n                    coco_annotation.category_id = desired_category_id\n                    # append updated annotation to target coco dict\n                    updated_coco_image.add_annotation(coco_annotation)\n            updated_coco.add_image(updated_coco_image)\n\n        # overwrite instance\n        self.__dict__ = updated_coco.__dict__\n\n    def merge(self, coco, desired_name2id=None, verbose=1):\n        \"\"\"Combines the images/annotations/categories of given coco object with current one.\n\n        Args:\n            coco : sahi.utils.coco.Coco instance\n                A COCO dataset object\n            desired_name2id : dict\n                {\"human\": 1, \"car\": 2, \"big_vehicle\": 3}\n            verbose: bool\n                If True, merging info is printed\n        \"\"\"\n        if self.image_dir is None or coco.image_dir is None:\n            raise ValueError(\"image_dir should be provided for merging.\")\n        if verbose:\n            if not desired_name2id:\n                print(\"'desired_name2id' is not specified, combining all categories.\")\n\n        # create desired_name2id by combining all categories, if desired_name2id is not specified\n        coco1 = self\n        coco2 = coco\n        category_ind = 0\n        if desired_name2id is None:\n            desired_name2id = {}\n            for coco in [coco1, coco2]:\n                temp_categories = copy.deepcopy(coco.json_categories)\n                for temp_category in temp_categories:\n                    if temp_category[\"name\"] not in desired_name2id:\n                        desired_name2id[temp_category[\"name\"]] = category_ind\n                        category_ind += 1\n                    else:\n                        continue\n\n        # update categories and image paths\n        for coco in [coco1, coco2]:\n            coco.update_categories(desired_name2id=desired_name2id, update_image_filenames=True)\n\n        # combine images and categories\n        coco1.images.extend(coco2.images)\n        self.images: list[CocoImage] = coco1.images\n        self.categories = coco1.categories\n\n        # print categories\n        if verbose:\n            print(\n                \"Categories are formed as:\\n\",\n                self.json_categories,\n            )\n\n    @classmethod\n    def from_coco_dict_or_path(\n        cls,\n        coco_dict_or_path: dict | str,\n        image_dir: str | None = None,\n        remapping_dict: dict | None = None,\n        ignore_negative_samples: bool = False,\n        clip_bboxes_to_img_dims: bool = False,\n        use_threads: bool = False,\n        num_threads: int = 10,\n    ):\n        \"\"\"Creates coco object from COCO formatted dict or COCO dataset file path.\n\n        Args:\n            coco_dict_or_path: dict/str or List[dict/str]\n                COCO formatted dict or COCO dataset file path\n                List of COCO formatted dict or COCO dataset file path\n            image_dir: str\n                Base file directory that contains dataset images. Required for merging and yolov5 conversion.\n            remapping_dict: dict\n                {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1\n            ignore_negative_samples: bool\n                If True ignores images without annotations in all operations.\n            clip_bboxes_to_img_dims: bool = False\n                Limits bounding boxes to image dimensions.\n            use_threads: bool = False\n                Use threads when processing the json image list, defaults to False\n            num_threads: int = 10\n                Slice the image list to given number of chunks, defaults to 10\n\n        Properties:\n            images: list of CocoImage\n            category_mapping: dict\n        \"\"\"\n        # init coco object\n        coco = cls(\n            image_dir=image_dir,\n            remapping_dict=remapping_dict,\n            ignore_negative_samples=ignore_negative_samples,\n            clip_bboxes_to_img_dims=clip_bboxes_to_img_dims,\n        )\n\n        if type(coco_dict_or_path) not in [str, dict]:\n            raise TypeError(\"coco_dict_or_path should be a dict or str\")\n\n        # load coco dict if path is given\n        if isinstance(coco_dict_or_path, str):\n            coco_dict = load_json(coco_dict_or_path)\n        else:\n            coco_dict = coco_dict_or_path\n\n        dict_size = len(coco_dict[\"images\"])\n\n        # arrange image id to annotation id mapping\n        coco.add_categories_from_coco_category_list(coco_dict[\"categories\"])\n        image_id_to_annotation_list = get_imageid2annotationlist_mapping(coco_dict)\n        category_mapping = coco.category_mapping\n\n        # https://github.com/obss/sahi/issues/98\n        image_id_set: set = set()\n\n        lock = Lock()\n\n        def fill_image_id_set(start, finish, image_list, _image_id_set, _image_id_to_annotation_list, _coco, lock):\n            for coco_image_dict in tqdm(\n                image_list[start:finish], f\"Loading coco annotations between {start} and {finish}\"\n            ):\n                coco_image = CocoImage.from_coco_image_dict(coco_image_dict)\n                image_id = coco_image_dict[\"id\"]\n                # https://github.com/obss/sahi/issues/98\n                if image_id in _image_id_set:\n                    print(f\"duplicate image_id: {image_id}, will be ignored.\")\n                    continue\n                else:\n                    lock.acquire()\n                    _image_id_set.add(image_id)\n                    lock.release()\n\n                # select annotations of the image\n                annotation_list = _image_id_to_annotation_list[image_id]\n                for coco_annotation_dict in annotation_list:\n                    # apply category remapping if remapping_dict is provided\n                    if _coco.remapping_dict is not None:\n                        # apply category remapping (id:id)\n                        category_id = _coco.remapping_dict[coco_annotation_dict[\"category_id\"]]\n                        # update category id\n                        coco_annotation_dict[\"category_id\"] = category_id\n                    else:\n                        category_id = coco_annotation_dict[\"category_id\"]\n                    # get category name (id:name)\n                    category_name = category_mapping[category_id]\n                    coco_annotation = CocoAnnotation.from_coco_annotation_dict(\n                        category_name=category_name, annotation_dict=coco_annotation_dict\n                    )\n                    coco_image.add_annotation(coco_annotation)\n                _coco.add_image(coco_image)\n\n        chunk_size = dict_size / num_threads\n\n        if use_threads is True:\n            for i in range(num_threads):\n                start = i * chunk_size\n                finish = start + chunk_size\n                if finish &gt; dict_size:\n                    finish = dict_size\n                t = Thread(\n                    target=fill_image_id_set,\n                    args=(start, finish, coco_dict[\"images\"], image_id_set, image_id_to_annotation_list, coco, lock),\n                )\n                t.start()\n\n            main_thread = threading.currentThread()\n            for t in threading.enumerate():\n                if t is not main_thread:\n                    t.join()\n\n        else:\n            for coco_image_dict in tqdm(coco_dict[\"images\"], \"Loading coco annotations\"):\n                coco_image = CocoImage.from_coco_image_dict(coco_image_dict)\n                image_id = coco_image_dict[\"id\"]\n                # https://github.com/obss/sahi/issues/98\n                if image_id in image_id_set:\n                    print(f\"duplicate image_id: {image_id}, will be ignored.\")\n                    continue\n                else:\n                    image_id_set.add(image_id)\n                # select annotations of the image\n                annotation_list = image_id_to_annotation_list[image_id]\n                # TODO: coco_annotation_dict is of type CocoAnnotation according to how image_id_to_annotation_list\n                # was created. Either image_id_to_annotation_list is not defined correctly or the following\n                # loop is wrong as it expects a dict.\n                for coco_annotation_dict in annotation_list:\n                    # apply category remapping if remapping_dict is provided\n                    if coco.remapping_dict is not None:\n                        # apply category remapping (id:id)\n                        category_id = coco.remapping_dict[coco_annotation_dict[\"category_id\"]]\n                        # update category id\n                        coco_annotation_dict[\"category_id\"] = category_id\n                    else:\n                        category_id = coco_annotation_dict[\"category_id\"]\n                    # get category name (id:name)\n                    category_name = category_mapping[category_id]\n                    coco_annotation = CocoAnnotation.from_coco_annotation_dict(\n                        category_name=category_name, annotation_dict=coco_annotation_dict\n                    )\n                    coco_image.add_annotation(coco_annotation)\n                coco.add_image(coco_image)\n\n        if clip_bboxes_to_img_dims:\n            coco = coco.get_coco_with_clipped_bboxes()\n        return coco\n\n    @property\n    def json_categories(self):\n        categories = []\n        for category in self.categories:\n            categories.append(category.json)\n        return categories\n\n    @property\n    def category_mapping(self):\n        category_mapping = {}\n        for category in self.categories:\n            category_mapping[category.id] = category.name\n        return category_mapping\n\n    @property\n    def json(self):\n        return create_coco_dict(\n            images=self.images,\n            categories=self.json_categories,\n            ignore_negative_samples=self.ignore_negative_samples,\n            image_id_setting=self.image_id_setting,\n        )\n\n    @property\n    def prediction_array(self):\n        return create_coco_prediction_array(\n            images=self.images,\n            ignore_negative_samples=self.ignore_negative_samples,\n            image_id_setting=self.image_id_setting,\n        )\n\n    @property\n    def stats(self):\n        if not self._stats:\n            self.calculate_stats()\n        return self._stats\n\n    def calculate_stats(self):\n        \"\"\"Iterates over all annotations and calculates total number of.\"\"\"\n        # init all stats\n        num_annotations = 0\n        num_images = len(self.images)\n        num_negative_images = 0\n        num_categories = len(self.json_categories)\n        category_name_to_zero = {category[\"name\"]: 0 for category in self.json_categories}\n        category_name_to_inf = {category[\"name\"]: float(\"inf\") for category in self.json_categories}\n        num_images_per_category = copy.deepcopy(category_name_to_zero)\n        num_annotations_per_category = copy.deepcopy(category_name_to_zero)\n        min_annotation_area_per_category = copy.deepcopy(category_name_to_inf)\n        max_annotation_area_per_category = copy.deepcopy(category_name_to_zero)\n        min_num_annotations_in_image = float(\"inf\")\n        max_num_annotations_in_image = 0\n        total_annotation_area = 0\n        min_annotation_area = 1e10\n        max_annotation_area = 0\n        for image in self.images:\n            image_contains_category = {}\n            for annotation in image.annotations:\n                annotation_area = annotation.area\n                total_annotation_area += annotation_area\n                num_annotations_per_category[annotation.category_name] += 1\n                image_contains_category[annotation.category_name] = 1\n                # update min&amp;max annotation area\n                if annotation_area &gt; max_annotation_area:\n                    max_annotation_area = annotation_area\n                if annotation_area &lt; min_annotation_area:\n                    min_annotation_area = annotation_area\n                if annotation_area &gt; max_annotation_area_per_category[annotation.category_name]:\n                    max_annotation_area_per_category[annotation.category_name] = annotation_area\n                if annotation_area &lt; min_annotation_area_per_category[annotation.category_name]:\n                    min_annotation_area_per_category[annotation.category_name] = annotation_area\n            # update num_negative_images\n            if len(image.annotations) == 0:\n                num_negative_images += 1\n            # update num_annotations\n            num_annotations += len(image.annotations)\n            # update num_images_per_category\n            num_images_per_category = dict(Counter(num_images_per_category) + Counter(image_contains_category))\n            # update min&amp;max_num_annotations_in_image\n            num_annotations_in_image = len(image.annotations)\n            if num_annotations_in_image &gt; max_num_annotations_in_image:\n                max_num_annotations_in_image = num_annotations_in_image\n            if num_annotations_in_image &lt; min_num_annotations_in_image:\n                min_num_annotations_in_image = num_annotations_in_image\n        if (num_images - num_negative_images) &gt; 0:\n            avg_num_annotations_in_image = num_annotations / (num_images - num_negative_images)\n            avg_annotation_area = total_annotation_area / num_annotations\n        else:\n            avg_num_annotations_in_image = 0\n            avg_annotation_area = 0\n\n        self._stats = {\n            \"num_images\": num_images,\n            \"num_annotations\": num_annotations,\n            \"num_categories\": num_categories,\n            \"num_negative_images\": num_negative_images,\n            \"num_images_per_category\": num_images_per_category,\n            \"num_annotations_per_category\": num_annotations_per_category,\n            \"min_num_annotations_in_image\": min_num_annotations_in_image,\n            \"max_num_annotations_in_image\": max_num_annotations_in_image,\n            \"avg_num_annotations_in_image\": avg_num_annotations_in_image,\n            \"min_annotation_area\": min_annotation_area,\n            \"max_annotation_area\": max_annotation_area,\n            \"avg_annotation_area\": avg_annotation_area,\n            \"min_annotation_area_per_category\": min_annotation_area_per_category,\n            \"max_annotation_area_per_category\": max_annotation_area_per_category,\n        }\n\n    def split_coco_as_train_val(self, train_split_rate=0.9, numpy_seed=0):\n        \"\"\"Split images into train-val and returns them as sahi.utils.coco.Coco objects.\n\n        Args:\n            train_split_rate: float\n            numpy_seed: int\n                random seed. Actually, this doesn't use numpy, but the random package\n                from the standard library, but it is called numpy for compatibility.\n\n        Returns:\n            result : dict\n                {\n                    \"train_coco\": \"\",\n                    \"val_coco\": \"\",\n                }\n        \"\"\"\n        # divide images\n        num_images = len(self.images)\n        shuffled_images = copy.deepcopy(self.images)\n        random.seed(numpy_seed)\n        random.shuffle(shuffled_images)\n        num_train = int(num_images * train_split_rate)\n        train_images = shuffled_images[:num_train]\n        val_images = shuffled_images[num_train:]\n\n        # form train val coco objects\n        train_coco = Coco(\n            name=self.name if self.name else \"split\" + \"_train\",\n            image_dir=self.image_dir,\n        )\n        train_coco.images = train_images\n        train_coco.categories = self.categories\n\n        val_coco = Coco(name=self.name if self.name else \"split\" + \"_val\", image_dir=self.image_dir)\n        val_coco.images = val_images\n        val_coco.categories = self.categories\n\n        # return result\n        return {\n            \"train_coco\": train_coco,\n            \"val_coco\": val_coco,\n        }\n\n    def export_as_yolov5(\n        self,\n        output_dir: str | Path,\n        train_split_rate: float = 1.0,\n        numpy_seed: int = 0,\n        mp: bool = False,\n        disable_symlink: bool = False,\n    ):\n        \"\"\"Deprecated.\n\n        Please use export_as_yolo instead. Calls export_as_yolo with the same arguments.\n        \"\"\"\n        warnings.warn(\n            \"export_as_yolov5 is deprecated. Please use export_as_yolo instead.\",\n            DeprecationWarning,\n        )\n        self.export_as_yolo(\n            output_dir=output_dir,\n            train_split_rate=train_split_rate,\n            numpy_seed=numpy_seed,\n            mp=mp,\n            disable_symlink=disable_symlink,\n        )\n\n    def export_as_yolo(\n        self,\n        output_dir: str | Path,\n        train_split_rate: float = 1.0,\n        numpy_seed: int = 0,\n        mp: bool = False,\n        disable_symlink: bool = False,\n    ):\n        \"\"\"Exports current COCO dataset in ultralytics/yolo format. Creates train val folders with image symlinks and\n        txt files and a data yaml file.\n\n        Args:\n            output_dir: str\n                Export directory.\n            train_split_rate: float\n                If given 1, will be exported as train split.\n                If given 0, will be exported as val split.\n                If in between 0-1, both train/val splits will be calculated and exported.\n            numpy_seed: int\n                To fix the numpy seed.\n            mp: bool\n                If True, multiprocess mode is on.\n                Should be called in 'if __name__ == __main__:' block.\n            disable_symlink: bool\n                If True, symlinks will not be created. Instead, images will be copied.\n        \"\"\"\n        try:\n            import yaml\n        except ImportError:\n            raise ImportError('Please run \"pip install -U pyyaml\" to install yaml first for yolo formatted exporting.')\n\n        # set split_mode\n        if 0 &lt; train_split_rate and train_split_rate &lt; 1:\n            split_mode = \"TRAINVAL\"\n        elif train_split_rate == 0:\n            split_mode = \"VAL\"\n        elif train_split_rate == 1:\n            split_mode = \"TRAIN\"\n        else:\n            raise ValueError(\"train_split_rate cannot be &lt;0 or &gt;1\")\n\n        # split dataset\n        if split_mode == \"TRAINVAL\":\n            result = self.split_coco_as_train_val(\n                train_split_rate=train_split_rate,\n                numpy_seed=numpy_seed,\n            )\n            train_coco = result[\"train_coco\"]\n            val_coco = result[\"val_coco\"]\n        elif split_mode == \"TRAIN\":\n            train_coco = self\n            val_coco = None\n        elif split_mode == \"VAL\":\n            train_coco = None\n            val_coco = self\n\n        # create train val image dirs\n        train_dir = \"\"\n        val_dir = \"\"\n        if split_mode in [\"TRAINVAL\", \"TRAIN\"]:\n            train_dir = Path(os.path.abspath(output_dir)) / \"train/\"\n            train_dir.mkdir(parents=True, exist_ok=True)  # create dir\n        if split_mode in [\"TRAINVAL\", \"VAL\"]:\n            val_dir = Path(os.path.abspath(output_dir)) / \"val/\"\n            val_dir.mkdir(parents=True, exist_ok=True)  # create dir\n\n        # create image symlinks and annotation txts\n        if split_mode in [\"TRAINVAL\", \"TRAIN\"]:\n            export_yolo_images_and_txts_from_coco_object(\n                output_dir=train_dir,\n                coco=train_coco,\n                ignore_negative_samples=self.ignore_negative_samples,\n                mp=mp,\n                disable_symlink=disable_symlink,\n            )\n        if split_mode in [\"TRAINVAL\", \"VAL\"]:\n            export_yolo_images_and_txts_from_coco_object(\n                output_dir=val_dir,\n                coco=val_coco,\n                ignore_negative_samples=self.ignore_negative_samples,\n                mp=mp,\n                disable_symlink=disable_symlink,\n            )\n\n        # create yolov5 data yaml\n        data = {\n            \"train\": str(train_dir),\n            \"val\": str(val_dir),\n            \"nc\": len(self.category_mapping),\n            \"names\": list(self.category_mapping.values()),\n        }\n        yaml_path = str(Path(output_dir) / \"data.yml\")\n        with open(yaml_path, \"w\") as outfile:\n            yaml.dump(data, outfile, default_flow_style=None)\n\n    def get_subsampled_coco(self, subsample_ratio: int = 2, category_id: int | None = None):\n        \"\"\"Subsamples images with subsample_ratio and returns as sahi.utils.coco.Coco object.\n\n        Args:\n            subsample_ratio: int\n                10 means take every 10th image with its annotations\n            category_id: int\n                subsample only images containing given category_id, if -1 then subsamples negative samples\n        Returns:\n            subsampled_coco: sahi.utils.coco.Coco\n        \"\"\"\n        subsampled_coco = Coco(\n            name=self.name,\n            image_dir=self.image_dir,\n            remapping_dict=self.remapping_dict,\n            ignore_negative_samples=self.ignore_negative_samples,\n        )\n        subsampled_coco.add_categories_from_coco_category_list(self.json_categories)\n\n        if category_id is not None:\n            # get images that contain given category id\n            images_that_contain_category: list[CocoImage] = []\n            annotation: CocoAnnotation\n            for image in self.images:\n                category_id_to_contains = defaultdict(int)\n                for annotation in image.annotations:\n                    category_id_to_contains[annotation.category_id] = 1\n                if category_id_to_contains[category_id]:\n                    add_this_image = True\n                elif category_id == -1 and len(image.annotations) == 0:\n                    # if category_id is given as -1, select negative samples\n                    add_this_image = True\n                else:\n                    add_this_image = False\n\n                if add_this_image:\n                    images_that_contain_category.append(image)\n\n            # get images that does not contain given category id\n            images_that_doesnt_contain_category: list[CocoImage] = []\n            for image in self.images:\n                category_id_to_contains = defaultdict(int)\n                for annotation in image.annotations:\n                    category_id_to_contains[annotation.category_id] = 1\n                if category_id_to_contains[category_id]:\n                    add_this_image = False\n                elif category_id == -1 and len(image.annotations) == 0:\n                    # if category_id is given as -1, dont select negative samples\n                    add_this_image = False\n                else:\n                    add_this_image = True\n\n                if add_this_image:\n                    images_that_doesnt_contain_category.append(image)\n\n        if category_id:\n            selected_images = images_that_contain_category\n            # add images that does not contain given category without subsampling\n            for image_ind in range(len(images_that_doesnt_contain_category)):\n                subsampled_coco.add_image(images_that_doesnt_contain_category[image_ind])\n        else:\n            selected_images = self.images\n        for image_ind in range(0, len(selected_images), subsample_ratio):\n            subsampled_coco.add_image(selected_images[image_ind])\n\n        return subsampled_coco\n\n    def get_upsampled_coco(self, upsample_ratio: int = 2, category_id: int | None = None):\n        \"\"\"Upsamples images with upsample_ratio and returns as sahi.utils.coco.Coco object.\n\n        Args:\n            upsample_ratio: int\n                10 means copy each sample 10 times\n            category_id: int\n                upsample only images containing given category_id, if -1 then upsamples negative samples\n        Returns:\n            upsampled_coco: sahi.utils.coco.Coco\n        \"\"\"\n        upsampled_coco = Coco(\n            name=self.name,\n            image_dir=self.image_dir,\n            remapping_dict=self.remapping_dict,\n            ignore_negative_samples=self.ignore_negative_samples,\n        )\n        upsampled_coco.add_categories_from_coco_category_list(self.json_categories)\n        for ind in range(upsample_ratio):\n            for image_ind in range(len(self.images)):\n                # calculate add_this_image\n                if category_id is not None:\n                    category_id_to_contains = defaultdict(int)\n                    annotation: CocoAnnotation\n                    for annotation in self.images[image_ind].annotations:\n                        category_id_to_contains[annotation.category_id] = 1\n                    if category_id_to_contains[category_id]:\n                        add_this_image = True\n                    elif category_id == -1 and len(self.images[image_ind].annotations) == 0:\n                        # if category_id is given as -1, select negative samples\n                        add_this_image = True\n                    elif ind == 0:\n                        # in first iteration add all images\n                        add_this_image = True\n                    else:\n                        add_this_image = False\n                else:\n                    add_this_image = True\n\n                if add_this_image:\n                    upsampled_coco.add_image(self.images[image_ind])\n\n        return upsampled_coco\n\n    def get_area_filtered_coco(self, min=0, max_val=float(\"inf\"), intervals_per_category=None):\n        \"\"\"Filters annotation areas with given min and max values and returns remaining images as sahi.utils.coco.Coco\n        object.\n\n        Args:\n            min: int\n                minimum allowed area\n            max_val: int\n                maximum allowed area\n            intervals_per_category: dict of dicts\n                {\n                    \"human\": {\"min\": 20, \"max\": 10000},\n                    \"vehicle\": {\"min\": 50, \"max\": 15000},\n                }\n        Returns:\n            area_filtered_coco: sahi.utils.coco.Coco\n        \"\"\"\n        area_filtered_coco = Coco(\n            name=self.name,\n            image_dir=self.image_dir,\n            remapping_dict=self.remapping_dict,\n            ignore_negative_samples=self.ignore_negative_samples,\n        )\n        area_filtered_coco.add_categories_from_coco_category_list(self.json_categories)\n        for image in self.images:\n            is_valid_image = True\n            for annotation in image.annotations:\n                if intervals_per_category is not None and annotation.category_name in intervals_per_category.keys():\n                    category_based_min = intervals_per_category[annotation.category_name][\"min\"]\n                    category_based_max = intervals_per_category[annotation.category_name][\"max\"]\n                    if annotation.area &lt; category_based_min or annotation.area &gt; category_based_max:\n                        is_valid_image = False\n                if annotation.area &lt; min or annotation.area &gt; max_val:\n                    is_valid_image = False\n            if is_valid_image:\n                area_filtered_coco.add_image(image)\n\n        return area_filtered_coco\n\n    def get_coco_with_clipped_bboxes(self):\n        \"\"\"Limits overflowing bounding boxes to image dimensions.\"\"\"\n        from sahi.slicing import annotation_inside_slice\n\n        coco = Coco(\n            name=self.name,\n            image_dir=self.image_dir,\n            remapping_dict=self.remapping_dict,\n            ignore_negative_samples=self.ignore_negative_samples,\n        )\n        coco.add_categories_from_coco_category_list(self.json_categories)\n\n        for coco_img in self.images:\n            img_dims = [0, 0, coco_img.width, coco_img.height]\n            coco_image = CocoImage(\n                file_name=coco_img.file_name, height=coco_img.height, width=coco_img.width, id=coco_img.id\n            )\n            for coco_ann in coco_img.annotations:\n                ann_dict: dict = coco_ann.json\n                if annotation_inside_slice(annotation=ann_dict, slice_bbox=img_dims):\n                    shapely_ann = coco_ann.get_sliced_coco_annotation(img_dims)\n                    bbox = ShapelyAnnotation.to_xywh(shapely_ann._shapely_annotation)\n                    coco_ann_from_shapely = CocoAnnotation(\n                        bbox=bbox,\n                        category_id=coco_ann.category_id,\n                        category_name=coco_ann.category_name,\n                        image_id=coco_ann.image_id,\n                    )\n                    coco_image.add_annotation(coco_ann_from_shapely)\n                else:\n                    continue\n            coco.add_image(coco_image)\n        return coco\n</code></pre> Functions\u00b6 <code></code> <code>__init__(name=None, image_dir=None, remapping_dict=None, ignore_negative_samples=False, clip_bboxes_to_img_dims=False, image_id_setting='auto')</code> \u00b6 <p>Creates Coco object.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> \u00b6 <code>str | None</code> <p>str Name of the Coco dataset, it determines exported json name.</p> <code>None</code> <code>image_dir</code> \u00b6 <code>str | None</code> <p>str Base file directory that contains dataset images. Required for dataset merging.</p> <code>None</code> <code>remapping_dict</code> \u00b6 <code>dict[int, int] | None</code> <p>dict {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1</p> <code>None</code> <code>ignore_negative_samples</code> \u00b6 <code>bool</code> <p>bool If True ignores images without annotations in all operations.</p> <code>False</code> <code>image_id_setting</code> \u00b6 <code>Literal['auto', 'manual']</code> <p>str how to assign image ids while exporting can be auto -&gt; will assign id from scratch (.id will be ignored) manual -&gt; you will need to provide image ids in  instances (.id can not be None) <code>'auto'</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(\n    self,\n    name: str | None = None,\n    image_dir: str | None = None,\n    remapping_dict: dict[int, int] | None = None,\n    ignore_negative_samples: bool = False,\n    clip_bboxes_to_img_dims: bool = False,\n    image_id_setting: Literal[\"auto\", \"manual\"] = \"auto\",\n):\n    \"\"\"Creates Coco object.\n\n    Args:\n        name: str\n            Name of the Coco dataset, it determines exported json name.\n        image_dir: str\n            Base file directory that contains dataset images. Required for dataset merging.\n        remapping_dict: dict\n            {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1\n        ignore_negative_samples: bool\n            If True ignores images without annotations in all operations.\n        image_id_setting: str\n            how to assign image ids while exporting can be\n            auto -&gt; will assign id from scratch (&lt;CocoImage&gt;.id will be ignored)\n            manual -&gt; you will need to provide image ids in &lt;CocoImage&gt; instances (&lt;CocoImage&gt;.id can not be None)\n    \"\"\"\n    if image_id_setting not in [\"auto\", \"manual\"]:\n        raise ValueError(\"image_id_setting must be either 'auto' or 'manual'\")\n    self.name: str | None = name\n    self.image_dir: str | None = image_dir\n    self.remapping_dict: dict[int, int] | None = remapping_dict\n    self.ignore_negative_samples = ignore_negative_samples\n    self.categories: list[CocoCategory] = []\n    self.images = []\n    self._stats = None\n    self.clip_bboxes_to_img_dims = clip_bboxes_to_img_dims\n    self.image_id_setting = image_id_setting\n</code></pre> <code></code> <code>add_categories_from_coco_category_list(coco_category_list)</code> \u00b6 <p>Creates CocoCategory object using coco category list.</p> <p>Parameters:</p> Name Type Description Default <code>coco_category_list</code> \u00b6 <p>List[Dict] [     {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},     {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"} ]</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_categories_from_coco_category_list(self, coco_category_list):\n    \"\"\"Creates CocoCategory object using coco category list.\n\n    Args:\n        coco_category_list: List[Dict]\n            [\n                {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},\n                {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"}\n            ]\n    \"\"\"\n\n    for coco_category in coco_category_list:\n        if self.remapping_dict is not None:\n            for source_id in self.remapping_dict.keys():\n                if coco_category[\"id\"] == source_id:\n                    target_id = self.remapping_dict[source_id]\n                    coco_category[\"id\"] = target_id\n\n        self.add_category(CocoCategory.from_coco_category(coco_category))\n</code></pre> <code></code> <code>add_category(category)</code> \u00b6 <p>Adds category to this Coco instance.</p> <p>Parameters:</p> Name Type Description Default <code>category</code> \u00b6 <p>CocoCategory</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_category(self, category):\n    \"\"\"Adds category to this Coco instance.\n\n    Args:\n        category: CocoCategory\n    \"\"\"\n\n    # assert type(category) == CocoCategory, \"category must be a CocoCategory instance\"\n    if not isinstance(category, CocoCategory):\n        raise TypeError(\"category must be a CocoCategory instance\")\n    self.categories.append(category)\n</code></pre> <code></code> <code>add_image(image)</code> \u00b6 <p>Adds image to this Coco instance.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <p>CocoImage</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_image(self, image):\n    \"\"\"Adds image to this Coco instance.\n\n    Args:\n        image: CocoImage\n    \"\"\"\n\n    if self.image_id_setting == \"manual\" and image.id is None:\n        raise ValueError(\"image id should be manually set for image_id_setting='manual'\")\n    self.images.append(image)\n</code></pre> <code></code> <code>calculate_stats()</code> \u00b6 <p>Iterates over all annotations and calculates total number of.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def calculate_stats(self):\n    \"\"\"Iterates over all annotations and calculates total number of.\"\"\"\n    # init all stats\n    num_annotations = 0\n    num_images = len(self.images)\n    num_negative_images = 0\n    num_categories = len(self.json_categories)\n    category_name_to_zero = {category[\"name\"]: 0 for category in self.json_categories}\n    category_name_to_inf = {category[\"name\"]: float(\"inf\") for category in self.json_categories}\n    num_images_per_category = copy.deepcopy(category_name_to_zero)\n    num_annotations_per_category = copy.deepcopy(category_name_to_zero)\n    min_annotation_area_per_category = copy.deepcopy(category_name_to_inf)\n    max_annotation_area_per_category = copy.deepcopy(category_name_to_zero)\n    min_num_annotations_in_image = float(\"inf\")\n    max_num_annotations_in_image = 0\n    total_annotation_area = 0\n    min_annotation_area = 1e10\n    max_annotation_area = 0\n    for image in self.images:\n        image_contains_category = {}\n        for annotation in image.annotations:\n            annotation_area = annotation.area\n            total_annotation_area += annotation_area\n            num_annotations_per_category[annotation.category_name] += 1\n            image_contains_category[annotation.category_name] = 1\n            # update min&amp;max annotation area\n            if annotation_area &gt; max_annotation_area:\n                max_annotation_area = annotation_area\n            if annotation_area &lt; min_annotation_area:\n                min_annotation_area = annotation_area\n            if annotation_area &gt; max_annotation_area_per_category[annotation.category_name]:\n                max_annotation_area_per_category[annotation.category_name] = annotation_area\n            if annotation_area &lt; min_annotation_area_per_category[annotation.category_name]:\n                min_annotation_area_per_category[annotation.category_name] = annotation_area\n        # update num_negative_images\n        if len(image.annotations) == 0:\n            num_negative_images += 1\n        # update num_annotations\n        num_annotations += len(image.annotations)\n        # update num_images_per_category\n        num_images_per_category = dict(Counter(num_images_per_category) + Counter(image_contains_category))\n        # update min&amp;max_num_annotations_in_image\n        num_annotations_in_image = len(image.annotations)\n        if num_annotations_in_image &gt; max_num_annotations_in_image:\n            max_num_annotations_in_image = num_annotations_in_image\n        if num_annotations_in_image &lt; min_num_annotations_in_image:\n            min_num_annotations_in_image = num_annotations_in_image\n    if (num_images - num_negative_images) &gt; 0:\n        avg_num_annotations_in_image = num_annotations / (num_images - num_negative_images)\n        avg_annotation_area = total_annotation_area / num_annotations\n    else:\n        avg_num_annotations_in_image = 0\n        avg_annotation_area = 0\n\n    self._stats = {\n        \"num_images\": num_images,\n        \"num_annotations\": num_annotations,\n        \"num_categories\": num_categories,\n        \"num_negative_images\": num_negative_images,\n        \"num_images_per_category\": num_images_per_category,\n        \"num_annotations_per_category\": num_annotations_per_category,\n        \"min_num_annotations_in_image\": min_num_annotations_in_image,\n        \"max_num_annotations_in_image\": max_num_annotations_in_image,\n        \"avg_num_annotations_in_image\": avg_num_annotations_in_image,\n        \"min_annotation_area\": min_annotation_area,\n        \"max_annotation_area\": max_annotation_area,\n        \"avg_annotation_area\": avg_annotation_area,\n        \"min_annotation_area_per_category\": min_annotation_area_per_category,\n        \"max_annotation_area_per_category\": max_annotation_area_per_category,\n    }\n</code></pre> <code></code> <code>export_as_yolo(output_dir, train_split_rate=1.0, numpy_seed=0, mp=False, disable_symlink=False)</code> \u00b6 <p>Exports current COCO dataset in ultralytics/yolo format. Creates train val folders with image symlinks and txt files and a data yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> \u00b6 <code>str | Path</code> <p>str Export directory.</p> required <code>train_split_rate</code> \u00b6 <code>float</code> <p>float If given 1, will be exported as train split. If given 0, will be exported as val split. If in between 0-1, both train/val splits will be calculated and exported.</p> <code>1.0</code> <code>numpy_seed</code> \u00b6 <code>int</code> <p>int To fix the numpy seed.</p> <code>0</code> <code>mp</code> \u00b6 <code>bool</code> <p>bool If True, multiprocess mode is on. Should be called in 'if name == main:' block.</p> <code>False</code> <code>disable_symlink</code> \u00b6 <code>bool</code> <p>bool If True, symlinks will not be created. Instead, images will be copied.</p> <code>False</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_as_yolo(\n    self,\n    output_dir: str | Path,\n    train_split_rate: float = 1.0,\n    numpy_seed: int = 0,\n    mp: bool = False,\n    disable_symlink: bool = False,\n):\n    \"\"\"Exports current COCO dataset in ultralytics/yolo format. Creates train val folders with image symlinks and\n    txt files and a data yaml file.\n\n    Args:\n        output_dir: str\n            Export directory.\n        train_split_rate: float\n            If given 1, will be exported as train split.\n            If given 0, will be exported as val split.\n            If in between 0-1, both train/val splits will be calculated and exported.\n        numpy_seed: int\n            To fix the numpy seed.\n        mp: bool\n            If True, multiprocess mode is on.\n            Should be called in 'if __name__ == __main__:' block.\n        disable_symlink: bool\n            If True, symlinks will not be created. Instead, images will be copied.\n    \"\"\"\n    try:\n        import yaml\n    except ImportError:\n        raise ImportError('Please run \"pip install -U pyyaml\" to install yaml first for yolo formatted exporting.')\n\n    # set split_mode\n    if 0 &lt; train_split_rate and train_split_rate &lt; 1:\n        split_mode = \"TRAINVAL\"\n    elif train_split_rate == 0:\n        split_mode = \"VAL\"\n    elif train_split_rate == 1:\n        split_mode = \"TRAIN\"\n    else:\n        raise ValueError(\"train_split_rate cannot be &lt;0 or &gt;1\")\n\n    # split dataset\n    if split_mode == \"TRAINVAL\":\n        result = self.split_coco_as_train_val(\n            train_split_rate=train_split_rate,\n            numpy_seed=numpy_seed,\n        )\n        train_coco = result[\"train_coco\"]\n        val_coco = result[\"val_coco\"]\n    elif split_mode == \"TRAIN\":\n        train_coco = self\n        val_coco = None\n    elif split_mode == \"VAL\":\n        train_coco = None\n        val_coco = self\n\n    # create train val image dirs\n    train_dir = \"\"\n    val_dir = \"\"\n    if split_mode in [\"TRAINVAL\", \"TRAIN\"]:\n        train_dir = Path(os.path.abspath(output_dir)) / \"train/\"\n        train_dir.mkdir(parents=True, exist_ok=True)  # create dir\n    if split_mode in [\"TRAINVAL\", \"VAL\"]:\n        val_dir = Path(os.path.abspath(output_dir)) / \"val/\"\n        val_dir.mkdir(parents=True, exist_ok=True)  # create dir\n\n    # create image symlinks and annotation txts\n    if split_mode in [\"TRAINVAL\", \"TRAIN\"]:\n        export_yolo_images_and_txts_from_coco_object(\n            output_dir=train_dir,\n            coco=train_coco,\n            ignore_negative_samples=self.ignore_negative_samples,\n            mp=mp,\n            disable_symlink=disable_symlink,\n        )\n    if split_mode in [\"TRAINVAL\", \"VAL\"]:\n        export_yolo_images_and_txts_from_coco_object(\n            output_dir=val_dir,\n            coco=val_coco,\n            ignore_negative_samples=self.ignore_negative_samples,\n            mp=mp,\n            disable_symlink=disable_symlink,\n        )\n\n    # create yolov5 data yaml\n    data = {\n        \"train\": str(train_dir),\n        \"val\": str(val_dir),\n        \"nc\": len(self.category_mapping),\n        \"names\": list(self.category_mapping.values()),\n    }\n    yaml_path = str(Path(output_dir) / \"data.yml\")\n    with open(yaml_path, \"w\") as outfile:\n        yaml.dump(data, outfile, default_flow_style=None)\n</code></pre> <code></code> <code>export_as_yolov5(output_dir, train_split_rate=1.0, numpy_seed=0, mp=False, disable_symlink=False)</code> \u00b6 <p>Deprecated.</p> <p>Please use export_as_yolo instead. Calls export_as_yolo with the same arguments.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_as_yolov5(\n    self,\n    output_dir: str | Path,\n    train_split_rate: float = 1.0,\n    numpy_seed: int = 0,\n    mp: bool = False,\n    disable_symlink: bool = False,\n):\n    \"\"\"Deprecated.\n\n    Please use export_as_yolo instead. Calls export_as_yolo with the same arguments.\n    \"\"\"\n    warnings.warn(\n        \"export_as_yolov5 is deprecated. Please use export_as_yolo instead.\",\n        DeprecationWarning,\n    )\n    self.export_as_yolo(\n        output_dir=output_dir,\n        train_split_rate=train_split_rate,\n        numpy_seed=numpy_seed,\n        mp=mp,\n        disable_symlink=disable_symlink,\n    )\n</code></pre> <code></code> <code>from_coco_dict_or_path(coco_dict_or_path, image_dir=None, remapping_dict=None, ignore_negative_samples=False, clip_bboxes_to_img_dims=False, use_threads=False, num_threads=10)</code> <code>classmethod</code> \u00b6 <p>Creates coco object from COCO formatted dict or COCO dataset file path.</p> <p>Parameters:</p> Name Type Description Default <code>coco_dict_or_path</code> \u00b6 <code>dict | str</code> <p>dict/str or List[dict/str] COCO formatted dict or COCO dataset file path List of COCO formatted dict or COCO dataset file path</p> required <code>image_dir</code> \u00b6 <code>str | None</code> <p>str Base file directory that contains dataset images. Required for merging and yolov5 conversion.</p> <code>None</code> <code>remapping_dict</code> \u00b6 <code>dict | None</code> <p>dict {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1</p> <code>None</code> <code>ignore_negative_samples</code> \u00b6 <code>bool</code> <p>bool If True ignores images without annotations in all operations.</p> <code>False</code> <code>clip_bboxes_to_img_dims</code> \u00b6 <code>bool</code> <p>bool = False Limits bounding boxes to image dimensions.</p> <code>False</code> <code>use_threads</code> \u00b6 <code>bool</code> <p>bool = False Use threads when processing the json image list, defaults to False</p> <code>False</code> <code>num_threads</code> \u00b6 <code>int</code> <p>int = 10 Slice the image list to given number of chunks, defaults to 10</p> <code>10</code> Properties <p>images: list of CocoImage category_mapping: dict</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_dict_or_path(\n    cls,\n    coco_dict_or_path: dict | str,\n    image_dir: str | None = None,\n    remapping_dict: dict | None = None,\n    ignore_negative_samples: bool = False,\n    clip_bboxes_to_img_dims: bool = False,\n    use_threads: bool = False,\n    num_threads: int = 10,\n):\n    \"\"\"Creates coco object from COCO formatted dict or COCO dataset file path.\n\n    Args:\n        coco_dict_or_path: dict/str or List[dict/str]\n            COCO formatted dict or COCO dataset file path\n            List of COCO formatted dict or COCO dataset file path\n        image_dir: str\n            Base file directory that contains dataset images. Required for merging and yolov5 conversion.\n        remapping_dict: dict\n            {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1\n        ignore_negative_samples: bool\n            If True ignores images without annotations in all operations.\n        clip_bboxes_to_img_dims: bool = False\n            Limits bounding boxes to image dimensions.\n        use_threads: bool = False\n            Use threads when processing the json image list, defaults to False\n        num_threads: int = 10\n            Slice the image list to given number of chunks, defaults to 10\n\n    Properties:\n        images: list of CocoImage\n        category_mapping: dict\n    \"\"\"\n    # init coco object\n    coco = cls(\n        image_dir=image_dir,\n        remapping_dict=remapping_dict,\n        ignore_negative_samples=ignore_negative_samples,\n        clip_bboxes_to_img_dims=clip_bboxes_to_img_dims,\n    )\n\n    if type(coco_dict_or_path) not in [str, dict]:\n        raise TypeError(\"coco_dict_or_path should be a dict or str\")\n\n    # load coco dict if path is given\n    if isinstance(coco_dict_or_path, str):\n        coco_dict = load_json(coco_dict_or_path)\n    else:\n        coco_dict = coco_dict_or_path\n\n    dict_size = len(coco_dict[\"images\"])\n\n    # arrange image id to annotation id mapping\n    coco.add_categories_from_coco_category_list(coco_dict[\"categories\"])\n    image_id_to_annotation_list = get_imageid2annotationlist_mapping(coco_dict)\n    category_mapping = coco.category_mapping\n\n    # https://github.com/obss/sahi/issues/98\n    image_id_set: set = set()\n\n    lock = Lock()\n\n    def fill_image_id_set(start, finish, image_list, _image_id_set, _image_id_to_annotation_list, _coco, lock):\n        for coco_image_dict in tqdm(\n            image_list[start:finish], f\"Loading coco annotations between {start} and {finish}\"\n        ):\n            coco_image = CocoImage.from_coco_image_dict(coco_image_dict)\n            image_id = coco_image_dict[\"id\"]\n            # https://github.com/obss/sahi/issues/98\n            if image_id in _image_id_set:\n                print(f\"duplicate image_id: {image_id}, will be ignored.\")\n                continue\n            else:\n                lock.acquire()\n                _image_id_set.add(image_id)\n                lock.release()\n\n            # select annotations of the image\n            annotation_list = _image_id_to_annotation_list[image_id]\n            for coco_annotation_dict in annotation_list:\n                # apply category remapping if remapping_dict is provided\n                if _coco.remapping_dict is not None:\n                    # apply category remapping (id:id)\n                    category_id = _coco.remapping_dict[coco_annotation_dict[\"category_id\"]]\n                    # update category id\n                    coco_annotation_dict[\"category_id\"] = category_id\n                else:\n                    category_id = coco_annotation_dict[\"category_id\"]\n                # get category name (id:name)\n                category_name = category_mapping[category_id]\n                coco_annotation = CocoAnnotation.from_coco_annotation_dict(\n                    category_name=category_name, annotation_dict=coco_annotation_dict\n                )\n                coco_image.add_annotation(coco_annotation)\n            _coco.add_image(coco_image)\n\n    chunk_size = dict_size / num_threads\n\n    if use_threads is True:\n        for i in range(num_threads):\n            start = i * chunk_size\n            finish = start + chunk_size\n            if finish &gt; dict_size:\n                finish = dict_size\n            t = Thread(\n                target=fill_image_id_set,\n                args=(start, finish, coco_dict[\"images\"], image_id_set, image_id_to_annotation_list, coco, lock),\n            )\n            t.start()\n\n        main_thread = threading.currentThread()\n        for t in threading.enumerate():\n            if t is not main_thread:\n                t.join()\n\n    else:\n        for coco_image_dict in tqdm(coco_dict[\"images\"], \"Loading coco annotations\"):\n            coco_image = CocoImage.from_coco_image_dict(coco_image_dict)\n            image_id = coco_image_dict[\"id\"]\n            # https://github.com/obss/sahi/issues/98\n            if image_id in image_id_set:\n                print(f\"duplicate image_id: {image_id}, will be ignored.\")\n                continue\n            else:\n                image_id_set.add(image_id)\n            # select annotations of the image\n            annotation_list = image_id_to_annotation_list[image_id]\n            # TODO: coco_annotation_dict is of type CocoAnnotation according to how image_id_to_annotation_list\n            # was created. Either image_id_to_annotation_list is not defined correctly or the following\n            # loop is wrong as it expects a dict.\n            for coco_annotation_dict in annotation_list:\n                # apply category remapping if remapping_dict is provided\n                if coco.remapping_dict is not None:\n                    # apply category remapping (id:id)\n                    category_id = coco.remapping_dict[coco_annotation_dict[\"category_id\"]]\n                    # update category id\n                    coco_annotation_dict[\"category_id\"] = category_id\n                else:\n                    category_id = coco_annotation_dict[\"category_id\"]\n                # get category name (id:name)\n                category_name = category_mapping[category_id]\n                coco_annotation = CocoAnnotation.from_coco_annotation_dict(\n                    category_name=category_name, annotation_dict=coco_annotation_dict\n                )\n                coco_image.add_annotation(coco_annotation)\n            coco.add_image(coco_image)\n\n    if clip_bboxes_to_img_dims:\n        coco = coco.get_coco_with_clipped_bboxes()\n    return coco\n</code></pre> <code></code> <code>get_area_filtered_coco(min=0, max_val=float('inf'), intervals_per_category=None)</code> \u00b6 <p>Filters annotation areas with given min and max values and returns remaining images as sahi.utils.coco.Coco object.</p> <p>Parameters:</p> Name Type Description Default <code>min</code> \u00b6 <p>int minimum allowed area</p> <code>0</code> <code>max_val</code> \u00b6 <p>int maximum allowed area</p> <code>float('inf')</code> <code>intervals_per_category</code> \u00b6 <p>dict of dicts {     \"human\": {\"min\": 20, \"max\": 10000},     \"vehicle\": {\"min\": 50, \"max\": 15000}, }</p> <code>None</code> <p>Returns:     area_filtered_coco: sahi.utils.coco.Coco</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def get_area_filtered_coco(self, min=0, max_val=float(\"inf\"), intervals_per_category=None):\n    \"\"\"Filters annotation areas with given min and max values and returns remaining images as sahi.utils.coco.Coco\n    object.\n\n    Args:\n        min: int\n            minimum allowed area\n        max_val: int\n            maximum allowed area\n        intervals_per_category: dict of dicts\n            {\n                \"human\": {\"min\": 20, \"max\": 10000},\n                \"vehicle\": {\"min\": 50, \"max\": 15000},\n            }\n    Returns:\n        area_filtered_coco: sahi.utils.coco.Coco\n    \"\"\"\n    area_filtered_coco = Coco(\n        name=self.name,\n        image_dir=self.image_dir,\n        remapping_dict=self.remapping_dict,\n        ignore_negative_samples=self.ignore_negative_samples,\n    )\n    area_filtered_coco.add_categories_from_coco_category_list(self.json_categories)\n    for image in self.images:\n        is_valid_image = True\n        for annotation in image.annotations:\n            if intervals_per_category is not None and annotation.category_name in intervals_per_category.keys():\n                category_based_min = intervals_per_category[annotation.category_name][\"min\"]\n                category_based_max = intervals_per_category[annotation.category_name][\"max\"]\n                if annotation.area &lt; category_based_min or annotation.area &gt; category_based_max:\n                    is_valid_image = False\n            if annotation.area &lt; min or annotation.area &gt; max_val:\n                is_valid_image = False\n        if is_valid_image:\n            area_filtered_coco.add_image(image)\n\n    return area_filtered_coco\n</code></pre> <code></code> <code>get_coco_with_clipped_bboxes()</code> \u00b6 <p>Limits overflowing bounding boxes to image dimensions.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def get_coco_with_clipped_bboxes(self):\n    \"\"\"Limits overflowing bounding boxes to image dimensions.\"\"\"\n    from sahi.slicing import annotation_inside_slice\n\n    coco = Coco(\n        name=self.name,\n        image_dir=self.image_dir,\n        remapping_dict=self.remapping_dict,\n        ignore_negative_samples=self.ignore_negative_samples,\n    )\n    coco.add_categories_from_coco_category_list(self.json_categories)\n\n    for coco_img in self.images:\n        img_dims = [0, 0, coco_img.width, coco_img.height]\n        coco_image = CocoImage(\n            file_name=coco_img.file_name, height=coco_img.height, width=coco_img.width, id=coco_img.id\n        )\n        for coco_ann in coco_img.annotations:\n            ann_dict: dict = coco_ann.json\n            if annotation_inside_slice(annotation=ann_dict, slice_bbox=img_dims):\n                shapely_ann = coco_ann.get_sliced_coco_annotation(img_dims)\n                bbox = ShapelyAnnotation.to_xywh(shapely_ann._shapely_annotation)\n                coco_ann_from_shapely = CocoAnnotation(\n                    bbox=bbox,\n                    category_id=coco_ann.category_id,\n                    category_name=coco_ann.category_name,\n                    image_id=coco_ann.image_id,\n                )\n                coco_image.add_annotation(coco_ann_from_shapely)\n            else:\n                continue\n        coco.add_image(coco_image)\n    return coco\n</code></pre> <code></code> <code>get_subsampled_coco(subsample_ratio=2, category_id=None)</code> \u00b6 <p>Subsamples images with subsample_ratio and returns as sahi.utils.coco.Coco object.</p> <p>Parameters:</p> Name Type Description Default <code>subsample_ratio</code> \u00b6 <code>int</code> <p>int 10 means take every 10th image with its annotations</p> <code>2</code> <code>category_id</code> \u00b6 <code>int | None</code> <p>int subsample only images containing given category_id, if -1 then subsamples negative samples</p> <code>None</code> <p>Returns:     subsampled_coco: sahi.utils.coco.Coco</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def get_subsampled_coco(self, subsample_ratio: int = 2, category_id: int | None = None):\n    \"\"\"Subsamples images with subsample_ratio and returns as sahi.utils.coco.Coco object.\n\n    Args:\n        subsample_ratio: int\n            10 means take every 10th image with its annotations\n        category_id: int\n            subsample only images containing given category_id, if -1 then subsamples negative samples\n    Returns:\n        subsampled_coco: sahi.utils.coco.Coco\n    \"\"\"\n    subsampled_coco = Coco(\n        name=self.name,\n        image_dir=self.image_dir,\n        remapping_dict=self.remapping_dict,\n        ignore_negative_samples=self.ignore_negative_samples,\n    )\n    subsampled_coco.add_categories_from_coco_category_list(self.json_categories)\n\n    if category_id is not None:\n        # get images that contain given category id\n        images_that_contain_category: list[CocoImage] = []\n        annotation: CocoAnnotation\n        for image in self.images:\n            category_id_to_contains = defaultdict(int)\n            for annotation in image.annotations:\n                category_id_to_contains[annotation.category_id] = 1\n            if category_id_to_contains[category_id]:\n                add_this_image = True\n            elif category_id == -1 and len(image.annotations) == 0:\n                # if category_id is given as -1, select negative samples\n                add_this_image = True\n            else:\n                add_this_image = False\n\n            if add_this_image:\n                images_that_contain_category.append(image)\n\n        # get images that does not contain given category id\n        images_that_doesnt_contain_category: list[CocoImage] = []\n        for image in self.images:\n            category_id_to_contains = defaultdict(int)\n            for annotation in image.annotations:\n                category_id_to_contains[annotation.category_id] = 1\n            if category_id_to_contains[category_id]:\n                add_this_image = False\n            elif category_id == -1 and len(image.annotations) == 0:\n                # if category_id is given as -1, dont select negative samples\n                add_this_image = False\n            else:\n                add_this_image = True\n\n            if add_this_image:\n                images_that_doesnt_contain_category.append(image)\n\n    if category_id:\n        selected_images = images_that_contain_category\n        # add images that does not contain given category without subsampling\n        for image_ind in range(len(images_that_doesnt_contain_category)):\n            subsampled_coco.add_image(images_that_doesnt_contain_category[image_ind])\n    else:\n        selected_images = self.images\n    for image_ind in range(0, len(selected_images), subsample_ratio):\n        subsampled_coco.add_image(selected_images[image_ind])\n\n    return subsampled_coco\n</code></pre> <code></code> <code>get_upsampled_coco(upsample_ratio=2, category_id=None)</code> \u00b6 <p>Upsamples images with upsample_ratio and returns as sahi.utils.coco.Coco object.</p> <p>Parameters:</p> Name Type Description Default <code>upsample_ratio</code> \u00b6 <code>int</code> <p>int 10 means copy each sample 10 times</p> <code>2</code> <code>category_id</code> \u00b6 <code>int | None</code> <p>int upsample only images containing given category_id, if -1 then upsamples negative samples</p> <code>None</code> <p>Returns:     upsampled_coco: sahi.utils.coco.Coco</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def get_upsampled_coco(self, upsample_ratio: int = 2, category_id: int | None = None):\n    \"\"\"Upsamples images with upsample_ratio and returns as sahi.utils.coco.Coco object.\n\n    Args:\n        upsample_ratio: int\n            10 means copy each sample 10 times\n        category_id: int\n            upsample only images containing given category_id, if -1 then upsamples negative samples\n    Returns:\n        upsampled_coco: sahi.utils.coco.Coco\n    \"\"\"\n    upsampled_coco = Coco(\n        name=self.name,\n        image_dir=self.image_dir,\n        remapping_dict=self.remapping_dict,\n        ignore_negative_samples=self.ignore_negative_samples,\n    )\n    upsampled_coco.add_categories_from_coco_category_list(self.json_categories)\n    for ind in range(upsample_ratio):\n        for image_ind in range(len(self.images)):\n            # calculate add_this_image\n            if category_id is not None:\n                category_id_to_contains = defaultdict(int)\n                annotation: CocoAnnotation\n                for annotation in self.images[image_ind].annotations:\n                    category_id_to_contains[annotation.category_id] = 1\n                if category_id_to_contains[category_id]:\n                    add_this_image = True\n                elif category_id == -1 and len(self.images[image_ind].annotations) == 0:\n                    # if category_id is given as -1, select negative samples\n                    add_this_image = True\n                elif ind == 0:\n                    # in first iteration add all images\n                    add_this_image = True\n                else:\n                    add_this_image = False\n            else:\n                add_this_image = True\n\n            if add_this_image:\n                upsampled_coco.add_image(self.images[image_ind])\n\n    return upsampled_coco\n</code></pre> <code></code> <code>merge(coco, desired_name2id=None, verbose=1)</code> \u00b6 <p>Combines the images/annotations/categories of given coco object with current one.</p> <p>Parameters:</p> Name Type Description Default <code>coco </code> \u00b6 <p>sahi.utils.coco.Coco instance A COCO dataset object</p> required <code>desired_name2id </code> \u00b6 <p>dict</p> required <code>verbose</code> \u00b6 <p>bool If True, merging info is printed</p> <code>1</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def merge(self, coco, desired_name2id=None, verbose=1):\n    \"\"\"Combines the images/annotations/categories of given coco object with current one.\n\n    Args:\n        coco : sahi.utils.coco.Coco instance\n            A COCO dataset object\n        desired_name2id : dict\n            {\"human\": 1, \"car\": 2, \"big_vehicle\": 3}\n        verbose: bool\n            If True, merging info is printed\n    \"\"\"\n    if self.image_dir is None or coco.image_dir is None:\n        raise ValueError(\"image_dir should be provided for merging.\")\n    if verbose:\n        if not desired_name2id:\n            print(\"'desired_name2id' is not specified, combining all categories.\")\n\n    # create desired_name2id by combining all categories, if desired_name2id is not specified\n    coco1 = self\n    coco2 = coco\n    category_ind = 0\n    if desired_name2id is None:\n        desired_name2id = {}\n        for coco in [coco1, coco2]:\n            temp_categories = copy.deepcopy(coco.json_categories)\n            for temp_category in temp_categories:\n                if temp_category[\"name\"] not in desired_name2id:\n                    desired_name2id[temp_category[\"name\"]] = category_ind\n                    category_ind += 1\n                else:\n                    continue\n\n    # update categories and image paths\n    for coco in [coco1, coco2]:\n        coco.update_categories(desired_name2id=desired_name2id, update_image_filenames=True)\n\n    # combine images and categories\n    coco1.images.extend(coco2.images)\n    self.images: list[CocoImage] = coco1.images\n    self.categories = coco1.categories\n\n    # print categories\n    if verbose:\n        print(\n            \"Categories are formed as:\\n\",\n            self.json_categories,\n        )\n</code></pre> <code></code> <code>split_coco_as_train_val(train_split_rate=0.9, numpy_seed=0)</code> \u00b6 <p>Split images into train-val and returns them as sahi.utils.coco.Coco objects.</p> <p>Parameters:</p> Name Type Description Default <code>train_split_rate</code> \u00b6 <p>float</p> <code>0.9</code> <code>numpy_seed</code> \u00b6 <p>int random seed. Actually, this doesn't use numpy, but the random package from the standard library, but it is called numpy for compatibility.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>result</code> <p>dict {     \"train_coco\": \"\",     \"val_coco\": \"\", }</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def split_coco_as_train_val(self, train_split_rate=0.9, numpy_seed=0):\n    \"\"\"Split images into train-val and returns them as sahi.utils.coco.Coco objects.\n\n    Args:\n        train_split_rate: float\n        numpy_seed: int\n            random seed. Actually, this doesn't use numpy, but the random package\n            from the standard library, but it is called numpy for compatibility.\n\n    Returns:\n        result : dict\n            {\n                \"train_coco\": \"\",\n                \"val_coco\": \"\",\n            }\n    \"\"\"\n    # divide images\n    num_images = len(self.images)\n    shuffled_images = copy.deepcopy(self.images)\n    random.seed(numpy_seed)\n    random.shuffle(shuffled_images)\n    num_train = int(num_images * train_split_rate)\n    train_images = shuffled_images[:num_train]\n    val_images = shuffled_images[num_train:]\n\n    # form train val coco objects\n    train_coco = Coco(\n        name=self.name if self.name else \"split\" + \"_train\",\n        image_dir=self.image_dir,\n    )\n    train_coco.images = train_images\n    train_coco.categories = self.categories\n\n    val_coco = Coco(name=self.name if self.name else \"split\" + \"_val\", image_dir=self.image_dir)\n    val_coco.images = val_images\n    val_coco.categories = self.categories\n\n    # return result\n    return {\n        \"train_coco\": train_coco,\n        \"val_coco\": val_coco,\n    }\n</code></pre> <code></code> <code>update_categories(desired_name2id, update_image_filenames=False)</code> \u00b6 <p>Rearranges category mapping of given COCO object based on given desired_name2id. Can also be used to filter some of the categories.</p> <p>Parameters:</p> Name Type Description Default <code>desired_name2id</code> \u00b6 <code>dict[str, int]</code> <p>dict</p> required <code>update_image_filenames</code> \u00b6 <code>bool</code> <p>bool If True, updates coco image file_names with absolute file paths.</p> <code>False</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def update_categories(self, desired_name2id: dict[str, int], update_image_filenames: bool = False):\n    \"\"\"Rearranges category mapping of given COCO object based on given desired_name2id. Can also be used to filter\n    some of the categories.\n\n    Args:\n        desired_name2id: dict\n            {\"big_vehicle\": 1, \"car\": 2, \"human\": 3}\n        update_image_filenames: bool\n            If True, updates coco image file_names with absolute file paths.\n    \"\"\"\n    # init vars\n    currentid2desiredid_mapping: dict[int, int | None] = {}\n    updated_coco = Coco(\n        name=self.name,\n        image_dir=self.image_dir,\n        remapping_dict=self.remapping_dict,\n        ignore_negative_samples=self.ignore_negative_samples,\n    )\n    # create category id mapping (currentid2desiredid_mapping)\n    for coco_category in self.categories:\n        current_category_id = coco_category.id\n        current_category_name = coco_category.name\n        if not current_category_name:\n            logger.warning(\"no category name provided to update categories\")\n            continue\n        if current_category_name in desired_name2id.keys():\n            currentid2desiredid_mapping[current_category_id] = desired_name2id[current_category_name]\n        else:\n            # ignore categories that are not included in desired_name2id\n            currentid2desiredid_mapping[current_category_id] = None\n\n    # add updated categories\n    for name in desired_name2id.keys():\n        updated_coco_category = CocoCategory(id=desired_name2id[name], name=name, supercategory=name)\n        updated_coco.add_category(updated_coco_category)\n\n    # add updated images &amp; annotations\n    for coco_image in copy.deepcopy(self.images):\n        updated_coco_image = CocoImage.from_coco_image_dict(coco_image.json)\n        # update filename to abspath\n        file_name_is_abspath = True if os.path.abspath(coco_image.file_name) == coco_image.file_name else False\n        if update_image_filenames and not file_name_is_abspath:\n            if not self.image_dir:\n                logger.error(\"image directory not set\")\n            else:\n                updated_coco_image.file_name = str(Path(os.path.abspath(self.image_dir)) / coco_image.file_name)\n        # update annotations\n        for coco_annotation in coco_image.annotations:\n            current_category_id = coco_annotation.category_id\n            desired_category_id = currentid2desiredid_mapping[current_category_id]\n            # append annotations with category id present in desired_name2id\n            if desired_category_id is not None:\n                # update cetegory id\n                coco_annotation.category_id = desired_category_id\n                # append updated annotation to target coco dict\n                updated_coco_image.add_annotation(coco_annotation)\n        updated_coco.add_image(updated_coco_image)\n\n    # overwrite instance\n    self.__dict__ = updated_coco.__dict__\n</code></pre> <code></code> <code>CocoAnnotation</code> \u00b6 <p>COCO formatted annotation.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoAnnotation:\n    \"\"\"COCO formatted annotation.\"\"\"\n\n    @classmethod\n    def from_coco_segmentation(cls, segmentation, category_id, category_name, iscrowd=0):\n        \"\"\"Creates CocoAnnotation object using coco segmentation.\n\n        Args:\n            segmentation: List[List]\n                [[1, 1, 325, 125, 250, 200, 5, 200]]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            iscrowd: int\n                0 or 1\n        \"\"\"\n        return cls(\n            segmentation=segmentation,\n            category_id=category_id,\n            category_name=category_name,\n            iscrowd=iscrowd,\n        )\n\n    @classmethod\n    def from_coco_bbox(cls, bbox, category_id, category_name, iscrowd=0):\n        \"\"\"Creates CocoAnnotation object using coco bbox.\n\n        Args:\n            bbox: List\n                [xmin, ymin, width, height]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            iscrowd: int\n                0 or 1\n        \"\"\"\n        return cls(\n            bbox=bbox,\n            category_id=category_id,\n            category_name=category_name,\n            iscrowd=iscrowd,\n        )\n\n    @classmethod\n    def from_coco_annotation_dict(cls, annotation_dict: dict, category_name: str | None = None):\n        \"\"\"Creates CocoAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\",\n        \"segmentation\", \"category_id\").\n\n        Args:\n            category_name: str\n                Category name of the annotation\n            annotation_dict: dict\n                COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")\n        \"\"\"\n        if annotation_dict.__contains__(\"segmentation\") and isinstance(annotation_dict[\"segmentation\"], dict):\n            has_rle_segmentation = True\n            logger.warning(\n                f\"Segmentation annotation for id {annotation_dict['id']} is skipped since \"\n                \"RLE segmentation format is not supported.\"\n            )\n        else:\n            has_rle_segmentation = False\n\n        if (\n            annotation_dict.__contains__(\"segmentation\")\n            and annotation_dict[\"segmentation\"]\n            and not has_rle_segmentation\n        ):\n            return cls(\n                segmentation=annotation_dict[\"segmentation\"],\n                category_id=annotation_dict[\"category_id\"],\n                category_name=category_name,\n            )\n        else:\n            return cls(\n                bbox=annotation_dict[\"bbox\"],\n                category_id=annotation_dict[\"category_id\"],\n                category_name=category_name,\n            )\n\n    @classmethod\n    def from_shapely_annotation(\n        cls,\n        shapely_annotation: ShapelyAnnotation,\n        category_id: int,\n        category_name: str,\n        iscrowd: int,\n    ):\n        \"\"\"Creates CocoAnnotation object from ShapelyAnnotation object.\n\n        Args:\n            shapely_annotation (ShapelyAnnotation)\n            category_id (int): Category id of the annotation\n            category_name (str): Category name of the annotation\n            iscrowd (int): 0 or 1\n        \"\"\"\n        coco_annotation = cls(\n            bbox=[0, 0, 0, 0],\n            category_id=category_id,\n            category_name=category_name,\n            iscrowd=iscrowd,\n        )\n        coco_annotation._segmentation = shapely_annotation.to_coco_segmentation()\n        coco_annotation._shapely_annotation = shapely_annotation\n        return coco_annotation\n\n    def __init__(\n        self,\n        category_id: int,\n        category_name: str | None = None,\n        segmentation=None,\n        bbox: list[int] | None = None,\n        image_id=None,\n        iscrowd=0,\n    ):\n        \"\"\"Creates coco annotation object using bbox or segmentation.\n\n        Args:\n            segmentation: List[List]\n                [[1, 1, 325, 125, 250, 200, 5, 200]]\n            bbox: List\n                [xmin, ymin, width, height]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            image_id: int\n                Image ID of the annotation\n            iscrowd: int\n                0 or 1\n        \"\"\"\n        if bbox is None and segmentation is None:\n            raise ValueError(\"you must provide a bbox or polygon\")\n\n        self._segmentation = segmentation\n        self._category_id = category_id\n        self._category_name = category_name\n        self._image_id = image_id\n        self._iscrowd = iscrowd\n\n        if self._segmentation:\n            shapely_annotation = ShapelyAnnotation.from_coco_segmentation(segmentation=self._segmentation)\n        else:\n            if not bbox:\n                raise TypeError(\"Coco bounding box not set\")\n            shapely_annotation = ShapelyAnnotation.from_coco_bbox(bbox=bbox)\n        self._shapely_annotation = shapely_annotation\n\n    def get_sliced_coco_annotation(self, slice_bbox: list[int]):\n        shapely_polygon = box(slice_bbox[0], slice_bbox[1], slice_bbox[2], slice_bbox[3])\n        intersection_shapely_annotation = self._shapely_annotation.get_intersection(shapely_polygon)\n        return CocoAnnotation.from_shapely_annotation(\n            intersection_shapely_annotation,\n            category_id=self.category_id,\n            category_name=self.category_name or \"\",\n            iscrowd=self.iscrowd,\n        )\n\n    @property\n    def area(self):\n        \"\"\"Returns area of annotation polygon (or bbox if no polygon available)\"\"\"\n        return self._shapely_annotation.area\n\n    @property\n    def bbox(self):\n        \"\"\"Returns coco formatted bbox of the annotation as [xmin, ymin, width, height]\"\"\"\n        return self._shapely_annotation.to_xywh()\n\n    @property\n    def segmentation(self):\n        \"\"\"Returns coco formatted segmentation of the annotation as [[1, 1, 325, 125, 250, 200, 5, 200]]\"\"\"\n        if self._segmentation:\n            return self._shapely_annotation.to_coco_segmentation()\n        else:\n            return []\n\n    @property\n    def category_id(self):\n        \"\"\"Returns category id of the annotation as int.\"\"\"\n        return self._category_id\n\n    @category_id.setter\n    def category_id(self, i):\n        if not isinstance(i, int):\n            raise Exception(\"category_id must be an integer\")\n        self._category_id = i\n\n    @property\n    def image_id(self):\n        \"\"\"Returns image id of the annotation as int.\"\"\"\n        return self._image_id\n\n    @image_id.setter\n    def image_id(self, i):\n        if not isinstance(i, int):\n            raise Exception(\"image_id must be an integer\")\n        self._image_id = i\n\n    @property\n    def category_name(self):\n        \"\"\"Returns category name of the annotation as str.\"\"\"\n        return self._category_name\n\n    @category_name.setter\n    def category_name(self, n):\n        if not isinstance(n, str):\n            raise Exception(\"category_name must be a string\")\n        self._category_name = n\n\n    @property\n    def iscrowd(self):\n        \"\"\"Returns iscrowd info of the annotation.\"\"\"\n        return self._iscrowd\n\n    @property\n    def json(self):\n        return {\n            \"image_id\": self.image_id,\n            \"bbox\": self.bbox,\n            \"category_id\": self.category_id,\n            \"segmentation\": self.segmentation,\n            \"iscrowd\": self.iscrowd,\n            \"area\": self.area,\n        }\n\n    def serialize(self):\n        warnings.warn(\"Use json property instead of serialize method\", DeprecationWarning, stacklevel=2)\n        return self.json\n\n    def __repr__(self):\n        return f\"\"\"CocoAnnotation&lt;\n    image_id: {self.image_id},\n    bbox: {self.bbox},\n    segmentation: {self.segmentation},\n    category_id: {self.category_id},\n    category_name: {self.category_name},\n    iscrowd: {self.iscrowd},\n    area: {self.area}&gt;\"\"\"\n</code></pre> Attributes\u00b6 <code></code> <code>area</code> <code>property</code> \u00b6 <p>Returns area of annotation polygon (or bbox if no polygon available)</p> <code></code> <code>bbox</code> <code>property</code> \u00b6 <p>Returns coco formatted bbox of the annotation as [xmin, ymin, width, height]</p> <code></code> <code>category_id</code> <code>property</code> <code>writable</code> \u00b6 <p>Returns category id of the annotation as int.</p> <code></code> <code>category_name</code> <code>property</code> <code>writable</code> \u00b6 <p>Returns category name of the annotation as str.</p> <code></code> <code>image_id</code> <code>property</code> <code>writable</code> \u00b6 <p>Returns image id of the annotation as int.</p> <code></code> <code>iscrowd</code> <code>property</code> \u00b6 <p>Returns iscrowd info of the annotation.</p> <code></code> <code>segmentation</code> <code>property</code> \u00b6 <p>Returns coco formatted segmentation of the annotation as [[1, 1, 325, 125, 250, 200, 5, 200]]</p> Functions\u00b6 <code></code> <code>__init__(category_id, category_name=None, segmentation=None, bbox=None, image_id=None, iscrowd=0)</code> \u00b6 <p>Creates coco annotation object using bbox or segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>segmentation</code> \u00b6 <p>List[List][[1, 1, 325, 125, 250, 200, 5, 200]]</p> <code>None</code> <code>bbox</code> \u00b6 <code>list[int] | None</code> <p>List [xmin, ymin, width, height]</p> <code>None</code> <code>category_id</code> \u00b6 <code>int</code> <p>int Category id of the annotation</p> required <code>category_name</code> \u00b6 <code>str | None</code> <p>str Category name of the annotation</p> <code>None</code> <code>image_id</code> \u00b6 <p>int Image ID of the annotation</p> <code>None</code> <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(\n    self,\n    category_id: int,\n    category_name: str | None = None,\n    segmentation=None,\n    bbox: list[int] | None = None,\n    image_id=None,\n    iscrowd=0,\n):\n    \"\"\"Creates coco annotation object using bbox or segmentation.\n\n    Args:\n        segmentation: List[List]\n            [[1, 1, 325, 125, 250, 200, 5, 200]]\n        bbox: List\n            [xmin, ymin, width, height]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        image_id: int\n            Image ID of the annotation\n        iscrowd: int\n            0 or 1\n    \"\"\"\n    if bbox is None and segmentation is None:\n        raise ValueError(\"you must provide a bbox or polygon\")\n\n    self._segmentation = segmentation\n    self._category_id = category_id\n    self._category_name = category_name\n    self._image_id = image_id\n    self._iscrowd = iscrowd\n\n    if self._segmentation:\n        shapely_annotation = ShapelyAnnotation.from_coco_segmentation(segmentation=self._segmentation)\n    else:\n        if not bbox:\n            raise TypeError(\"Coco bounding box not set\")\n        shapely_annotation = ShapelyAnnotation.from_coco_bbox(bbox=bbox)\n    self._shapely_annotation = shapely_annotation\n</code></pre> <code></code> <code>from_coco_annotation_dict(annotation_dict, category_name=None)</code> <code>classmethod</code> \u00b6 <p>Creates CocoAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\").</p> <p>Parameters:</p> Name Type Description Default <code>category_name</code> \u00b6 <code>str | None</code> <p>str Category name of the annotation</p> <code>None</code> <code>annotation_dict</code> \u00b6 <code>dict</code> <p>dict COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_annotation_dict(cls, annotation_dict: dict, category_name: str | None = None):\n    \"\"\"Creates CocoAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\",\n    \"segmentation\", \"category_id\").\n\n    Args:\n        category_name: str\n            Category name of the annotation\n        annotation_dict: dict\n            COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")\n    \"\"\"\n    if annotation_dict.__contains__(\"segmentation\") and isinstance(annotation_dict[\"segmentation\"], dict):\n        has_rle_segmentation = True\n        logger.warning(\n            f\"Segmentation annotation for id {annotation_dict['id']} is skipped since \"\n            \"RLE segmentation format is not supported.\"\n        )\n    else:\n        has_rle_segmentation = False\n\n    if (\n        annotation_dict.__contains__(\"segmentation\")\n        and annotation_dict[\"segmentation\"]\n        and not has_rle_segmentation\n    ):\n        return cls(\n            segmentation=annotation_dict[\"segmentation\"],\n            category_id=annotation_dict[\"category_id\"],\n            category_name=category_name,\n        )\n    else:\n        return cls(\n            bbox=annotation_dict[\"bbox\"],\n            category_id=annotation_dict[\"category_id\"],\n            category_name=category_name,\n        )\n</code></pre> <code></code> <code>from_coco_bbox(bbox, category_id, category_name, iscrowd=0)</code> <code>classmethod</code> \u00b6 <p>Creates CocoAnnotation object using coco bbox.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> \u00b6 <p>List [xmin, ymin, width, height]</p> required <code>category_id</code> \u00b6 <p>int Category id of the annotation</p> required <code>category_name</code> \u00b6 <p>str Category name of the annotation</p> required <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_bbox(cls, bbox, category_id, category_name, iscrowd=0):\n    \"\"\"Creates CocoAnnotation object using coco bbox.\n\n    Args:\n        bbox: List\n            [xmin, ymin, width, height]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        iscrowd: int\n            0 or 1\n    \"\"\"\n    return cls(\n        bbox=bbox,\n        category_id=category_id,\n        category_name=category_name,\n        iscrowd=iscrowd,\n    )\n</code></pre> <code></code> <code>from_coco_segmentation(segmentation, category_id, category_name, iscrowd=0)</code> <code>classmethod</code> \u00b6 <p>Creates CocoAnnotation object using coco segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>segmentation</code> \u00b6 <p>List[List][[1, 1, 325, 125, 250, 200, 5, 200]]</p> required <code>category_id</code> \u00b6 <p>int Category id of the annotation</p> required <code>category_name</code> \u00b6 <p>str Category name of the annotation</p> required <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_segmentation(cls, segmentation, category_id, category_name, iscrowd=0):\n    \"\"\"Creates CocoAnnotation object using coco segmentation.\n\n    Args:\n        segmentation: List[List]\n            [[1, 1, 325, 125, 250, 200, 5, 200]]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        iscrowd: int\n            0 or 1\n    \"\"\"\n    return cls(\n        segmentation=segmentation,\n        category_id=category_id,\n        category_name=category_name,\n        iscrowd=iscrowd,\n    )\n</code></pre> <code></code> <code>from_shapely_annotation(shapely_annotation, category_id, category_name, iscrowd)</code> <code>classmethod</code> \u00b6 <p>Creates CocoAnnotation object from ShapelyAnnotation object.</p> <p>Parameters:</p> Name Type Description Default <code>category_id</code> \u00b6 <code>int</code> <p>Category id of the annotation</p> required <code>category_name</code> \u00b6 <code>str</code> <p>Category name of the annotation</p> required <code>iscrowd</code> \u00b6 <code>int</code> <p>0 or 1</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_shapely_annotation(\n    cls,\n    shapely_annotation: ShapelyAnnotation,\n    category_id: int,\n    category_name: str,\n    iscrowd: int,\n):\n    \"\"\"Creates CocoAnnotation object from ShapelyAnnotation object.\n\n    Args:\n        shapely_annotation (ShapelyAnnotation)\n        category_id (int): Category id of the annotation\n        category_name (str): Category name of the annotation\n        iscrowd (int): 0 or 1\n    \"\"\"\n    coco_annotation = cls(\n        bbox=[0, 0, 0, 0],\n        category_id=category_id,\n        category_name=category_name,\n        iscrowd=iscrowd,\n    )\n    coco_annotation._segmentation = shapely_annotation.to_coco_segmentation()\n    coco_annotation._shapely_annotation = shapely_annotation\n    return coco_annotation\n</code></pre> <code></code> <code>CocoCategory</code> \u00b6 <p>COCO formatted category.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoCategory:\n    \"\"\"COCO formatted category.\"\"\"\n\n    def __init__(self, id: int = 0, name: str | None = None, supercategory: str | None = None):\n        self.id = int(id)\n        self.name = name\n        self.supercategory = supercategory if supercategory else name\n\n    @classmethod\n    def from_coco_category(cls, category):\n        \"\"\"Creates CocoCategory object using coco category.\n\n        Args:\n            category: Dict\n                {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},\n        \"\"\"\n        return cls(\n            id=category[\"id\"],\n            name=category[\"name\"],\n            supercategory=category[\"supercategory\"] if \"supercategory\" in category else category[\"name\"],\n        )\n\n    @property\n    def json(self):\n        return {\n            \"id\": self.id,\n            \"name\": self.name,\n            \"supercategory\": self.supercategory,\n        }\n\n    def __repr__(self):\n        return f\"\"\"CocoCategory&lt;\n    id: {self.id},\n    name: {self.name},\n    supercategory: {self.supercategory}&gt;\"\"\"\n</code></pre> Functions\u00b6 <code></code> <code>from_coco_category(category)</code> <code>classmethod</code> \u00b6 <p>Creates CocoCategory object using coco category.</p> <p>Parameters:</p> Name Type Description Default <code>category</code> \u00b6 <p>Dict {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_category(cls, category):\n    \"\"\"Creates CocoCategory object using coco category.\n\n    Args:\n        category: Dict\n            {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},\n    \"\"\"\n    return cls(\n        id=category[\"id\"],\n        name=category[\"name\"],\n        supercategory=category[\"supercategory\"] if \"supercategory\" in category else category[\"name\"],\n    )\n</code></pre> <code></code> <code>CocoImage</code> \u00b6 Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoImage:\n    @classmethod\n    def from_coco_image_dict(cls, image_dict):\n        \"\"\"Creates CocoImage object from COCO formatted image dict (with fields \"id\", \"file_name\", \"height\" and\n        \"weight\").\n\n        Args:\n            image_dict: dict\n                COCO formatted image dict (with fields \"id\", \"file_name\", \"height\" and \"weight\")\n        \"\"\"\n        return cls(\n            id=image_dict[\"id\"],\n            file_name=image_dict[\"file_name\"],\n            height=image_dict[\"height\"],\n            width=image_dict[\"width\"],\n        )\n\n    def __init__(self, file_name: str, height: int, width: int, id: int | None = None):\n        \"\"\"Creates CocoImage object.\n\n        Args:\n            id : int\n                Image id\n            file_name : str\n                Image path\n            height : int\n                Image height in pixels\n            width : int\n                Image width in pixels\n        \"\"\"\n        self.id = int(id) if id else id\n        self.file_name = file_name\n        self.height = int(height)\n        self.width = int(width)\n        self.annotations = []  # list of CocoAnnotation that belong to this image\n        self.predictions = []  # list of CocoPrediction that belong to this image\n\n    def add_annotation(self, annotation):\n        \"\"\"Adds annotation to this CocoImage instance.\n\n        annotation : CocoAnnotation\n        \"\"\"\n\n        if not isinstance(annotation, CocoAnnotation):\n            raise TypeError(\"annotation must be a CocoAnnotation instance\")\n        self.annotations.append(annotation)\n\n    def add_prediction(self, prediction):\n        \"\"\"Adds prediction to this CocoImage instance.\n\n        prediction : CocoPrediction\n        \"\"\"\n\n        if not isinstance(prediction, CocoPrediction):\n            raise TypeError(\"prediction must be a CocoPrediction instance\")\n        self.predictions.append(prediction)\n\n    @property\n    def json(self):\n        return {\n            \"id\": self.id,\n            \"file_name\": self.file_name,\n            \"height\": self.height,\n            \"width\": self.width,\n        }\n\n    def __repr__(self):\n        return f\"\"\"CocoImage&lt;\n    id: {self.id},\n    file_name: {self.file_name},\n    height: {self.height},\n    width: {self.width},\n    annotations: List[CocoAnnotation],\n    predictions: List[CocoPrediction]&gt;\"\"\"\n</code></pre> Functions\u00b6 <code></code> <code>__init__(file_name, height, width, id=None)</code> \u00b6 <p>Creates CocoImage object.</p> <p>Parameters:</p> Name Type Description Default <code>id </code> \u00b6 <p>int Image id</p> required <code>file_name </code> \u00b6 <p>str Image path</p> required <code>height </code> \u00b6 <p>int Image height in pixels</p> required <code>width </code> \u00b6 <p>int Image width in pixels</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(self, file_name: str, height: int, width: int, id: int | None = None):\n    \"\"\"Creates CocoImage object.\n\n    Args:\n        id : int\n            Image id\n        file_name : str\n            Image path\n        height : int\n            Image height in pixels\n        width : int\n            Image width in pixels\n    \"\"\"\n    self.id = int(id) if id else id\n    self.file_name = file_name\n    self.height = int(height)\n    self.width = int(width)\n    self.annotations = []  # list of CocoAnnotation that belong to this image\n    self.predictions = []  # list of CocoPrediction that belong to this image\n</code></pre> <code></code> <code>add_annotation(annotation)</code> \u00b6 <p>Adds annotation to this CocoImage instance.</p> <p>annotation : CocoAnnotation</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_annotation(self, annotation):\n    \"\"\"Adds annotation to this CocoImage instance.\n\n    annotation : CocoAnnotation\n    \"\"\"\n\n    if not isinstance(annotation, CocoAnnotation):\n        raise TypeError(\"annotation must be a CocoAnnotation instance\")\n    self.annotations.append(annotation)\n</code></pre> <code></code> <code>add_prediction(prediction)</code> \u00b6 <p>Adds prediction to this CocoImage instance.</p> <p>prediction : CocoPrediction</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_prediction(self, prediction):\n    \"\"\"Adds prediction to this CocoImage instance.\n\n    prediction : CocoPrediction\n    \"\"\"\n\n    if not isinstance(prediction, CocoPrediction):\n        raise TypeError(\"prediction must be a CocoPrediction instance\")\n    self.predictions.append(prediction)\n</code></pre> <code></code> <code>from_coco_image_dict(image_dict)</code> <code>classmethod</code> \u00b6 <p>Creates CocoImage object from COCO formatted image dict (with fields \"id\", \"file_name\", \"height\" and \"weight\").</p> <p>Parameters:</p> Name Type Description Default <code>image_dict</code> \u00b6 <p>dict COCO formatted image dict (with fields \"id\", \"file_name\", \"height\" and \"weight\")</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_image_dict(cls, image_dict):\n    \"\"\"Creates CocoImage object from COCO formatted image dict (with fields \"id\", \"file_name\", \"height\" and\n    \"weight\").\n\n    Args:\n        image_dict: dict\n            COCO formatted image dict (with fields \"id\", \"file_name\", \"height\" and \"weight\")\n    \"\"\"\n    return cls(\n        id=image_dict[\"id\"],\n        file_name=image_dict[\"file_name\"],\n        height=image_dict[\"height\"],\n        width=image_dict[\"width\"],\n    )\n</code></pre> <code></code> <code>CocoPrediction</code> \u00b6 <p>               Bases: <code>CocoAnnotation</code></p> <p>Class for handling predictions in coco format.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoPrediction(CocoAnnotation):\n    \"\"\"Class for handling predictions in coco format.\"\"\"\n\n    @classmethod\n    def from_coco_segmentation(cls, segmentation, category_id, category_name, score, iscrowd=0, image_id=None):\n        \"\"\"Creates CocoAnnotation object using coco segmentation.\n\n        Args:\n            segmentation: List[List]\n                [[1, 1, 325, 125, 250, 200, 5, 200]]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            score: float\n                Prediction score between 0 and 1\n            iscrowd: int\n                0 or 1\n        \"\"\"\n        return cls(\n            segmentation=segmentation,\n            category_id=category_id,\n            category_name=category_name,\n            score=score,\n            iscrowd=iscrowd,\n            image_id=image_id,\n        )\n\n    @classmethod\n    def from_coco_bbox(cls, bbox, category_id, category_name, score, iscrowd=0, image_id=None):\n        \"\"\"Creates CocoAnnotation object using coco bbox.\n\n        Args:\n            bbox: List\n                [xmin, ymin, width, height]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            score: float\n                Prediction score between 0 and 1\n            iscrowd: int\n                0 or 1\n        \"\"\"\n        return cls(\n            bbox=bbox,\n            category_id=category_id,\n            category_name=category_name,\n            score=score,\n            iscrowd=iscrowd,\n            image_id=image_id,\n        )\n\n    @classmethod\n    def from_coco_annotation_dict(cls, category_name, annotation_dict, score, image_id=None):\n        \"\"\"Creates CocoAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\",\n        \"segmentation\", \"category_id\").\n\n        Args:\n            category_name: str\n                Category name of the annotation\n            annotation_dict: dict\n                COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")\n            score: float\n                Prediction score between 0 and 1\n        \"\"\"\n        if annotation_dict[\"segmentation\"]:\n            return cls(\n                segmentation=annotation_dict[\"segmentation\"],\n                category_id=annotation_dict[\"category_id\"],\n                category_name=category_name,\n                score=score,\n                image_id=image_id,\n            )\n        else:\n            return cls(\n                bbox=annotation_dict[\"bbox\"],\n                category_id=annotation_dict[\"category_id\"],\n                category_name=category_name,\n                image_id=image_id,\n            )\n\n    def __init__(\n        self,\n        segmentation=None,\n        bbox=None,\n        category_id: int = 0,\n        category_name: str = \"\",\n        image_id=None,\n        score=None,\n        iscrowd=0,\n    ):\n        \"\"\"\n\n        Args:\n            segmentation: List[List]\n                [[1, 1, 325, 125, 250, 200, 5, 200]]\n            bbox: List\n                [xmin, ymin, width, height]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            image_id: int\n                Image ID of the annotation\n            score: float\n                Prediction score between 0 and 1\n            iscrowd: int\n                0 or 1\n        \"\"\"\n        self.score = score\n        super().__init__(\n            segmentation=segmentation,\n            bbox=bbox,\n            category_id=category_id,\n            category_name=category_name,\n            image_id=image_id,\n            iscrowd=iscrowd,\n        )\n\n    @property\n    def json(self):\n        return {\n            \"image_id\": self.image_id,\n            \"bbox\": self.bbox,\n            \"score\": self.score,\n            \"category_id\": self.category_id,\n            \"category_name\": self.category_name,\n            \"segmentation\": self.segmentation,\n            \"iscrowd\": self.iscrowd,\n            \"area\": self.area,\n        }\n\n    def serialize(self):\n        warnings.warn(\"Use json property instead of serialize method\", DeprecationWarning, stacklevel=2)\n\n    def __repr__(self):\n        return f\"\"\"CocoPrediction&lt;\n    image_id: {self.image_id},\n    bbox: {self.bbox},\n    segmentation: {self.segmentation},\n    score: {self.score},\n    category_id: {self.category_id},\n    category_name: {self.category_name},\n    iscrowd: {self.iscrowd},\n    area: {self.area}&gt;\"\"\"\n</code></pre> Functions\u00b6 <code></code> <code>__init__(segmentation=None, bbox=None, category_id=0, category_name='', image_id=None, score=None, iscrowd=0)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>segmentation</code> \u00b6 <p>List[List][[1, 1, 325, 125, 250, 200, 5, 200]]</p> <code>None</code> <code>bbox</code> \u00b6 <p>List [xmin, ymin, width, height]</p> <code>None</code> <code>category_id</code> \u00b6 <code>int</code> <p>int Category id of the annotation</p> <code>0</code> <code>category_name</code> \u00b6 <code>str</code> <p>str Category name of the annotation</p> <code>''</code> <code>image_id</code> \u00b6 <p>int Image ID of the annotation</p> <code>None</code> <code>score</code> \u00b6 <p>float Prediction score between 0 and 1</p> <code>None</code> <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(\n    self,\n    segmentation=None,\n    bbox=None,\n    category_id: int = 0,\n    category_name: str = \"\",\n    image_id=None,\n    score=None,\n    iscrowd=0,\n):\n    \"\"\"\n\n    Args:\n        segmentation: List[List]\n            [[1, 1, 325, 125, 250, 200, 5, 200]]\n        bbox: List\n            [xmin, ymin, width, height]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        image_id: int\n            Image ID of the annotation\n        score: float\n            Prediction score between 0 and 1\n        iscrowd: int\n            0 or 1\n    \"\"\"\n    self.score = score\n    super().__init__(\n        segmentation=segmentation,\n        bbox=bbox,\n        category_id=category_id,\n        category_name=category_name,\n        image_id=image_id,\n        iscrowd=iscrowd,\n    )\n</code></pre> <code></code> <code>from_coco_annotation_dict(category_name, annotation_dict, score, image_id=None)</code> <code>classmethod</code> \u00b6 <p>Creates CocoAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\").</p> <p>Parameters:</p> Name Type Description Default <code>category_name</code> \u00b6 <p>str Category name of the annotation</p> required <code>annotation_dict</code> \u00b6 <p>dict COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")</p> required <code>score</code> \u00b6 <p>float Prediction score between 0 and 1</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_annotation_dict(cls, category_name, annotation_dict, score, image_id=None):\n    \"\"\"Creates CocoAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\",\n    \"segmentation\", \"category_id\").\n\n    Args:\n        category_name: str\n            Category name of the annotation\n        annotation_dict: dict\n            COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")\n        score: float\n            Prediction score between 0 and 1\n    \"\"\"\n    if annotation_dict[\"segmentation\"]:\n        return cls(\n            segmentation=annotation_dict[\"segmentation\"],\n            category_id=annotation_dict[\"category_id\"],\n            category_name=category_name,\n            score=score,\n            image_id=image_id,\n        )\n    else:\n        return cls(\n            bbox=annotation_dict[\"bbox\"],\n            category_id=annotation_dict[\"category_id\"],\n            category_name=category_name,\n            image_id=image_id,\n        )\n</code></pre> <code></code> <code>from_coco_bbox(bbox, category_id, category_name, score, iscrowd=0, image_id=None)</code> <code>classmethod</code> \u00b6 <p>Creates CocoAnnotation object using coco bbox.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> \u00b6 <p>List [xmin, ymin, width, height]</p> required <code>category_id</code> \u00b6 <p>int Category id of the annotation</p> required <code>category_name</code> \u00b6 <p>str Category name of the annotation</p> required <code>score</code> \u00b6 <p>float Prediction score between 0 and 1</p> required <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_bbox(cls, bbox, category_id, category_name, score, iscrowd=0, image_id=None):\n    \"\"\"Creates CocoAnnotation object using coco bbox.\n\n    Args:\n        bbox: List\n            [xmin, ymin, width, height]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        score: float\n            Prediction score between 0 and 1\n        iscrowd: int\n            0 or 1\n    \"\"\"\n    return cls(\n        bbox=bbox,\n        category_id=category_id,\n        category_name=category_name,\n        score=score,\n        iscrowd=iscrowd,\n        image_id=image_id,\n    )\n</code></pre> <code></code> <code>from_coco_segmentation(segmentation, category_id, category_name, score, iscrowd=0, image_id=None)</code> <code>classmethod</code> \u00b6 <p>Creates CocoAnnotation object using coco segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>segmentation</code> \u00b6 <p>List[List][[1, 1, 325, 125, 250, 200, 5, 200]]</p> required <code>category_id</code> \u00b6 <p>int Category id of the annotation</p> required <code>category_name</code> \u00b6 <p>str Category name of the annotation</p> required <code>score</code> \u00b6 <p>float Prediction score between 0 and 1</p> required <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_segmentation(cls, segmentation, category_id, category_name, score, iscrowd=0, image_id=None):\n    \"\"\"Creates CocoAnnotation object using coco segmentation.\n\n    Args:\n        segmentation: List[List]\n            [[1, 1, 325, 125, 250, 200, 5, 200]]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        score: float\n            Prediction score between 0 and 1\n        iscrowd: int\n            0 or 1\n    \"\"\"\n    return cls(\n        segmentation=segmentation,\n        category_id=category_id,\n        category_name=category_name,\n        score=score,\n        iscrowd=iscrowd,\n        image_id=image_id,\n    )\n</code></pre> <code></code> <code>CocoVid</code> \u00b6 Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoVid:\n    def __init__(self, name=None, remapping_dict=None):\n        \"\"\"Creates CocoVid object.\n\n        Args:\n            name: str\n                Name of the CocoVid dataset, it determines exported json name.\n            remapping_dict: dict\n                {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1\n        \"\"\"\n        self.name = name\n        self.remapping_dict = remapping_dict\n        self.categories = []\n        self.videos = []\n\n    def add_categories_from_coco_category_list(self, coco_category_list):\n        \"\"\"Creates CocoCategory object using coco category list.\n\n        Args:\n            coco_category_list: List[Dict]\n                [\n                    {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},\n                    {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"}\n                ]\n        \"\"\"\n\n        for coco_category in coco_category_list:\n            if self.remapping_dict is not None:\n                for source_id in self.remapping_dict.keys():\n                    if coco_category[\"id\"] == source_id:\n                        target_id = self.remapping_dict[source_id]\n                        coco_category[\"id\"] = target_id\n\n            self.add_category(CocoCategory.from_coco_category(coco_category))\n\n    def add_category(self, category: CocoCategory):\n        \"\"\"Adds category to this CocoVid instance.\n\n        Args:\n            category: CocoCategory\n        \"\"\"\n\n        if not isinstance(category, CocoCategory):\n            raise TypeError(\"category must be a CocoCategory instance\")  # type: ignore\n        self.categories.append(category)\n\n    @property\n    def json_categories(self):\n        categories = []\n        for category in self.categories:\n            categories.append(category.json)\n        return categories\n\n    @property\n    def category_mapping(self):\n        category_mapping = {}\n        for category in self.categories:\n            category_mapping[category.id] = category.name\n        return category_mapping\n\n    def add_video(self, video: CocoVideo):\n        \"\"\"Adds video to this CocoVid instance.\n\n        Args:\n            video: CocoVideo\n        \"\"\"\n\n        if not isinstance(video, CocoVideo):\n            raise TypeError(\"video must be a CocoVideo instance\")  # type: ignore\n        self.videos.append(video)\n\n    @property\n    def json(self):\n        coco_dict = {\n            \"videos\": [],\n            \"images\": [],\n            \"annotations\": [],\n            \"categories\": self.json_categories,\n        }\n        annotation_id = 1\n        image_id = 1\n        video_id = 1\n        global_instance_id = 1\n        for coco_video in self.videos:\n            coco_video.id = video_id\n            coco_dict[\"videos\"].append(coco_video.json)\n\n            frame_id = 0\n            instance_id_set = set()\n            for cocovid_image in coco_video.images:\n                cocovid_image.id = image_id\n                cocovid_image.frame_id = frame_id\n                cocovid_image.video_id = coco_video.id\n                coco_dict[\"images\"].append(cocovid_image.json)\n\n                for cocovid_annotation in cocovid_image.annotations:\n                    instance_id_set.add(cocovid_annotation.instance_id)\n                    cocovid_annotation.instance_id += global_instance_id\n\n                    cocovid_annotation.id = annotation_id\n                    cocovid_annotation.image_id = cocovid_image.id\n                    coco_dict[\"annotations\"].append(cocovid_annotation.json)\n\n                    # increment annotation_id\n                    annotation_id = copy.deepcopy(annotation_id + 1)\n                # increment image_id and frame_id\n                image_id = copy.deepcopy(image_id + 1)\n                frame_id = copy.deepcopy(frame_id + 1)\n            # increment video_id and global_instance_id\n            video_id = copy.deepcopy(video_id + 1)\n            global_instance_id += len(instance_id_set)\n\n        return coco_dict\n</code></pre> Functions\u00b6 <code></code> <code>__init__(name=None, remapping_dict=None)</code> \u00b6 <p>Creates CocoVid object.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> \u00b6 <p>str Name of the CocoVid dataset, it determines exported json name.</p> <code>None</code> <code>remapping_dict</code> \u00b6 <p>dict {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1</p> <code>None</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(self, name=None, remapping_dict=None):\n    \"\"\"Creates CocoVid object.\n\n    Args:\n        name: str\n            Name of the CocoVid dataset, it determines exported json name.\n        remapping_dict: dict\n            {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1\n    \"\"\"\n    self.name = name\n    self.remapping_dict = remapping_dict\n    self.categories = []\n    self.videos = []\n</code></pre> <code></code> <code>add_categories_from_coco_category_list(coco_category_list)</code> \u00b6 <p>Creates CocoCategory object using coco category list.</p> <p>Parameters:</p> Name Type Description Default <code>coco_category_list</code> \u00b6 <p>List[Dict] [     {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},     {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"} ]</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_categories_from_coco_category_list(self, coco_category_list):\n    \"\"\"Creates CocoCategory object using coco category list.\n\n    Args:\n        coco_category_list: List[Dict]\n            [\n                {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},\n                {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"}\n            ]\n    \"\"\"\n\n    for coco_category in coco_category_list:\n        if self.remapping_dict is not None:\n            for source_id in self.remapping_dict.keys():\n                if coco_category[\"id\"] == source_id:\n                    target_id = self.remapping_dict[source_id]\n                    coco_category[\"id\"] = target_id\n\n        self.add_category(CocoCategory.from_coco_category(coco_category))\n</code></pre> <code></code> <code>add_category(category)</code> \u00b6 <p>Adds category to this CocoVid instance.</p> <p>Parameters:</p> Name Type Description Default <code>category</code> \u00b6 <code>CocoCategory</code> <p>CocoCategory</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_category(self, category: CocoCategory):\n    \"\"\"Adds category to this CocoVid instance.\n\n    Args:\n        category: CocoCategory\n    \"\"\"\n\n    if not isinstance(category, CocoCategory):\n        raise TypeError(\"category must be a CocoCategory instance\")  # type: ignore\n    self.categories.append(category)\n</code></pre> <code></code> <code>add_video(video)</code> \u00b6 <p>Adds video to this CocoVid instance.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> \u00b6 <code>CocoVideo</code> <p>CocoVideo</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_video(self, video: CocoVideo):\n    \"\"\"Adds video to this CocoVid instance.\n\n    Args:\n        video: CocoVideo\n    \"\"\"\n\n    if not isinstance(video, CocoVideo):\n        raise TypeError(\"video must be a CocoVideo instance\")  # type: ignore\n    self.videos.append(video)\n</code></pre> <code></code> <code>CocoVidAnnotation</code> \u00b6 <p>               Bases: <code>CocoAnnotation</code></p> <p>COCOVid formatted annotation.</p> <p>https://github.com/open-mmlab/mmtracking/blob/master/docs/tutorials/customize_dataset.md#the-cocovid-annotation-file</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoVidAnnotation(CocoAnnotation):\n    \"\"\"COCOVid formatted annotation.\n\n    https://github.com/open-mmlab/mmtracking/blob/master/docs/tutorials/customize_dataset.md#the-cocovid-annotation-file\n    \"\"\"\n\n    def __init__(\n        self,\n        category_id: int,\n        category_name: str,\n        bbox: list[int],\n        image_id=None,\n        instance_id=None,\n        iscrowd=0,\n        id=None,\n    ):\n        \"\"\"\n        Args:\n            bbox: List\n                [xmin, ymin, width, height]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            image_id: int\n                Image ID of the annotation\n            instance_id: int\n                Used for tracking\n            iscrowd: int\n                0 or 1\n            id: int\n                Annotation id\n        \"\"\"\n        super().__init__(\n            bbox=bbox,\n            category_id=category_id,\n            category_name=category_name,\n            image_id=image_id,\n            iscrowd=iscrowd,\n        )\n        self.instance_id = instance_id\n        self.id = id\n\n    @property\n    def json(self):\n        return {\n            \"id\": self.id,\n            \"image_id\": self.image_id,\n            \"bbox\": self.bbox,\n            \"segmentation\": self.segmentation,\n            \"category_id\": self.category_id,\n            \"category_name\": self.category_name,\n            \"instance_id\": self.instance_id,\n            \"iscrowd\": self.iscrowd,\n            \"area\": self.area,\n        }\n\n    def __repr__(self):\n        return f\"\"\"CocoAnnotation&lt;\n    id: {self.id},\n    image_id: {self.image_id},\n    bbox: {self.bbox},\n    segmentation: {self.segmentation},\n    category_id: {self.category_id},\n    category_name: {self.category_name},\n    instance_id: {self.instance_id},\n    iscrowd: {self.iscrowd},\n    area: {self.area}&gt;\"\"\"\n</code></pre> Functions\u00b6 <code></code> <code>__init__(category_id, category_name, bbox, image_id=None, instance_id=None, iscrowd=0, id=None)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>bbox</code> \u00b6 <code>list[int]</code> <p>List [xmin, ymin, width, height]</p> required <code>category_id</code> \u00b6 <code>int</code> <p>int Category id of the annotation</p> required <code>category_name</code> \u00b6 <code>str</code> <p>str Category name of the annotation</p> required <code>image_id</code> \u00b6 <p>int Image ID of the annotation</p> <code>None</code> <code>instance_id</code> \u00b6 <p>int Used for tracking</p> <code>None</code> <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> <code>id</code> \u00b6 <p>int Annotation id</p> <code>None</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(\n    self,\n    category_id: int,\n    category_name: str,\n    bbox: list[int],\n    image_id=None,\n    instance_id=None,\n    iscrowd=0,\n    id=None,\n):\n    \"\"\"\n    Args:\n        bbox: List\n            [xmin, ymin, width, height]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        image_id: int\n            Image ID of the annotation\n        instance_id: int\n            Used for tracking\n        iscrowd: int\n            0 or 1\n        id: int\n            Annotation id\n    \"\"\"\n    super().__init__(\n        bbox=bbox,\n        category_id=category_id,\n        category_name=category_name,\n        image_id=image_id,\n        iscrowd=iscrowd,\n    )\n    self.instance_id = instance_id\n    self.id = id\n</code></pre> <code></code> <code>CocoVidImage</code> \u00b6 <p>               Bases: <code>CocoImage</code></p> <p>COCOVid formatted image.</p> <p>https://github.com/open-mmlab/mmtracking/blob/master/docs/tutorials/customize_dataset.md#the-cocovid-annotation-file</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoVidImage(CocoImage):\n    \"\"\"COCOVid formatted image.\n\n    https://github.com/open-mmlab/mmtracking/blob/master/docs/tutorials/customize_dataset.md#the-cocovid-annotation-file\n    \"\"\"\n\n    def __init__(\n        self,\n        file_name,\n        height,\n        width,\n        video_id=None,\n        frame_id=None,\n        id=None,\n    ):\n        \"\"\"Creates CocoVidImage object.\n\n        Args:\n            id: int\n                Image id\n            file_name: str\n                Image path\n            height: int\n                Image height in pixels\n            width: int\n                Image width in pixels\n            frame_id: int\n                0-indexed frame id\n            video_id: int\n                Video id\n        \"\"\"\n        super().__init__(file_name=file_name, height=height, width=width, id=id)\n        self.frame_id = frame_id\n        self.video_id = video_id\n\n    @classmethod\n    def from_coco_image(cls, coco_image, video_id=None, frame_id=None):\n        \"\"\"Creates CocoVidImage object using CocoImage object.\n\n        Args:\n            coco_image: CocoImage\n            frame_id: int\n                0-indexed frame id\n            video_id: int\n                Video id\n        \"\"\"\n        return cls(\n            file_name=coco_image.file_name,\n            height=coco_image.height,\n            width=coco_image.width,\n            id=coco_image.id,\n            video_id=video_id,\n            frame_id=frame_id,\n        )\n\n    def add_annotation(self, annotation):\n        \"\"\"\n        Adds annotation to this CocoImage instance\n        annotation : CocoVidAnnotation\n        \"\"\"\n\n        if not isinstance(annotation, CocoVidAnnotation):\n            raise TypeError(\"annotation must be a CocoVidAnnotation instance\")\n        self.annotations.append(annotation)\n\n    @property\n    def json(self):\n        return {\n            \"file_name\": self.file_name,\n            \"height\": self.height,\n            \"width\": self.width,\n            \"id\": self.id,\n            \"video_id\": self.video_id,\n            \"frame_id\": self.frame_id,\n        }\n\n    def __repr__(self):\n        return f\"\"\"CocoVidImage&lt;\n    file_name: {self.file_name},\n    height: {self.height},\n    width: {self.width},\n    id: {self.id},\n    video_id: {self.video_id},\n    frame_id: {self.frame_id},\n    annotations: List[CocoVidAnnotation]&gt;\"\"\"\n</code></pre> Functions\u00b6 <code></code> <code>__init__(file_name, height, width, video_id=None, frame_id=None, id=None)</code> \u00b6 <p>Creates CocoVidImage object.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> \u00b6 <p>int Image id</p> <code>None</code> <code>file_name</code> \u00b6 <p>str Image path</p> required <code>height</code> \u00b6 <p>int Image height in pixels</p> required <code>width</code> \u00b6 <p>int Image width in pixels</p> required <code>frame_id</code> \u00b6 <p>int 0-indexed frame id</p> <code>None</code> <code>video_id</code> \u00b6 <p>int Video id</p> <code>None</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(\n    self,\n    file_name,\n    height,\n    width,\n    video_id=None,\n    frame_id=None,\n    id=None,\n):\n    \"\"\"Creates CocoVidImage object.\n\n    Args:\n        id: int\n            Image id\n        file_name: str\n            Image path\n        height: int\n            Image height in pixels\n        width: int\n            Image width in pixels\n        frame_id: int\n            0-indexed frame id\n        video_id: int\n            Video id\n    \"\"\"\n    super().__init__(file_name=file_name, height=height, width=width, id=id)\n    self.frame_id = frame_id\n    self.video_id = video_id\n</code></pre> <code></code> <code>add_annotation(annotation)</code> \u00b6 <p>Adds annotation to this CocoImage instance annotation : CocoVidAnnotation</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_annotation(self, annotation):\n    \"\"\"\n    Adds annotation to this CocoImage instance\n    annotation : CocoVidAnnotation\n    \"\"\"\n\n    if not isinstance(annotation, CocoVidAnnotation):\n        raise TypeError(\"annotation must be a CocoVidAnnotation instance\")\n    self.annotations.append(annotation)\n</code></pre> <code></code> <code>from_coco_image(coco_image, video_id=None, frame_id=None)</code> <code>classmethod</code> \u00b6 <p>Creates CocoVidImage object using CocoImage object.</p> <p>Parameters:</p> Name Type Description Default <code>coco_image</code> \u00b6 <p>CocoImage</p> required <code>frame_id</code> \u00b6 <p>int 0-indexed frame id</p> <code>None</code> <code>video_id</code> \u00b6 <p>int Video id</p> <code>None</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_image(cls, coco_image, video_id=None, frame_id=None):\n    \"\"\"Creates CocoVidImage object using CocoImage object.\n\n    Args:\n        coco_image: CocoImage\n        frame_id: int\n            0-indexed frame id\n        video_id: int\n            Video id\n    \"\"\"\n    return cls(\n        file_name=coco_image.file_name,\n        height=coco_image.height,\n        width=coco_image.width,\n        id=coco_image.id,\n        video_id=video_id,\n        frame_id=frame_id,\n    )\n</code></pre> <code></code> <code>CocoVideo</code> \u00b6 <p>COCO formatted video.</p> <p>https://github.com/open-mmlab/mmtracking/blob/master/docs/tutorials/customize_dataset.md#the-cocovid-annotation-file</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoVideo:\n    \"\"\"COCO formatted video.\n\n    https://github.com/open-mmlab/mmtracking/blob/master/docs/tutorials/customize_dataset.md#the-cocovid-annotation-file\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        id: int | None = None,\n        fps: float | None = None,\n        height: int | None = None,\n        width: int | None = None,\n    ):\n        \"\"\"Creates CocoVideo object.\n\n        Args:\n            name: str\n                Video name\n            id: int\n                Video id\n            fps: float\n                Video fps\n            height: int\n                Video height in pixels\n            width: int\n                Video width in pixels\n        \"\"\"\n        self.name = name\n        self.id = id\n        self.fps = fps\n        self.height = height\n        self.width = width\n        self.images = []  # list of CocoImage that belong to this video\n\n    def add_image(self, image):\n        \"\"\"\n        Adds image to this CocoVideo instance\n        Args:\n            image: CocoImage\n        \"\"\"\n\n        if not isinstance(image, CocoImage):\n            raise TypeError(\"image must be a CocoImage instance\")\n        self.images.append(CocoVidImage.from_coco_image(image))\n\n    def add_cocovidimage(self, cocovidimage):\n        \"\"\"\n        Adds CocoVidImage to this CocoVideo instance\n        Args:\n            cocovidimage: CocoVidImage\n        \"\"\"\n\n        if not isinstance(cocovidimage, CocoVidImage):\n            raise TypeError(\"cocovidimage must be a CocoVidImage instance\")\n        self.images.append(cocovidimage)\n\n    @property\n    def json(self):\n        return {\n            \"name\": self.name,\n            \"id\": self.id,\n            \"fps\": self.fps,\n            \"height\": self.height,\n            \"width\": self.width,\n        }\n\n    def __repr__(self):\n        return f\"\"\"CocoVideo&lt;\n    id: {self.id},\n    name: {self.name},\n    fps: {self.fps},\n    height: {self.height},\n    width: {self.width},\n    images: List[CocoVidImage]&gt;\"\"\"\n</code></pre> Functions\u00b6 <code></code> <code>__init__(name, id=None, fps=None, height=None, width=None)</code> \u00b6 <p>Creates CocoVideo object.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> \u00b6 <code>str</code> <p>str Video name</p> required <code>id</code> \u00b6 <code>int | None</code> <p>int Video id</p> <code>None</code> <code>fps</code> \u00b6 <code>float | None</code> <p>float Video fps</p> <code>None</code> <code>height</code> \u00b6 <code>int | None</code> <p>int Video height in pixels</p> <code>None</code> <code>width</code> \u00b6 <code>int | None</code> <p>int Video width in pixels</p> <code>None</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    id: int | None = None,\n    fps: float | None = None,\n    height: int | None = None,\n    width: int | None = None,\n):\n    \"\"\"Creates CocoVideo object.\n\n    Args:\n        name: str\n            Video name\n        id: int\n            Video id\n        fps: float\n            Video fps\n        height: int\n            Video height in pixels\n        width: int\n            Video width in pixels\n    \"\"\"\n    self.name = name\n    self.id = id\n    self.fps = fps\n    self.height = height\n    self.width = width\n    self.images = []  # list of CocoImage that belong to this video\n</code></pre> <code></code> <code>add_cocovidimage(cocovidimage)</code> \u00b6 <p>Adds CocoVidImage to this CocoVideo instance Args:     cocovidimage: CocoVidImage</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_cocovidimage(self, cocovidimage):\n    \"\"\"\n    Adds CocoVidImage to this CocoVideo instance\n    Args:\n        cocovidimage: CocoVidImage\n    \"\"\"\n\n    if not isinstance(cocovidimage, CocoVidImage):\n        raise TypeError(\"cocovidimage must be a CocoVidImage instance\")\n    self.images.append(cocovidimage)\n</code></pre> <code></code> <code>add_image(image)</code> \u00b6 <p>Adds image to this CocoVideo instance Args:     image: CocoImage</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_image(self, image):\n    \"\"\"\n    Adds image to this CocoVideo instance\n    Args:\n        image: CocoImage\n    \"\"\"\n\n    if not isinstance(image, CocoImage):\n        raise TypeError(\"image must be a CocoImage instance\")\n    self.images.append(CocoVidImage.from_coco_image(image))\n</code></pre> <code></code> <code>DatasetClassCounts</code> <code>dataclass</code> \u00b6 <p>Stores the number of images that include each category in a dataset.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>@dataclass\nclass DatasetClassCounts:\n    \"\"\"Stores the number of images that include each category in a dataset.\"\"\"\n\n    counts: dict\n    total_images: int\n\n    def frequencies(self):\n        \"\"\"Calculates the frequency of images that contain each category.\"\"\"\n        return {cid: count / self.total_images for cid, count in self.counts.items()}\n\n    def __add__(self, o):\n        total = self.total_images + o.total_images\n        exclusive_keys = set(o.counts.keys()) - set(self.counts.keys())\n        counts = {}\n        for k, v in self.counts.items():\n            counts[k] = v + o.counts.get(k, 0)\n        for k in exclusive_keys:\n            counts[k] = o.counts[k]\n        return DatasetClassCounts(counts, total)\n</code></pre> Functions\u00b6 <code></code> <code>frequencies()</code> \u00b6 <p>Calculates the frequency of images that contain each category.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def frequencies(self):\n    \"\"\"Calculates the frequency of images that contain each category.\"\"\"\n    return {cid: count / self.total_images for cid, count in self.counts.items()}\n</code></pre> Functions\u00b6 <code></code> <code>add_bbox_and_area_to_coco(source_coco_path='', target_coco_path='', add_bbox=True, add_area=True)</code> \u00b6 <p>Takes single coco dataset file path, calculates and fills bbox and area fields of the annotations and exports the updated coco dict.</p> <p>coco_dict : dict     Updated coco dict</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_bbox_and_area_to_coco(\n    source_coco_path: str = \"\",\n    target_coco_path: str = \"\",\n    add_bbox: bool = True,\n    add_area: bool = True,\n) -&gt; dict:\n    \"\"\"Takes single coco dataset file path, calculates and fills bbox and area fields of the annotations and exports the\n    updated coco dict.\n\n    Returns:\n    coco_dict : dict\n        Updated coco dict\n    \"\"\"\n    coco_dict = load_json(source_coco_path)\n    coco_dict = copy.deepcopy(coco_dict)\n\n    annotations = coco_dict[\"annotations\"]\n    for ind, annotation in enumerate(annotations):\n        # assign annotation bbox\n        if add_bbox:\n            coco_polygons = []\n            [coco_polygons.extend(coco_polygon) for coco_polygon in annotation[\"segmentation\"]]\n            minx, miny, maxx, maxy = list(\n                [\n                    min(coco_polygons[0::2]),\n                    min(coco_polygons[1::2]),\n                    max(coco_polygons[0::2]),\n                    max(coco_polygons[1::2]),\n                ]\n            )\n            x, y, width, height = (\n                minx,\n                miny,\n                maxx - minx,\n                maxy - miny,\n            )\n            annotations[ind][\"bbox\"] = [x, y, width, height]\n\n        # assign annotation area\n        if add_area:\n            shapely_multipolygon = get_shapely_multipolygon(coco_segmentation=annotation[\"segmentation\"])\n            annotations[ind][\"area\"] = shapely_multipolygon.area\n\n    coco_dict[\"annotations\"] = annotations\n    save_json(coco_dict, target_coco_path)\n    return coco_dict\n</code></pre> <code></code> <code>count_images_with_category(coco_file_path)</code> \u00b6 <p>Reads a coco dataset file and returns an DatasetClassCounts object  that stores the number of images that include each category in a dataset Returns: DatasetClassCounts object coco_file_path : str     path to coco dataset file</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def count_images_with_category(coco_file_path):\n    \"\"\"Reads a coco dataset file and returns an DatasetClassCounts object\n     that stores the number of images that include each category in a dataset\n    Returns: DatasetClassCounts object\n    coco_file_path : str\n        path to coco dataset file\n    \"\"\"\n\n    image_id_2_category_2_count = defaultdict(lambda: defaultdict(int))\n    coco = load_json(coco_file_path)\n    for annotation in coco[\"annotations\"]:\n        image_id = annotation[\"image_id\"]\n        cid = annotation[\"category_id\"]\n        image_id_2_category_2_count[image_id][cid] = image_id_2_category_2_count[image_id][cid] + 1\n\n    category_2_count = defaultdict(int)\n    for image_id, image_category_2_count in image_id_2_category_2_count.items():\n        for cid, count in image_category_2_count.items():\n            if count &gt; 0:\n                category_2_count[cid] = category_2_count[cid] + 1\n\n    category_2_count = dict(category_2_count)\n    total_images = len(image_id_2_category_2_count.keys())\n    return DatasetClassCounts(category_2_count, total_images)\n</code></pre> <code></code> <code>create_coco_dict(images, categories, ignore_negative_samples=False, image_id_setting='auto')</code> \u00b6 <p>Creates COCO dict with fields \"images\", \"annotations\", \"categories\".</p> <p>Args</p> <pre><code>images : List of CocoImage containing a list of CocoAnnotation\ncategories : List of Dict\n    COCO categories\nignore_negative_samples : Bool\n    If True, images without annotations are ignored\nimage_id_setting: str\n    how to assign image ids while exporting can be\n        auto --&gt; will assign id from scratch (&lt;CocoImage&gt;.id will be ignored)\n        manual --&gt; you will need to provide image ids in &lt;CocoImage&gt; instances (&lt;CocoImage&gt;.id can not be None)\n</code></pre> <p>Returns</p> <pre><code>coco_dict : Dict\n    COCO dict with fields \"images\", \"annotations\", \"categories\"\n</code></pre> Source code in <code>sahi/utils/coco.py</code> <pre><code>def create_coco_dict(images, categories, ignore_negative_samples=False, image_id_setting=\"auto\"):\n    \"\"\"Creates COCO dict with fields \"images\", \"annotations\", \"categories\".\n\n    Args\n\n        images : List of CocoImage containing a list of CocoAnnotation\n        categories : List of Dict\n            COCO categories\n        ignore_negative_samples : Bool\n            If True, images without annotations are ignored\n        image_id_setting: str\n            how to assign image ids while exporting can be\n                auto --&gt; will assign id from scratch (&lt;CocoImage&gt;.id will be ignored)\n                manual --&gt; you will need to provide image ids in &lt;CocoImage&gt; instances (&lt;CocoImage&gt;.id can not be None)\n    Returns\n\n        coco_dict : Dict\n            COCO dict with fields \"images\", \"annotations\", \"categories\"\n    \"\"\"\n    # assertion of parameters\n    if image_id_setting not in [\"auto\", \"manual\"]:\n        raise ValueError(\"'image_id_setting' should be one of ['auto', 'manual']\")\n\n    # define accumulators\n    image_index = 1\n    annotation_id = 1\n    coco_dict = dict(images=[], annotations=[], categories=categories)\n    for coco_image in images:\n        # get coco annotations\n        coco_annotations = coco_image.annotations\n        # get num annotations\n        num_annotations = len(coco_annotations)\n        # if ignore_negative_samples is True and no annotations, skip image\n        if ignore_negative_samples and num_annotations == 0:\n            continue\n        else:\n            # get image_id\n            if image_id_setting == \"auto\":\n                image_id = image_index\n                image_index += 1\n            elif image_id_setting == \"manual\":\n                if coco_image.id is None:\n                    raise ValueError(\"'coco_image.id' should be set manually when image_id_setting == 'manual'\")\n                image_id = coco_image.id\n\n            # create coco image object\n            out_image = {\n                \"height\": coco_image.height,\n                \"width\": coco_image.width,\n                \"id\": image_id,\n                \"file_name\": coco_image.file_name,\n            }\n            coco_dict[\"images\"].append(out_image)\n\n            # do the same for image annotations\n            for coco_annotation in coco_annotations:\n                # create coco annotation object\n                out_annotation = {\n                    \"iscrowd\": 0,\n                    \"image_id\": image_id,\n                    \"bbox\": coco_annotation.bbox,\n                    \"segmentation\": coco_annotation.segmentation,\n                    \"category_id\": coco_annotation.category_id,\n                    \"id\": annotation_id,\n                    \"area\": coco_annotation.area,\n                }\n                coco_dict[\"annotations\"].append(out_annotation)\n                # increment annotation id\n                annotation_id += 1\n\n    # return coco dict\n    return coco_dict\n</code></pre> <code></code> <code>create_coco_prediction_array(images, ignore_negative_samples=False, image_id_setting='auto')</code> \u00b6 <p>Creates COCO prediction array which is list of predictions.</p> <p>Args</p> <pre><code>images : List of CocoImage containing a list of CocoAnnotation\nignore_negative_samples : Bool\n    If True, images without predictions are ignored\nimage_id_setting: str\n    how to assign image ids while exporting can be\n        auto --&gt; will assign id from scratch (&lt;CocoImage&gt;.id will be ignored)\n        manual --&gt; you will need to provide image ids in &lt;CocoImage&gt; instances (&lt;CocoImage&gt;.id can not be None)\n</code></pre> <p>Returns</p> <pre><code>coco_prediction_array : List\n    COCO predictions array\n</code></pre> Source code in <code>sahi/utils/coco.py</code> <pre><code>def create_coco_prediction_array(images, ignore_negative_samples=False, image_id_setting=\"auto\"):\n    \"\"\"Creates COCO prediction array which is list of predictions.\n\n    Args\n\n        images : List of CocoImage containing a list of CocoAnnotation\n        ignore_negative_samples : Bool\n            If True, images without predictions are ignored\n        image_id_setting: str\n            how to assign image ids while exporting can be\n                auto --&gt; will assign id from scratch (&lt;CocoImage&gt;.id will be ignored)\n                manual --&gt; you will need to provide image ids in &lt;CocoImage&gt; instances (&lt;CocoImage&gt;.id can not be None)\n    Returns\n\n        coco_prediction_array : List\n            COCO predictions array\n    \"\"\"\n    # assertion of parameters\n    if image_id_setting not in [\"auto\", \"manual\"]:\n        raise ValueError(\"'image_id_setting' should be one of ['auto', 'manual']\")\n    # define accumulators\n    image_index = 1\n    prediction_id = 1\n    predictions_array = []\n    for coco_image in images:\n        # get coco predictions\n        coco_predictions = coco_image.predictions\n        # get num predictions\n        num_predictions = len(coco_predictions)\n        # if ignore_negative_samples is True and no annotations, skip image\n        if ignore_negative_samples and num_predictions == 0:\n            continue\n        else:\n            # get image_id\n            if image_id_setting == \"auto\":\n                image_id = image_index\n                image_index += 1\n            elif image_id_setting == \"manual\":\n                if coco_image.id is None:\n                    raise ValueError(\"'coco_image.id' should be set manually when image_id_setting == 'manual'\")\n                image_id = coco_image.id\n\n            # create coco prediction object\n            for prediction_index, coco_prediction in enumerate(coco_predictions):\n                # create coco prediction object\n                out_prediction = {\n                    \"id\": prediction_id,\n                    \"image_id\": image_id,\n                    \"bbox\": coco_prediction.bbox,\n                    \"score\": coco_prediction.score,\n                    \"category_id\": coco_prediction.category_id,\n                    \"segmentation\": coco_prediction.segmentation,\n                    \"iscrowd\": coco_prediction.iscrowd,\n                    \"area\": coco_prediction.area,\n                }\n                predictions_array.append(out_prediction)\n\n                # increment prediction id\n                prediction_id += 1\n\n    # return predictions array\n    return predictions_array\n</code></pre> <code></code> <code>export_coco_as_yolo(output_dir, train_coco=None, val_coco=None, train_split_rate=0.9, numpy_seed=0, disable_symlink=False)</code> \u00b6 <p>Exports current COCO dataset in ultralytics/YOLO format. Creates train val folders with image symlinks and txt files and a data yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> \u00b6 <code>str</code> <p>str Export directory.</p> required <code>train_coco</code> \u00b6 <code>Coco | None</code> <p>Coco coco object for training</p> <code>None</code> <code>val_coco</code> \u00b6 <code>Coco | None</code> <p>Coco coco object for val</p> <code>None</code> <code>train_split_rate</code> \u00b6 <code>float</code> <p>float train split rate between 0 and 1. will be used when val_coco is None.</p> <code>0.9</code> <code>numpy_seed</code> \u00b6 <p>int To fix the numpy seed.</p> <code>0</code> <code>disable_symlink</code> \u00b6 <p>bool If True, copy images instead of creating symlinks.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>yaml_path</code> <p>str Path for the exported YOLO data.yml</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_coco_as_yolo(\n    output_dir: str,\n    train_coco: Coco | None = None,\n    val_coco: Coco | None = None,\n    train_split_rate: float = 0.9,\n    numpy_seed=0,\n    disable_symlink=False,\n):\n    \"\"\"Exports current COCO dataset in ultralytics/YOLO format. Creates train val folders with image symlinks and txt\n    files and a data yaml file.\n\n    Args:\n        output_dir: str\n            Export directory.\n        train_coco: Coco\n            coco object for training\n        val_coco: Coco\n            coco object for val\n        train_split_rate: float\n            train split rate between 0 and 1. will be used when val_coco is None.\n        numpy_seed: int\n            To fix the numpy seed.\n        disable_symlink: bool\n            If True, copy images instead of creating symlinks.\n\n    Returns:\n        yaml_path: str\n            Path for the exported YOLO data.yml\n    \"\"\"\n    try:\n        import yaml\n    except ImportError:\n        raise ImportError('Please run \"pip install -U pyyaml\" to install yaml first for YOLO formatted exporting.')\n\n    # set split_mode\n    if train_coco and not val_coco:\n        split_mode = True\n    elif train_coco and val_coco:\n        split_mode = False\n    else:\n        raise ValueError(\"'train_coco' have to be provided\")\n\n    # check train_split_rate\n    if split_mode and not (0 &lt; train_split_rate &lt; 1):\n        raise ValueError(\"train_split_rate cannot be &lt;0 or &gt;1\")\n\n    # split dataset\n    if split_mode:\n        result = train_coco.split_coco_as_train_val(\n            train_split_rate=train_split_rate,\n            numpy_seed=numpy_seed,\n        )\n        train_coco = result[\"train_coco\"]\n        val_coco = result[\"val_coco\"]\n\n    # create train val image dirs\n    train_dir = Path(os.path.abspath(output_dir)) / \"train/\"\n    train_dir.mkdir(parents=True, exist_ok=True)  # create dir\n    val_dir = Path(os.path.abspath(output_dir)) / \"val/\"\n    val_dir.mkdir(parents=True, exist_ok=True)  # create dir\n\n    # create image symlinks and annotation txts\n    export_yolo_images_and_txts_from_coco_object(\n        output_dir=train_dir,\n        coco=train_coco,\n        ignore_negative_samples=train_coco.ignore_negative_samples,\n        mp=False,\n        disable_symlink=disable_symlink,\n    )\n    assert val_coco, \"Validation Coco object not set\"\n    export_yolo_images_and_txts_from_coco_object(\n        output_dir=val_dir,\n        coco=val_coco,\n        ignore_negative_samples=val_coco.ignore_negative_samples,\n        mp=False,\n        disable_symlink=disable_symlink,\n    )\n\n    # create yolov5 data yaml\n    data = {\n        \"train\": str(train_dir).replace(\"\\\\\", \"/\"),\n        \"val\": str(val_dir).replace(\"\\\\\", \"/\"),\n        \"nc\": len(train_coco.category_mapping),\n        \"names\": list(train_coco.category_mapping.values()),\n    }\n    yaml_path = str(Path(output_dir) / \"data.yml\")\n    with open(yaml_path, \"w\") as outfile:\n        yaml.dump(data, outfile, default_flow_style=False)\n\n    return yaml_path\n</code></pre> <code></code> <code>export_coco_as_yolo_via_yml(yml_path, output_dir, train_split_rate=0.9, numpy_seed=0, disable_symlink=False)</code> \u00b6 <p>Exports current COCO dataset in ultralytics/YOLO format. Creates train val folders with image symlinks and txt files and a data yaml file. Uses a yml file as input.</p> <p>Parameters:</p> Name Type Description Default <code>yml_path</code> \u00b6 <code>str</code> <p>str file should contain these fields:     train_json_path: str     train_image_dir: str     val_json_path: str     val_image_dir: str</p> required <code>output_dir</code> \u00b6 <code>str</code> <p>str Export directory.</p> required <code>train_split_rate</code> \u00b6 <code>float</code> <p>float train split rate between 0 and 1. will be used when val_json_path is None.</p> <code>0.9</code> <code>numpy_seed</code> \u00b6 <p>int To fix the numpy seed.</p> <code>0</code> <code>disable_symlink</code> \u00b6 <p>bool If True, copy images instead of creating symlinks.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>yaml_path</code> <p>str Path for the exported YOLO data.yml</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_coco_as_yolo_via_yml(\n    yml_path: str, output_dir: str, train_split_rate: float = 0.9, numpy_seed=0, disable_symlink=False\n):\n    \"\"\"Exports current COCO dataset in ultralytics/YOLO format. Creates train val folders with image symlinks and txt\n    files and a data yaml file. Uses a yml file as input.\n\n    Args:\n        yml_path: str\n            file should contain these fields:\n                train_json_path: str\n                train_image_dir: str\n                val_json_path: str\n                val_image_dir: str\n        output_dir: str\n            Export directory.\n        train_split_rate: float\n            train split rate between 0 and 1. will be used when val_json_path is None.\n        numpy_seed: int\n            To fix the numpy seed.\n        disable_symlink: bool\n            If True, copy images instead of creating symlinks.\n\n    Returns:\n        yaml_path: str\n            Path for the exported YOLO data.yml\n    \"\"\"\n    try:\n        import yaml\n    except ImportError:\n        raise ImportError('Please run \"pip install -U pyyaml\" to install yaml first for YOLO formatted exporting.')\n\n    with open(yml_path) as stream:\n        config_dict = yaml.safe_load(stream)\n\n    if config_dict[\"train_json_path\"]:\n        if not config_dict[\"train_image_dir\"]:\n            raise ValueError(f\"{yml_path} is missing `train_image_dir`\")\n        train_coco = Coco.from_coco_dict_or_path(\n            config_dict[\"train_json_path\"], image_dir=config_dict[\"train_image_dir\"]\n        )\n    else:\n        train_coco = None\n\n    if config_dict[\"val_json_path\"]:\n        if not config_dict[\"val_image_dir\"]:\n            raise ValueError(f\"{yml_path} is missing `val_image_dir`\")\n        val_coco = Coco.from_coco_dict_or_path(config_dict[\"val_json_path\"], image_dir=config_dict[\"val_image_dir\"])\n    else:\n        val_coco = None\n\n    yaml_path = export_coco_as_yolo(\n        output_dir=output_dir,\n        train_coco=train_coco,\n        val_coco=val_coco,\n        train_split_rate=train_split_rate,\n        numpy_seed=numpy_seed,\n        disable_symlink=disable_symlink,\n    )\n\n    return yaml_path\n</code></pre> <code></code> <code>export_coco_as_yolov5(output_dir, train_coco=None, val_coco=None, train_split_rate=0.9, numpy_seed=0, disable_symlink=False)</code> \u00b6 <p>Deprecated.</p> <p>Please use export_coco_as_yolo instead. Calls export_coco_as_yolo with the same arguments.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_coco_as_yolov5(\n    output_dir: str,\n    train_coco: Coco | None = None,\n    val_coco: Coco | None = None,\n    train_split_rate: float = 0.9,\n    numpy_seed=0,\n    disable_symlink=False,\n):\n    \"\"\"Deprecated.\n\n    Please use export_coco_as_yolo instead. Calls export_coco_as_yolo with the same arguments.\n    \"\"\"\n    warnings.warn(\n        \"export_coco_as_yolov5 is deprecated. Please use export_coco_as_yolo instead.\",\n        DeprecationWarning,\n    )\n    export_coco_as_yolo(\n        output_dir=output_dir,\n        train_coco=train_coco,\n        val_coco=val_coco,\n        train_split_rate=train_split_rate,\n        numpy_seed=numpy_seed,\n        disable_symlink=disable_symlink,\n    )\n</code></pre> <code></code> <code>export_coco_as_yolov5_via_yml(yml_path, output_dir, train_split_rate=0.9, numpy_seed=0, disable_symlink=False)</code> \u00b6 <p>Deprecated.</p> <p>Please use export_coco_as_yolo_via_yml instead. Calls export_coco_as_yolo_via_yml with the same arguments.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_coco_as_yolov5_via_yml(\n    yml_path: str, output_dir: str, train_split_rate: float = 0.9, numpy_seed=0, disable_symlink=False\n):\n    \"\"\"Deprecated.\n\n    Please use export_coco_as_yolo_via_yml instead. Calls export_coco_as_yolo_via_yml with the same arguments.\n    \"\"\"\n    warnings.warn(\n        \"export_coco_as_yolov5_via_yml is deprecated. Please use export_coco_as_yolo_via_yml instead.\",\n        DeprecationWarning,\n    )\n    export_coco_as_yolo_via_yml(\n        yml_path=yml_path,\n        output_dir=output_dir,\n        train_split_rate=train_split_rate,\n        numpy_seed=numpy_seed,\n        disable_symlink=disable_symlink,\n    )\n</code></pre> <code></code> <code>export_single_yolo_image_and_corresponding_txt(coco_image, coco_image_dir, output_dir, ignore_negative_samples=False, disable_symlink=False)</code> \u00b6 <p>Generates YOLO formatted image symlink and annotation txt file.</p> <p>Parameters:</p> Name Type Description Default <code>coco_image</code> \u00b6 <p>sahi.utils.coco.CocoImage</p> required <code>coco_image_dir</code> \u00b6 <p>str</p> required <code>output_dir</code> \u00b6 <p>str Export directory.</p> required <code>ignore_negative_samples</code> \u00b6 <p>bool If True ignores images without annotations in all operations.</p> <code>False</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_single_yolo_image_and_corresponding_txt(\n    coco_image, coco_image_dir, output_dir, ignore_negative_samples=False, disable_symlink=False\n):\n    \"\"\"Generates YOLO formatted image symlink and annotation txt file.\n\n    Args:\n        coco_image: sahi.utils.coco.CocoImage\n        coco_image_dir: str\n        output_dir: str\n            Export directory.\n        ignore_negative_samples: bool\n            If True ignores images without annotations in all operations.\n    \"\"\"\n    # if coco_image contains any invalid annotations, skip it\n    contains_invalid_annotations = False\n    for coco_annotation in coco_image.annotations:\n        if len(coco_annotation.bbox) != 4:\n            contains_invalid_annotations = True\n            break\n    if contains_invalid_annotations:\n        return\n    # skip images without annotations\n    if len(coco_image.annotations) == 0 and ignore_negative_samples:\n        return\n    # skip images without suffix\n    # https://github.com/obss/sahi/issues/114\n    if Path(coco_image.file_name).suffix == \"\":\n        print(f\"image file has no suffix, skipping it: '{coco_image.file_name}'\")\n        return\n    elif Path(coco_image.file_name).suffix in [\".txt\"]:  # TODO: extend this list\n        print(f\"image file has incorrect suffix, skipping it: '{coco_image.file_name}'\")\n        return\n    # set coco and yolo image paths\n    if Path(coco_image.file_name).is_file():\n        coco_image_path = os.path.abspath(coco_image.file_name)\n    else:\n        if coco_image_dir is None:\n            raise ValueError(\"You have to specify image_dir of Coco object for yolo conversion.\")\n\n        coco_image_path = os.path.abspath(str(Path(coco_image_dir) / coco_image.file_name))\n\n    yolo_image_path_temp = str(Path(output_dir) / Path(coco_image.file_name).name)\n    # increment target file name if already present\n    yolo_image_path = copy.deepcopy(yolo_image_path_temp)\n    name_increment = 2\n    while Path(yolo_image_path).is_file():\n        parent_dir = Path(yolo_image_path_temp).parent\n        filename = Path(yolo_image_path_temp).stem\n        filesuffix = Path(yolo_image_path_temp).suffix\n        filename = filename + \"_\" + str(name_increment)\n        yolo_image_path = str(parent_dir / (filename + filesuffix))\n        name_increment += 1\n    # create a symbolic link pointing to coco_image_path named yolo_image_path\n    if disable_symlink:\n        import shutil\n\n        shutil.copy(coco_image_path, yolo_image_path)\n    else:\n        os.symlink(coco_image_path, yolo_image_path)\n    # calculate annotation normalization ratios\n    width = coco_image.width\n    height = coco_image.height\n    dw = 1.0 / (width)\n    dh = 1.0 / (height)\n    # set annotation filepath\n    image_file_suffix = Path(yolo_image_path).suffix\n    yolo_annotation_path = yolo_image_path.replace(image_file_suffix, \".txt\")\n    # create annotation file\n    annotations = coco_image.annotations\n    with open(yolo_annotation_path, \"w\") as outfile:\n        for annotation in annotations:\n            # convert coco bbox to yolo bbox\n            x_center = annotation.bbox[0] + annotation.bbox[2] / 2.0\n            y_center = annotation.bbox[1] + annotation.bbox[3] / 2.0\n            bbox_width = annotation.bbox[2]\n            bbox_height = annotation.bbox[3]\n            x_center = x_center * dw\n            y_center = y_center * dh\n            bbox_width = bbox_width * dw\n            bbox_height = bbox_height * dh\n            category_id = annotation.category_id\n            yolo_bbox = (x_center, y_center, bbox_width, bbox_height)\n            # save yolo annotation\n            outfile.write(str(category_id) + \" \" + \" \".join([str(value) for value in yolo_bbox]) + \"\\n\")\n</code></pre> <code></code> <code>export_yolo_images_and_txts_from_coco_object(output_dir, coco, ignore_negative_samples=False, mp=False, disable_symlink=False)</code> \u00b6 <p>Creates image symlinks and annotation txts in yolo format from coco dataset.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> \u00b6 <p>str Export directory.</p> required <code>coco</code> \u00b6 <p>sahi.utils.coco.Coco Initialized Coco object that contains images and categories.</p> required <code>ignore_negative_samples</code> \u00b6 <p>bool If True ignores images without annotations in all operations.</p> <code>False</code> <code>mp</code> \u00b6 <p>bool If True, multiprocess mode is on. Should be called in 'if name == main:' block.</p> <code>False</code> <code>disable_symlink</code> \u00b6 <p>bool If True, symlinks are not created. Instead images are copied.</p> <code>False</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_yolo_images_and_txts_from_coco_object(\n    output_dir, coco, ignore_negative_samples=False, mp=False, disable_symlink=False\n):\n    \"\"\"Creates image symlinks and annotation txts in yolo format from coco dataset.\n\n    Args:\n        output_dir: str\n            Export directory.\n        coco: sahi.utils.coco.Coco\n            Initialized Coco object that contains images and categories.\n        ignore_negative_samples: bool\n            If True ignores images without annotations in all operations.\n        mp: bool\n            If True, multiprocess mode is on.\n            Should be called in 'if __name__ == __main__:' block.\n        disable_symlink: bool\n            If True, symlinks are not created. Instead images are copied.\n    \"\"\"\n    logger.info(\"generating image symlinks and annotation files for yolo...\")\n    # symlink is not supported in colab\n    if is_colab() and not disable_symlink:\n        logger.warning(\"symlink is not supported in colab, disabling it...\")\n        disable_symlink = True\n    if mp:\n        with Pool(processes=48) as pool:\n            args = [\n                (coco_image, coco.image_dir, output_dir, ignore_negative_samples, disable_symlink)\n                for coco_image in coco.images\n            ]\n            pool.starmap(\n                export_single_yolo_image_and_corresponding_txt,\n                tqdm(args, total=len(args)),\n            )\n    else:\n        for coco_image in tqdm(coco.images):\n            export_single_yolo_image_and_corresponding_txt(\n                coco_image, coco.image_dir, output_dir, ignore_negative_samples, disable_symlink\n            )\n</code></pre> <code></code> <code>get_imageid2annotationlist_mapping(coco_dict)</code> \u00b6 <p>Get image_id to annotationlist mapping for faster indexing.</p> <p>Args</p> <pre><code>coco_dict : dict\n    coco dict with fields \"images\", \"annotations\", \"categories\"\n</code></pre> <p>Returns</p> <pre><code>image_id_to_annotation_list : dict\n{\n    1: [CocoAnnotation, CocoAnnotation, CocoAnnotation],\n    2: [CocoAnnotation]\n}\n\nwhere\nCocoAnnotation = {\n    'area': 2795520,\n    'bbox': [491.0, 1035.0, 153.0, 182.0],\n    'category_id': 1,\n    'id': 1,\n    'image_id': 1,\n    'iscrowd': 0,\n    'segmentation': [[491.0, 1035.0, 644.0, 1035.0, 644.0, 1217.0, 491.0, 1217.0]]\n}\n</code></pre> Source code in <code>sahi/utils/coco.py</code> <pre><code>def get_imageid2annotationlist_mapping(coco_dict: dict) -&gt; dict[int, list[CocoAnnotation]]:\n    \"\"\"Get image_id to annotationlist mapping for faster indexing.\n\n    Args\n\n        coco_dict : dict\n            coco dict with fields \"images\", \"annotations\", \"categories\"\n    Returns\n\n        image_id_to_annotation_list : dict\n        {\n            1: [CocoAnnotation, CocoAnnotation, CocoAnnotation],\n            2: [CocoAnnotation]\n        }\n\n        where\n        CocoAnnotation = {\n            'area': 2795520,\n            'bbox': [491.0, 1035.0, 153.0, 182.0],\n            'category_id': 1,\n            'id': 1,\n            'image_id': 1,\n            'iscrowd': 0,\n            'segmentation': [[491.0, 1035.0, 644.0, 1035.0, 644.0, 1217.0, 491.0, 1217.0]]\n        }\n    \"\"\"\n    image_id_to_annotation_list: dict = defaultdict(list)\n    logger.debug(\"indexing coco dataset annotations...\")\n    for annotation in coco_dict[\"annotations\"]:\n        image_id = annotation[\"image_id\"]\n        image_id_to_annotation_list[image_id].append(annotation)\n\n    return image_id_to_annotation_list\n</code></pre> <code></code> <code>merge(coco_dict1, coco_dict2, desired_name2id=None)</code> \u00b6 <p>Combines 2 coco formatted annotations dicts, and returns the combined coco dict.</p> <p>Parameters:</p> Name Type Description Default <code>coco_dict1 </code> \u00b6 <p>dict First coco dictionary.</p> required <code>coco_dict2 </code> \u00b6 <p>dict Second coco dictionary.</p> required <code>desired_name2id </code> \u00b6 <p>dict</p> required <p>Returns:     merged_coco_dict : dict         Merged COCO dict.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def merge(coco_dict1: dict, coco_dict2: dict, desired_name2id: dict | None = None) -&gt; dict:\n    \"\"\"Combines 2 coco formatted annotations dicts, and returns the combined coco dict.\n\n    Args:\n        coco_dict1 : dict\n            First coco dictionary.\n        coco_dict2 : dict\n            Second coco dictionary.\n        desired_name2id : dict\n            {\"human\": 1, \"car\": 2, \"big_vehicle\": 3}\n    Returns:\n        merged_coco_dict : dict\n            Merged COCO dict.\n    \"\"\"\n\n    # copy input dicts so that original dicts are not affected\n    temp_coco_dict1 = copy.deepcopy(coco_dict1)\n    temp_coco_dict2 = copy.deepcopy(coco_dict2)\n\n    # rearrange categories if any desired_name2id mapping is given\n    if desired_name2id is not None:\n        temp_coco_dict1 = update_categories(desired_name2id, temp_coco_dict1)\n        temp_coco_dict2 = update_categories(desired_name2id, temp_coco_dict2)\n\n    # rearrange categories of the second coco based on first, if their categories are not the same\n    if temp_coco_dict1[\"categories\"] != temp_coco_dict2[\"categories\"]:\n        desired_name2id = {category[\"name\"]: category[\"id\"] for category in temp_coco_dict1[\"categories\"]}\n        temp_coco_dict2 = update_categories(desired_name2id, temp_coco_dict2)\n\n    # calculate first image and annotation index of the second coco file\n    max_image_id = np.array([image[\"id\"] for image in coco_dict1[\"images\"]]).max()\n    max_annotation_id = np.array([annotation[\"id\"] for annotation in coco_dict1[\"annotations\"]]).max()\n\n    merged_coco_dict = temp_coco_dict1\n\n    for image in temp_coco_dict2[\"images\"]:\n        image[\"id\"] += max_image_id + 1\n        merged_coco_dict[\"images\"].append(image)\n\n    for annotation in temp_coco_dict2[\"annotations\"]:\n        annotation[\"image_id\"] += max_image_id + 1\n        annotation[\"id\"] += max_annotation_id + 1\n        merged_coco_dict[\"annotations\"].append(annotation)\n\n    return merged_coco_dict\n</code></pre> <code></code> <code>merge_from_file(coco_path1, coco_path2, save_path)</code> \u00b6 <p>Combines 2 coco formatted annotations files given their paths, and saves the combined file to save_path.</p> <p>Args:</p> <pre><code>coco_path1 : str\n    Path for the first coco file.\ncoco_path2 : str\n    Path for the second coco file.\nsave_path : str\n    \"dirname/coco.json\"\n</code></pre> Source code in <code>sahi/utils/coco.py</code> <pre><code>def merge_from_file(coco_path1: str, coco_path2: str, save_path: str):\n    \"\"\"Combines 2 coco formatted annotations files given their paths, and saves the combined file to save_path.\n\n    Args:\n\n        coco_path1 : str\n            Path for the first coco file.\n        coco_path2 : str\n            Path for the second coco file.\n        save_path : str\n            \"dirname/coco.json\"\n    \"\"\"\n\n    # load coco files to be combined\n    coco_dict1 = load_json(coco_path1)\n    coco_dict2 = load_json(coco_path2)\n\n    # merge coco dicts\n    merged_coco_dict = merge(coco_dict1, coco_dict2)\n\n    # save merged coco dict\n    save_json(merged_coco_dict, save_path)\n</code></pre> <code></code> <code>merge_from_list(coco_dict_list, desired_name2id=None, verbose=1)</code> \u00b6 <p>Combines a list of coco formatted annotations dicts, and returns the combined coco dict.</p> <p>Args:</p> <pre><code>coco_dict_list: list of dict\n    A list of coco dicts\ndesired_name2id: dict\n    {\"human\": 1, \"car\": 2, \"big_vehicle\": 3}\nverbose: bool\n    If True, merging info is printed\n</code></pre> <p>Returns:</p> <pre><code>merged_coco_dict: dict\n    Merged COCO dict.\n</code></pre> Source code in <code>sahi/utils/coco.py</code> <pre><code>def merge_from_list(coco_dict_list, desired_name2id=None, verbose=1):\n    \"\"\"Combines a list of coco formatted annotations dicts, and returns the combined coco dict.\n\n    Args:\n\n        coco_dict_list: list of dict\n            A list of coco dicts\n        desired_name2id: dict\n            {\"human\": 1, \"car\": 2, \"big_vehicle\": 3}\n        verbose: bool\n            If True, merging info is printed\n    Returns:\n\n        merged_coco_dict: dict\n            Merged COCO dict.\n    \"\"\"\n    if verbose:\n        if not desired_name2id:\n            print(\"'desired_name2id' is not specified, combining all categories.\")\n\n    # create desired_name2id by combinin all categories, if desired_name2id is not specified\n    if desired_name2id is None:\n        desired_name2id = {}\n        ind = 0\n        for coco_dict in coco_dict_list:\n            temp_categories = copy.deepcopy(coco_dict[\"categories\"])\n            for temp_category in temp_categories:\n                if temp_category[\"name\"] not in desired_name2id:\n                    desired_name2id[temp_category[\"name\"]] = ind\n                    ind += 1\n                else:\n                    continue\n\n    for ind, coco_dict in enumerate(coco_dict_list):\n        if ind == 0:\n            merged_coco_dict = copy.deepcopy(coco_dict)\n        else:\n            merged_coco_dict = merge(merged_coco_dict, coco_dict, desired_name2id)\n\n    # print categories\n    if verbose:\n        print(\n            \"Categories are formed as:\\n\",\n            merged_coco_dict[\"categories\"],\n        )\n\n    return merged_coco_dict\n</code></pre> <code></code> <code>remove_invalid_coco_results(result_list_or_path, dataset_dict_or_path=None)</code> \u00b6 Removes invalid predictions from coco result such as <ul> <li>negative bbox value</li> <li>extreme bbox value</li> </ul> <p>Parameters:</p> Name Type Description Default <code>result_list_or_path</code> \u00b6 <code>list | str</code> <p>path or list for coco result json</p> required <code>dataset_dict_or_path</code> \u00b6 <code>optional</code> <p>path or dict for coco dataset json</p> <code>None</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def remove_invalid_coco_results(result_list_or_path: list | str, dataset_dict_or_path: dict | str | None = None):\n    \"\"\"\n    Removes invalid predictions from coco result such as:\n        - negative bbox value\n        - extreme bbox value\n\n    Args:\n        result_list_or_path: path or list for coco result json\n        dataset_dict_or_path (optional): path or dict for coco dataset json\n    \"\"\"\n\n    # prepare coco results\n    if isinstance(result_list_or_path, str):\n        result_list = load_json(result_list_or_path)\n    elif isinstance(result_list_or_path, list):\n        result_list = result_list_or_path\n    else:\n        raise TypeError('incorrect type for \"result_list_or_path\"')  # type: ignore\n\n    # prepare image info from coco dataset\n    if dataset_dict_or_path is not None:\n        if isinstance(dataset_dict_or_path, str):\n            dataset_dict = load_json(dataset_dict_or_path)\n        elif isinstance(dataset_dict_or_path, dict):\n            dataset_dict = dataset_dict_or_path\n        else:\n            raise TypeError('incorrect type for \"dataset_dict\"')  # type: ignore\n        image_id_to_height = {}\n        image_id_to_width = {}\n        for coco_image in dataset_dict[\"images\"]:\n            image_id_to_height[coco_image[\"id\"]] = coco_image[\"height\"]\n            image_id_to_width[coco_image[\"id\"]] = coco_image[\"width\"]\n\n    # remove invalid predictions\n    fixed_result_list = []\n    for coco_result in result_list:\n        bbox = coco_result[\"bbox\"]\n        # ignore invalid predictions\n        if not bbox:\n            print(\"ignoring invalid prediction with empty bbox\")\n            continue\n        if bbox[0] &lt; 0 or bbox[1] &lt; 0 or bbox[2] &lt; 0 or bbox[3] &lt; 0:\n            print(f\"ignoring invalid prediction with bbox: {bbox}\")\n            continue\n        if dataset_dict_or_path is not None:\n            if (\n                bbox[1] &gt; image_id_to_height[coco_result[\"image_id\"]]\n                or bbox[3] &gt; image_id_to_height[coco_result[\"image_id\"]]\n                or bbox[0] &gt; image_id_to_width[coco_result[\"image_id\"]]\n                or bbox[2] &gt; image_id_to_width[coco_result[\"image_id\"]]\n            ):\n                print(f\"ignoring invalid prediction with bbox: {bbox}\")\n                continue\n        fixed_result_list.append(coco_result)\n    return fixed_result_list\n</code></pre> <code></code> <code>update_categories(desired_name2id, coco_dict)</code> \u00b6 <p>Rearranges category mapping of given COCO dictionary based on given category_mapping. Can also be used to filter some of the categories.</p> <p>Args:</p> <pre><code>desired_name2id : dict\n    {\"big_vehicle\": 1, \"car\": 2, \"human\": 3}\ncoco_dict : dict\n    COCO formatted dictionary.\n</code></pre> <p>Returns:</p> Name Type Description <code>coco_target</code> <code>dict</code> <p>dict COCO dict with updated/filtered categories.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def update_categories(desired_name2id: dict, coco_dict: dict) -&gt; dict:\n    \"\"\"Rearranges category mapping of given COCO dictionary based on given category_mapping. Can also be used to filter\n    some of the categories.\n\n    Args:\n\n        desired_name2id : dict\n            {\"big_vehicle\": 1, \"car\": 2, \"human\": 3}\n        coco_dict : dict\n            COCO formatted dictionary.\n\n    Returns:\n        coco_target : dict\n            COCO dict with updated/filtered categories.\n    \"\"\"\n    # so that original variable doesn't get affected\n    coco_source = copy.deepcopy(coco_dict)\n\n    # init target coco dict\n    coco_target = {\"images\": [], \"annotations\": [], \"categories\": []}\n\n    # init vars\n    currentid2desiredid_mapping = {}\n    # create category id mapping (currentid2desiredid_mapping)\n    for category in coco_source[\"categories\"]:\n        current_category_id = category[\"id\"]\n        current_category_name = category[\"name\"]\n        if current_category_name in desired_name2id.keys():\n            currentid2desiredid_mapping[current_category_id] = desired_name2id[current_category_name]\n        else:\n            # ignore categories that are not included in desired_name2id\n            currentid2desiredid_mapping[current_category_id] = -1\n\n    # update annotations\n    for annotation in coco_source[\"annotations\"]:\n        current_category_id = annotation[\"category_id\"]\n        desired_category_id = currentid2desiredid_mapping[current_category_id]\n        # append annotations with category id present in desired_name2id\n        if desired_category_id != -1:\n            # update cetegory id\n            annotation[\"category_id\"] = desired_category_id\n            # append updated annotation to target coco dict\n            coco_target[\"annotations\"].append(annotation)\n\n    # create desired categories\n    categories = []\n    for name in desired_name2id.keys():\n        category = {}\n        category[\"name\"] = category[\"supercategory\"] = name\n        category[\"id\"] = desired_name2id[name]\n        categories.append(category)\n\n    # update categories\n    coco_target[\"categories\"] = categories\n\n    # update images\n    coco_target[\"images\"] = coco_source[\"images\"]\n\n    return coco_target\n</code></pre> <code></code> <code>update_categories_from_file(desired_name2id, coco_path, save_path)</code> \u00b6 <p>Rearranges category mapping of a COCO dictionary in coco_path based on given category_mapping. Can also be used to filter some of the categories.</p> <p>Parameters:</p> Name Type Description Default <code>desired_name2id </code> \u00b6 <p>dict</p> required <code>coco_path </code> \u00b6 <p>str \"dirname/coco.json\"</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def update_categories_from_file(desired_name2id: dict, coco_path: str, save_path: str) -&gt; None:\n    \"\"\"Rearranges category mapping of a COCO dictionary in coco_path based on given category_mapping. Can also be used\n    to filter some of the categories.\n\n    Args:\n        desired_name2id : dict\n            {\"human\": 1, \"car\": 2, \"big_vehicle\": 3}\n        coco_path : str\n            \"dirname/coco.json\"\n    \"\"\"\n    # load source coco dict\n    coco_source = load_json(coco_path)\n\n    # update categories\n    coco_target = update_categories(desired_name2id, coco_source)\n\n    # save modified coco file\n    save_json(coco_target, save_path)\n</code></pre>"},{"location":"api/#sahi.utils.cv","title":"<code>cv</code>","text":"Classes\u00b6 <code>Colors</code> \u00b6 Source code in <code>sahi/utils/cv.py</code> <pre><code>class Colors:\n    def __init__(self):\n        hex_colors = (\n            \"FF3838 2C99A8 FF701F 6473FF CFD231 48F90A 92CC17 3DDB86 1A9334 00D4BB \"\n            \"FF9D97 00C2FF 344593 FFB21D 0018EC 8438FF 520085 CB38FF FF95C8 FF37C7\"\n        )\n\n        self.palette = [self.hex_to_rgb(f\"#{c}\") for c in hex_colors.split()]\n        self.n = len(self.palette)\n\n    def __call__(self, ind, bgr: bool = False):\n        \"\"\"Convert an index to a color code.\n\n        Args:\n            ind (int): The index to convert.\n            bgr (bool, optional): Whether to return the color code in BGR format. Defaults to False.\n\n        Returns:\n            tuple: The color code in RGB or BGR format, depending on the value of `bgr`.\n        \"\"\"\n        color_codes = self.palette[int(ind) % self.n]\n        return (color_codes[2], color_codes[1], color_codes[0]) if bgr else color_codes\n\n    @staticmethod\n    def hex_to_rgb(hex_code):\n        \"\"\"Converts a hexadecimal color code to RGB format.\n\n        Args:\n            hex_code (str): The hexadecimal color code to convert.\n\n        Returns:\n            tuple: A tuple representing the RGB values in the order (R, G, B).\n        \"\"\"\n        rgb = []\n        for i in (0, 2, 4):\n            rgb.append(int(hex_code[1 + i : 1 + i + 2], 16))\n        return tuple(rgb)\n</code></pre> Functions\u00b6 <code></code> <code>__call__(ind, bgr=False)</code> \u00b6 <p>Convert an index to a color code.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> \u00b6 <code>int</code> <p>The index to convert.</p> required <code>bgr</code> \u00b6 <code>bool</code> <p>Whether to return the color code in BGR format. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>The color code in RGB or BGR format, depending on the value of <code>bgr</code>.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def __call__(self, ind, bgr: bool = False):\n    \"\"\"Convert an index to a color code.\n\n    Args:\n        ind (int): The index to convert.\n        bgr (bool, optional): Whether to return the color code in BGR format. Defaults to False.\n\n    Returns:\n        tuple: The color code in RGB or BGR format, depending on the value of `bgr`.\n    \"\"\"\n    color_codes = self.palette[int(ind) % self.n]\n    return (color_codes[2], color_codes[1], color_codes[0]) if bgr else color_codes\n</code></pre> <code></code> <code>hex_to_rgb(hex_code)</code> <code>staticmethod</code> \u00b6 <p>Converts a hexadecimal color code to RGB format.</p> <p>Parameters:</p> Name Type Description Default <code>hex_code</code> \u00b6 <code>str</code> <p>The hexadecimal color code to convert.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple representing the RGB values in the order (R, G, B).</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>@staticmethod\ndef hex_to_rgb(hex_code):\n    \"\"\"Converts a hexadecimal color code to RGB format.\n\n    Args:\n        hex_code (str): The hexadecimal color code to convert.\n\n    Returns:\n        tuple: A tuple representing the RGB values in the order (R, G, B).\n    \"\"\"\n    rgb = []\n    for i in (0, 2, 4):\n        rgb.append(int(hex_code[1 + i : 1 + i + 2], 16))\n    return tuple(rgb)\n</code></pre> Functions\u00b6 <code></code> <code>apply_color_mask(image, color)</code> \u00b6 <p>Applies color mask to given input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>The input image to apply the color mask to.</p> required <code>color</code> \u00b6 <code>tuple</code> <p>The RGB color tuple to use for the mask.</p> required <p>Returns:</p> Type Description <p>np.ndarray: The resulting image with the applied color mask.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def apply_color_mask(image: np.ndarray, color: tuple[int, int, int]):\n    \"\"\"Applies color mask to given input image.\n\n    Args:\n        image (np.ndarray): The input image to apply the color mask to.\n        color (tuple): The RGB color tuple to use for the mask.\n\n    Returns:\n        np.ndarray: The resulting image with the applied color mask.\n    \"\"\"\n    r = np.zeros_like(image).astype(np.uint8)\n    g = np.zeros_like(image).astype(np.uint8)\n    b = np.zeros_like(image).astype(np.uint8)\n\n    (r[image == 1], g[image == 1], b[image == 1]) = color\n    colored_mask = np.stack([r, g, b], axis=2)\n    return colored_mask\n</code></pre> <code></code> <code>convert_image_to(read_path, extension='jpg', grayscale=False)</code> \u00b6 <p>Reads an image from the given path and saves it with the specified extension.</p> <p>Parameters:</p> Name Type Description Default <code>read_path</code> \u00b6 <code>str</code> <p>The path to the image file.</p> required <code>extension</code> \u00b6 <code>str</code> <p>The desired file extension for the saved image. Defaults to \"jpg\".</p> <code>'jpg'</code> <code>grayscale</code> \u00b6 <code>bool</code> <p>Whether to convert the image to grayscale. Defaults to False.</p> <code>False</code> Source code in <code>sahi/utils/cv.py</code> <pre><code>def convert_image_to(read_path, extension: str = \"jpg\", grayscale: bool = False):\n    \"\"\"Reads an image from the given path and saves it with the specified extension.\n\n    Args:\n        read_path (str): The path to the image file.\n        extension (str, optional): The desired file extension for the saved image. Defaults to \"jpg\".\n        grayscale (bool, optional): Whether to convert the image to grayscale. Defaults to False.\n    \"\"\"\n    image = cv2.imread(read_path)\n    pre, _ = os.path.splitext(read_path)\n    if grayscale:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        pre = pre + \"_gray\"\n    save_path = pre + \".\" + extension\n    cv2.imwrite(save_path, image)\n</code></pre> <code></code> <code>crop_object_predictions(image, object_prediction_list, output_dir='', file_name='prediction_visual', export_format='png')</code> \u00b6 <p>Crops bounding boxes over the source image and exports it to the output folder.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>The source image to crop bounding boxes from.</p> required <code>object_prediction_list</code> \u00b6 <p>A list of object predictions.</p> required <code>output_dir</code> \u00b6 <code>str</code> <p>The directory where the resulting visualizations will be exported. Defaults to an empty string.</p> <code>''</code> <code>file_name</code> \u00b6 <code>str</code> <p>The name of the exported file. The exported file will be saved as <code>output_dir + file_name + \".png\"</code>. Defaults to \"prediction_visual\".</p> <code>'prediction_visual'</code> <code>export_format</code> \u00b6 <code>str</code> <p>The format of the exported file. Can be specified as 'jpg' or 'png'. Defaults to \"png\".</p> <code>'png'</code> Source code in <code>sahi/utils/cv.py</code> <pre><code>def crop_object_predictions(\n    image: np.ndarray,\n    object_prediction_list,\n    output_dir: str = \"\",\n    file_name: str = \"prediction_visual\",\n    export_format: str = \"png\",\n):\n    \"\"\"Crops bounding boxes over the source image and exports it to the output folder.\n\n    Args:\n        image (np.ndarray): The source image to crop bounding boxes from.\n        object_prediction_list: A list of object predictions.\n        output_dir (str): The directory where the resulting visualizations will be exported. Defaults to an empty string.\n        file_name (str): The name of the exported file. The exported file will be saved as `output_dir + file_name + \".png\"`. Defaults to \"prediction_visual\".\n        export_format (str): The format of the exported file. Can be specified as 'jpg' or 'png'. Defaults to \"png\".\n    \"\"\"  # noqa\n\n    # create output folder if not present\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\n    # add bbox and mask to image if present\n    for ind, object_prediction in enumerate(object_prediction_list):\n        # deepcopy object_prediction_list so that the original is not altered\n        object_prediction = object_prediction.deepcopy()\n        bbox = object_prediction.bbox.to_xyxy()\n        category_id = object_prediction.category.id\n        # crop detections\n        # deepcopy crops so that the original is not altered\n        cropped_img = copy.deepcopy(\n            image[\n                int(bbox[1]) : int(bbox[3]),\n                int(bbox[0]) : int(bbox[2]),\n                :,\n            ]\n        )\n        save_path = os.path.join(\n            output_dir,\n            file_name + \"_box\" + str(ind) + \"_class\" + str(category_id) + \".\" + export_format,\n        )\n        cv2.imwrite(save_path, cv2.cvtColor(cropped_img, cv2.COLOR_RGB2BGR))\n</code></pre> <code></code> <code>get_bbox_from_bool_mask(bool_mask)</code> \u00b6 <p>Generate VOC bounding box [xmin, ymin, xmax, ymax] from given boolean mask.</p> <p>Parameters:</p> Name Type Description Default <code>bool_mask</code> \u00b6 <code>ndarray</code> <p>2D boolean mask.</p> required <p>Returns:</p> Type Description <code>list[int] | None</code> <p>Optional[List[int]]: VOC bounding box [xmin, ymin, xmax, ymax] or None if no bounding box is found.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def get_bbox_from_bool_mask(bool_mask: np.ndarray) -&gt; list[int] | None:\n    \"\"\"Generate VOC bounding box [xmin, ymin, xmax, ymax] from given boolean mask.\n\n    Args:\n        bool_mask (np.ndarray): 2D boolean mask.\n\n    Returns:\n        Optional[List[int]]: VOC bounding box [xmin, ymin, xmax, ymax] or None if no bounding box is found.\n    \"\"\"\n    rows = np.any(bool_mask, axis=1)\n    cols = np.any(bool_mask, axis=0)\n\n    if not np.any(rows) or not np.any(cols):\n        return None\n\n    ymin, ymax = np.where(rows)[0][[0, -1]]\n    xmin, xmax = np.where(cols)[0][[0, -1]]\n    width = xmax - xmin\n    height = ymax - ymin\n\n    if width == 0 or height == 0:\n        return None\n\n    return [xmin, ymin, xmax, ymax]\n</code></pre> <code></code> <code>get_bbox_from_coco_segmentation(coco_segmentation)</code> \u00b6 <p>Generate voc box ([xmin, ymin, xmax, ymax]) from given coco segmentation.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def get_bbox_from_coco_segmentation(coco_segmentation):\n    \"\"\"Generate voc box ([xmin, ymin, xmax, ymax]) from given coco segmentation.\"\"\"\n    xs = []\n    ys = []\n    for segm in coco_segmentation:\n        xs.extend(segm[::2])\n        ys.extend(segm[1::2])\n    if len(xs) == 0 or len(ys) == 0:\n        return None\n    xmin = min(xs)\n    xmax = max(xs)\n    ymin = min(ys)\n    ymax = max(ys)\n    return [xmin, ymin, xmax, ymax]\n</code></pre> <code></code> <code>get_bool_mask_from_coco_segmentation(coco_segmentation, width, height)</code> \u00b6 <p>Convert coco segmentation to 2D boolean mask of given height and width.</p> <p>Parameters: - coco_segmentation: list of points representing the coco segmentation - width: width of the boolean mask - height: height of the boolean mask</p> <p>Returns: - bool_mask: 2D boolean mask of size (height, width)</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def get_bool_mask_from_coco_segmentation(coco_segmentation: list[list[float]], width: int, height: int) -&gt; np.ndarray:\n    \"\"\"Convert coco segmentation to 2D boolean mask of given height and width.\n\n    Parameters:\n    - coco_segmentation: list of points representing the coco segmentation\n    - width: width of the boolean mask\n    - height: height of the boolean mask\n\n    Returns:\n    - bool_mask: 2D boolean mask of size (height, width)\n    \"\"\"\n    size = [height, width]\n    points = [np.array(point).reshape(-1, 2).round().astype(int) for point in coco_segmentation]\n    bool_mask = np.zeros(size)\n    bool_mask = cv2.fillPoly(bool_mask, points, (1.0,))\n    bool_mask.astype(bool)\n    return bool_mask\n</code></pre> <code></code> <code>get_coco_segmentation_from_bool_mask(bool_mask)</code> \u00b6 <p>Convert boolean mask to coco segmentation format [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def get_coco_segmentation_from_bool_mask(bool_mask: np.ndarray) -&gt; list[list[float]]:\n    \"\"\"\n    Convert boolean mask to coco segmentation format\n    [\n        [x1, y1, x2, y2, x3, y3, ...],\n        [x1, y1, x2, y2, x3, y3, ...],\n        ...\n    ]\n    \"\"\"\n    # Generate polygons from mask\n    mask = np.squeeze(bool_mask)\n    mask = mask.astype(np.uint8)\n    mask = cv2.copyMakeBorder(mask, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n    polygons = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE, offset=(-1, -1))\n    polygons = polygons[0] if len(polygons) == 2 else polygons[1]\n    # Convert polygon to coco segmentation\n    coco_segmentation = []\n    for polygon in polygons:\n        segmentation = polygon.flatten().tolist()\n        # at least 3 points needed for a polygon\n        if len(segmentation) &gt;= 6:\n            coco_segmentation.append(segmentation)\n    return coco_segmentation\n</code></pre> <code></code> <code>get_coco_segmentation_from_obb_points(obb_points)</code> \u00b6 <p>Convert OBB (Oriented Bounding Box) points to COCO polygon format.</p> <p>Parameters:</p> Name Type Description Default <code>obb_points</code> \u00b6 <code>ndarray</code> <p>np.ndarray OBB points tensor from ultralytics.engine.results.OBB Shape: (4, 2) containing 4 points with (x,y) coordinates each</p> required <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>List[List[float]]: Polygon points in COCO format [[x1, y1, x2, y2, x3, y3, x4, y4], [...], ...]</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def get_coco_segmentation_from_obb_points(obb_points: np.ndarray) -&gt; list[list[float]]:\n    \"\"\"Convert OBB (Oriented Bounding Box) points to COCO polygon format.\n\n    Args:\n        obb_points: np.ndarray\n            OBB points tensor from ultralytics.engine.results.OBB\n            Shape: (4, 2) containing 4 points with (x,y) coordinates each\n\n    Returns:\n        List[List[float]]: Polygon points in COCO format\n            [[x1, y1, x2, y2, x3, y3, x4, y4], [...], ...]\n    \"\"\"\n    # Convert from (4,2) to [x1,y1,x2,y2,x3,y3,x4,y4] format\n    points = obb_points.reshape(-1).tolist()\n\n    # Create polygon from points and close it by repeating first point\n    polygons = []\n    # Add first point to end to close polygon\n    closed_polygon = [*points, points[0], points[1]]\n    polygons.append(closed_polygon)\n\n    return polygons\n</code></pre> <code></code> <code>get_video_reader(source, save_dir, frame_skip_interval, export_visual=False, view_visual=False)</code> \u00b6 <p>Creates OpenCV video capture object from given video file path.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> \u00b6 <code>str</code> <p>Video file path</p> required <code>save_dir</code> \u00b6 <code>str</code> <p>Video export directory</p> required <code>frame_skip_interval</code> \u00b6 <code>int</code> <p>Frame skip interval</p> required <code>export_visual</code> \u00b6 <code>bool</code> <p>Set True if you want to export visuals</p> <code>False</code> <code>view_visual</code> \u00b6 <code>bool</code> <p>Set True if you want to render visual</p> <code>False</code> <p>Returns:</p> Name Type Description <code>iterator</code> <code>Generator[Image]</code> <p>Pillow Image</p> <code>video_writer</code> <code>VideoWriter | None</code> <p>cv2.VideoWriter</p> <code>video_file_name</code> <code>str</code> <p>video name with extension</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def get_video_reader(\n    source: str,\n    save_dir: str,\n    frame_skip_interval: int,\n    export_visual: bool = False,\n    view_visual: bool = False,\n) -&gt; tuple[Generator[Image.Image], cv2.VideoWriter | None, str, int]:\n    \"\"\"Creates OpenCV video capture object from given video file path.\n\n    Args:\n        source: Video file path\n        save_dir: Video export directory\n        frame_skip_interval: Frame skip interval\n        export_visual: Set True if you want to export visuals\n        view_visual: Set True if you want to render visual\n\n    Returns:\n        iterator: Pillow Image\n        video_writer: cv2.VideoWriter\n        video_file_name: video name with extension\n    \"\"\"\n    # get video name with extension\n    video_file_name = os.path.basename(source)\n    # get video from video path\n    video_capture = cv2.VideoCapture(source)\n\n    num_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    if view_visual:\n        num_frames /= frame_skip_interval + 1\n        num_frames = int(num_frames)\n\n    def read_video_frame(video_capture, frame_skip_interval) -&gt; Generator[Image.Image]:\n        if view_visual:\n            window_name = f\"Prediction of {video_file_name!s}\"\n            cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)\n            default_image = np.zeros((480, 640, 3), dtype=np.uint8)\n            cv2.imshow(window_name, default_image)\n\n            while video_capture.isOpened:\n                frame_num = video_capture.get(cv2.CAP_PROP_POS_FRAMES)\n                video_capture.set(cv2.CAP_PROP_POS_FRAMES, frame_num + frame_skip_interval)\n\n                k = cv2.waitKey(20)\n                frame_num = video_capture.get(cv2.CAP_PROP_POS_FRAMES)\n\n                if k == 27:\n                    print(\n                        \"\\n===========================Closing===========================\"\n                    )  # Exit the prediction, Key = Esc\n                    exit()\n                if k == 100:\n                    frame_num += 100  # Skip 100 frames, Key = d\n                if k == 97:\n                    frame_num -= 100  # Prev 100 frames, Key = a\n                if k == 103:\n                    frame_num += 20  # Skip 20 frames, Key = g\n                if k == 102:\n                    frame_num -= 20  # Prev 20 frames, Key = f\n                video_capture.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n\n                ret, frame = video_capture.read()\n                if not ret:\n                    print(\"\\n=========================== Video Ended ===========================\")\n                    break\n                yield Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\n        else:\n            while video_capture.isOpened:\n                frame_num = video_capture.get(cv2.CAP_PROP_POS_FRAMES)\n                video_capture.set(cv2.CAP_PROP_POS_FRAMES, frame_num + frame_skip_interval)\n\n                ret, frame = video_capture.read()\n                if not ret:\n                    print(\"\\n=========================== Video Ended ===========================\")\n                    break\n                yield Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\n    if export_visual:\n        # get video properties and create VideoWriter object\n        if frame_skip_interval != 0:\n            fps = video_capture.get(cv2.CAP_PROP_FPS)  # original fps of video\n            # The fps of export video is increasing during view_image because frame is skipped\n            fps = (\n                fps / frame_skip_interval\n            )  # How many time_interval equals to original fps. One time_interval skip x frames.\n        else:\n            fps = video_capture.get(cv2.CAP_PROP_FPS)\n\n        w = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n        h = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        size = (w, h)\n        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # pyright: ignore[reportAttributeAccessIssue]\n        video_writer = cv2.VideoWriter(os.path.join(save_dir, video_file_name), fourcc, fps, size)\n    else:\n        video_writer = None\n\n    return read_video_frame(video_capture, frame_skip_interval), video_writer, video_file_name, num_frames\n</code></pre> <code></code> <code>ipython_display(image)</code> \u00b6 <p>Displays numpy image in notebook.</p> <p>If input image is in range 0..1, please first multiply img by 255 Assumes image is ndarray of shape [height, width, channels] where channels can be 1, 3 or 4</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def ipython_display(image: np.ndarray):\n    \"\"\"Displays numpy image in notebook.\n\n    If input image is in range 0..1, please first multiply img by 255\n    Assumes image is ndarray of shape [height, width, channels] where channels can be 1, 3 or 4\n    \"\"\"\n    import IPython\n\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    _, ret = cv2.imencode(\".png\", image)\n    i = IPython.display.Image(data=ret)  # type: ignore\n    IPython.display.display(i)  # type: ignore\n</code></pre> <code></code> <code>normalize_numpy_image(image)</code> \u00b6 <p>Normalizes numpy image.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def normalize_numpy_image(image: np.ndarray):\n    \"\"\"Normalizes numpy image.\"\"\"\n    return image / np.max(image)\n</code></pre> <code></code> <code>read_image(image_path)</code> \u00b6 <p>Loads image as a numpy array from the given path.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> \u00b6 <code>str</code> <p>The path to the image file.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: The loaded image as a numpy array.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def read_image(image_path: str) -&gt; np.ndarray:\n    \"\"\"Loads image as a numpy array from the given path.\n\n    Args:\n        image_path (str): The path to the image file.\n\n    Returns:\n        numpy.ndarray: The loaded image as a numpy array.\n    \"\"\"\n    # read image\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    # return image\n    return image\n</code></pre> <code></code> <code>read_image_as_pil(image, exif_fix=True)</code> \u00b6 <p>Loads an image as PIL.Image.Image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>Union[Image, str, ndarray]</code> <p>The image to be loaded. It can be an image path or URL (str), a numpy image (np.ndarray), or a PIL.Image object.</p> required <code>exif_fix</code> \u00b6 <code>bool</code> <p>Whether to apply an EXIF fix to the image. Defaults to False.</p> <code>True</code> <p>Returns:</p> Type Description <code>Image</code> <p>PIL.Image.Image: The loaded image as a PIL.Image object.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def read_image_as_pil(image: Image.Image | str | np.ndarray, exif_fix: bool = True) -&gt; Image.Image:\n    \"\"\"Loads an image as PIL.Image.Image.\n\n    Args:\n        image (Union[Image.Image, str, np.ndarray]): The image to be loaded. It can be an image path or URL (str),\n            a numpy image (np.ndarray), or a PIL.Image object.\n        exif_fix (bool, optional): Whether to apply an EXIF fix to the image. Defaults to False.\n\n    Returns:\n        PIL.Image.Image: The loaded image as a PIL.Image object.\n    \"\"\"\n    # https://stackoverflow.com/questions/56174099/how-to-load-images-larger-than-max-image-pixels-with-pil\n    Image.MAX_IMAGE_PIXELS = None\n\n    if isinstance(image, Image.Image):\n        image_pil = image\n    elif isinstance(image, str):\n        # read image if str image path is provided\n        try:\n            image_pil = Image.open(\n                BytesIO(requests.get(image, stream=True).content) if str(image).startswith(\"http\") else image\n            ).convert(\"RGB\")\n            if exif_fix:\n                ImageOps.exif_transpose(image_pil, in_place=True)\n        except Exception as e:  # handle large/tiff image reading\n            logger.error(f\"PIL failed reading image with error {e}, trying skimage instead\")\n            try:\n                import skimage.io\n            except ImportError:\n                raise ImportError(\"Please run 'pip install -U scikit-image imagecodecs' for large image handling.\")\n            image_sk = skimage.io.imread(image).astype(np.uint8)\n            if len(image_sk.shape) == 2:  # b&amp;w\n                image_pil = Image.fromarray(image_sk, mode=\"1\")\n            elif image_sk.shape[2] == 4:  # rgba\n                image_pil = Image.fromarray(image_sk, mode=\"RGBA\")\n            elif image_sk.shape[2] == 3:  # rgb\n                image_pil = Image.fromarray(image_sk, mode=\"RGB\")\n            else:\n                raise TypeError(f\"image with shape: {image_sk.shape[3]} is not supported.\")\n    elif isinstance(image, np.ndarray):\n        if image.shape[0] &lt; 5:  # image in CHW\n            image = image[:, :, ::-1]\n        image_pil = Image.fromarray(image)\n    else:\n        raise TypeError(\"read image with 'pillow' using 'Image.open()'\")\n    return image_pil\n</code></pre> <code></code> <code>read_large_image(image_path)</code> \u00b6 <p>Reads a large image from the specified image path.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> \u00b6 <code>str</code> <p>The path to the image file.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the image data and a flag indicating whether cv2 was used to read the image. The image data is a numpy array representing the image in RGB format. The flag is True if cv2 was used, False otherwise.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def read_large_image(image_path: str):\n    \"\"\"Reads a large image from the specified image path.\n\n    Args:\n        image_path (str): The path to the image file.\n\n    Returns:\n        tuple: A tuple containing the image data and a flag indicating whether cv2 was used to read the image.\n            The image data is a numpy array representing the image in RGB format.\n            The flag is True if cv2 was used, False otherwise.\n    \"\"\"\n    use_cv2 = True\n    # read image, cv2 fails on large files\n    try:\n        # convert to rgb (cv2 reads in bgr)\n        img_cv2 = cv2.imread(image_path, 1)\n        image0 = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n    except Exception as e:\n        logger.error(f\"OpenCV failed reading image with error {e}, trying skimage instead\")\n        try:\n            import skimage.io\n        except ImportError:\n            raise ImportError(\n                'Please run \"pip install -U scikit-image\" to install scikit-image first for large image handling.'\n            )\n        image0 = skimage.io.imread(image_path, as_grey=False).astype(np.uint8)  # [::-1]\n        use_cv2 = False\n    return image0, use_cv2\n</code></pre> <code></code> <code>select_random_color()</code> \u00b6 <p>Selects a random color from a predefined list of colors.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list representing the RGB values of the selected color.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def select_random_color():\n    \"\"\"Selects a random color from a predefined list of colors.\n\n    Returns:\n        list: A list representing the RGB values of the selected color.\n    \"\"\"\n    colors = [\n        [0, 255, 0],\n        [0, 0, 255],\n        [255, 0, 0],\n        [0, 255, 255],\n        [255, 255, 0],\n        [255, 0, 255],\n        [80, 70, 180],\n        [250, 80, 190],\n        [245, 145, 50],\n        [70, 150, 250],\n        [50, 190, 190],\n    ]\n    return colors[random.randrange(0, 10)]\n</code></pre> <code></code> <code>visualize_object_predictions(image, object_prediction_list, rect_th=None, text_size=None, text_th=None, color=None, hide_labels=False, hide_conf=False, output_dir=None, file_name='prediction_visual', export_format='png')</code> \u00b6 <p>Visualizes prediction category names, bounding boxes over the source image and exports it to output folder.</p> <p>Parameters:</p> Name Type Description Default <code>object_prediction_list</code> \u00b6 <p>a list of prediction.ObjectPrediction</p> required <code>rect_th</code> \u00b6 <code>int | None</code> <p>rectangle thickness</p> <code>None</code> <code>text_size</code> \u00b6 <code>float | None</code> <p>size of the category name over box</p> <code>None</code> <code>text_th</code> \u00b6 <code>int | None</code> <p>text thickness</p> <code>None</code> <code>color</code> \u00b6 <code>tuple | None</code> <p>annotation color in the form: (0, 255, 0)</p> <code>None</code> <code>hide_labels</code> \u00b6 <code>bool</code> <p>hide labels</p> <code>False</code> <code>hide_conf</code> \u00b6 <code>bool</code> <p>hide confidence</p> <code>False</code> <code>output_dir</code> \u00b6 <code>str | None</code> <p>directory for resulting visualization to be exported</p> <code>None</code> <code>file_name</code> \u00b6 <code>str | None</code> <p>exported file will be saved as: output_dir+file_name+\".png\"</p> <code>'prediction_visual'</code> <code>export_format</code> \u00b6 <code>str | None</code> <p>can be specified as 'jpg' or 'png'</p> <code>'png'</code> Source code in <code>sahi/utils/cv.py</code> <pre><code>def visualize_object_predictions(\n    image: np.ndarray,\n    object_prediction_list,\n    rect_th: int | None = None,\n    text_size: float | None = None,\n    text_th: int | None = None,\n    color: tuple | None = None,\n    hide_labels: bool = False,\n    hide_conf: bool = False,\n    output_dir: str | None = None,\n    file_name: str | None = \"prediction_visual\",\n    export_format: str | None = \"png\",\n):\n    \"\"\"Visualizes prediction category names, bounding boxes over the source image and exports it to output folder.\n\n    Args:\n        object_prediction_list: a list of prediction.ObjectPrediction\n        rect_th: rectangle thickness\n        text_size: size of the category name over box\n        text_th: text thickness\n        color: annotation color in the form: (0, 255, 0)\n        hide_labels: hide labels\n        hide_conf: hide confidence\n        output_dir: directory for resulting visualization to be exported\n        file_name: exported file will be saved as: output_dir+file_name+\".png\"\n        export_format: can be specified as 'jpg' or 'png'\n    \"\"\"\n    elapsed_time = time.time()\n    # deepcopy image so that original is not altered\n    image = copy.deepcopy(image)\n    # select predefined classwise color palette if not specified\n    if color is None:\n        colors = Colors()\n    else:\n        colors = None\n    # set rect_th for boxes\n    rect_th = rect_th or max(round(sum(image.shape) / 2 * 0.003), 2)\n    # set text_th for category names\n    text_th = text_th or max(rect_th - 1, 1)\n    # set text_size for category names\n    text_size = text_size or rect_th / 3\n\n    # add masks or obb polygons to image if present\n    for object_prediction in object_prediction_list:\n        # deepcopy object_prediction_list so that original is not altered\n        object_prediction = object_prediction.deepcopy()\n        # arange label to be displayed\n        label = f\"{object_prediction.category.name}\"\n        if not hide_conf:\n            label += f\" {object_prediction.score.value:.2f}\"\n        # set color\n        if colors is not None:\n            color = colors(object_prediction.category.id)\n        # visualize masks or obb polygons if present\n        has_mask = object_prediction.mask is not None\n        is_obb_pred = False\n        if has_mask:\n            segmentation = object_prediction.mask.segmentation\n            if len(segmentation) == 1 and len(segmentation[0]) == 8:\n                is_obb_pred = True\n\n            if is_obb_pred:\n                points = np.array(segmentation).reshape((-1, 1, 2)).astype(np.int32)\n                cv2.polylines(image, [points], isClosed=True, color=color or (0, 0, 0), thickness=rect_th)\n\n                if not hide_labels:\n                    lowest_point = points[points[:, :, 1].argmax()][0]\n                    box_width, box_height = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[0]\n                    outside = lowest_point[1] - box_height - 3 &gt;= 0\n                    text_bg_point1 = (\n                        lowest_point[0],\n                        lowest_point[1] - box_height - 3 if outside else lowest_point[1] + 3,\n                    )\n                    text_bg_point2 = (lowest_point[0] + box_width, lowest_point[1])\n                    cv2.rectangle(\n                        image, text_bg_point1, text_bg_point2, color or (0, 0, 0), thickness=-1, lineType=cv2.LINE_AA\n                    )\n                    cv2.putText(\n                        image,\n                        label,\n                        (lowest_point[0], lowest_point[1] - 2 if outside else lowest_point[1] + box_height + 2),\n                        0,\n                        text_size,\n                        (255, 255, 255),\n                        thickness=text_th,\n                    )\n            else:\n                # draw mask\n                rgb_mask = apply_color_mask(object_prediction.mask.bool_mask, color or (0, 0, 0))\n                image = cv2.addWeighted(image, 1, rgb_mask, 0.6, 0)\n\n        # add bboxes to image if is_obb_pred=False\n        if not is_obb_pred:\n            bbox = object_prediction.bbox.to_xyxy()\n\n            # set bbox points\n            point1, point2 = (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3]))\n            # visualize boxes\n            cv2.rectangle(\n                image,\n                point1,\n                point2,\n                color=color or (0, 0, 0),\n                thickness=rect_th,\n            )\n\n            if not hide_labels:\n                box_width, box_height = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[\n                    0\n                ]  # label width, height\n                outside = point1[1] - box_height - 3 &gt;= 0  # label fits outside box\n                point2 = point1[0] + box_width, point1[1] - box_height - 3 if outside else point1[1] + box_height + 3\n                # add bounding box text\n                cv2.rectangle(image, point1, point2, color or (0, 0, 0), -1, cv2.LINE_AA)  # filled\n                cv2.putText(\n                    image,\n                    label,\n                    (point1[0], point1[1] - 2 if outside else point1[1] + box_height + 2),\n                    0,\n                    text_size,\n                    (255, 255, 255),\n                    thickness=text_th,\n                )\n\n    # export if output_dir is present\n    if output_dir is not None:\n        # export image with predictions\n        Path(output_dir).mkdir(parents=True, exist_ok=True)\n        # save inference result\n        save_path = str(Path(output_dir) / ((file_name or \"\") + \".\" + (export_format or \"\")))\n        cv2.imwrite(save_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n\n    elapsed_time = time.time() - elapsed_time\n    return {\"image\": image, \"elapsed_time\": elapsed_time}\n</code></pre> <code></code> <code>visualize_prediction(image, boxes, classes, masks=None, rect_th=None, text_size=None, text_th=None, color=None, hide_labels=False, output_dir=None, file_name='prediction_visual')</code> \u00b6 <p>Visualizes prediction classes, bounding boxes over the source image and exports it to output folder.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>The source image.</p> required <code>boxes</code> \u00b6 <code>List[List]</code> <p>List of bounding boxes coordinates.</p> required <code>classes</code> \u00b6 <code>List[str]</code> <p>List of class labels corresponding to each bounding box.</p> required <code>masks</code> \u00b6 <code>Optional[List[ndarray]]</code> <p>List of masks corresponding to each bounding box. Defaults to None.</p> <code>None</code> <code>rect_th</code> \u00b6 <code>int</code> <p>Thickness of the bounding box rectangle. Defaults to None.</p> <code>None</code> <code>text_size</code> \u00b6 <code>float</code> <p>Size of the text for class labels. Defaults to None.</p> <code>None</code> <code>text_th</code> \u00b6 <code>int</code> <p>Thickness of the text for class labels. Defaults to None.</p> <code>None</code> <code>color</code> \u00b6 <code>tuple</code> <p>Color of the bounding box and text. Defaults to None.</p> <code>None</code> <code>hide_labels</code> \u00b6 <code>bool</code> <p>Whether to hide the class labels. Defaults to False.</p> <code>False</code> <code>output_dir</code> \u00b6 <code>Optional[str]</code> <p>Output directory to save the visualization. Defaults to None.</p> <code>None</code> <code>file_name</code> \u00b6 <code>Optional[str]</code> <p>File name for the saved visualization. Defaults to \"prediction_visual\".</p> <code>'prediction_visual'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the visualized image and the elapsed time for the visualization process.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def visualize_prediction(\n    image: np.ndarray,\n    boxes: list[list],\n    classes: list[str],\n    masks: list[np.ndarray] | None = None,\n    rect_th: int | None = None,\n    text_size: float | None = None,\n    text_th: int | None = None,\n    color: tuple | None = None,\n    hide_labels: bool = False,\n    output_dir: str | None = None,\n    file_name: str | None = \"prediction_visual\",\n):\n    \"\"\"Visualizes prediction classes, bounding boxes over the source image and exports it to output folder.\n\n    Args:\n        image (np.ndarray): The source image.\n        boxes (List[List]): List of bounding boxes coordinates.\n        classes (List[str]): List of class labels corresponding to each bounding box.\n        masks (Optional[List[np.ndarray]], optional): List of masks corresponding to each bounding box. Defaults to None.\n        rect_th (int, optional): Thickness of the bounding box rectangle. Defaults to None.\n        text_size (float, optional): Size of the text for class labels. Defaults to None.\n        text_th (int, optional): Thickness of the text for class labels. Defaults to None.\n        color (tuple, optional): Color of the bounding box and text. Defaults to None.\n        hide_labels (bool, optional): Whether to hide the class labels. Defaults to False.\n        output_dir (Optional[str], optional): Output directory to save the visualization. Defaults to None.\n        file_name (Optional[str], optional): File name for the saved visualization. Defaults to \"prediction_visual\".\n\n    Returns:\n        dict: A dictionary containing the visualized image and the elapsed time for the visualization process.\n    \"\"\"  # noqa\n\n    elapsed_time = time.time()\n    # deepcopy image so that original is not altered\n    image = copy.deepcopy(image)\n    # select predefined classwise color palette if not specified\n    if color is None:\n        colors = Colors()\n    else:\n        colors = None\n    # set rect_th for boxes\n    rect_th = rect_th or max(round(sum(image.shape) / 2 * 0.003), 2)\n    # set text_th for category names\n    text_th = text_th or max(rect_th - 1, 1)\n    # set text_size for category names\n    text_size = text_size or rect_th / 3\n\n    # add masks to image if present\n    if masks is not None and color is None:\n        logger.error(\"Cannot add mask, no color tuple given\")\n    elif masks is not None and color is not None:\n        for mask in masks:\n            # deepcopy mask so that original is not altered\n            mask = copy.deepcopy(mask)\n            # draw mask\n            rgb_mask = apply_color_mask(np.squeeze(mask), color)\n            image = cv2.addWeighted(image, 1, rgb_mask, 0.6, 0)\n\n    # add bboxes to image if present\n    for box_indice in range(len(boxes)):\n        # deepcopy boxso that original is not altered\n        box = copy.deepcopy(boxes[box_indice])\n        class_ = classes[box_indice]\n\n        # set color\n        if colors is not None:\n            mycolor = colors(class_)\n        elif color is not None:\n            mycolor = color\n        else:\n            logger.error(\"color cannot be defined\")\n            continue\n\n        # set bbox points\n        point1, point2 = [int(box[0]), int(box[1])], [int(box[2]), int(box[3])]\n        # visualize boxes\n        cv2.rectangle(\n            image,\n            point1,\n            point2,\n            color=mycolor,\n            thickness=rect_th,\n        )\n\n        if not hide_labels:\n            # arange bounding box text location\n            label = f\"{class_}\"\n            box_width, box_height = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[\n                0\n            ]  # label width, height\n            outside = point1[1] - box_height - 3 &gt;= 0  # label fits outside box\n            point2 = point1[0] + box_width, point1[1] - box_height - 3 if outside else point1[1] + box_height + 3\n            # add bounding box text\n            cv2.rectangle(image, point1, point2, color or (0, 0, 0), -1, cv2.LINE_AA)  # filled\n            cv2.putText(\n                image,\n                label,\n                (point1[0], point1[1] - 2 if outside else point1[1] + box_height + 2),\n                0,\n                text_size,\n                (255, 255, 255),\n                thickness=text_th,\n            )\n    if output_dir:\n        # create output folder if not present\n        Path(output_dir).mkdir(parents=True, exist_ok=True)\n        # save inference result\n        save_path = os.path.join(output_dir, (file_name or \"unknown\") + \".png\")\n        cv2.imwrite(save_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n\n    elapsed_time = time.time() - elapsed_time\n    return {\"image\": image, \"elapsed_time\": elapsed_time}\n</code></pre>"},{"location":"api/#sahi.utils.detectron2","title":"<code>detectron2</code>","text":"Functions\u00b6 <code>export_cfg_as_yaml(cfg, export_path='config.yaml')</code> \u00b6 <p>Exports Detectron2 config object in yaml format so that it can be used later.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> \u00b6 <code>CfgNode</code> <p>Detectron2 config object.</p> required <code>export_path</code> \u00b6 <code>str</code> <p>Path to export the Detectron2 config.</p> <code>'config.yaml'</code> <p>Related Detectron2 doc: https://detectron2.readthedocs.io/en/stable/modules/config.html#detectron2.config.CfgNode.dump</p> Source code in <code>sahi/utils/detectron2.py</code> <pre><code>def export_cfg_as_yaml(cfg, export_path: str = \"config.yaml\"):\n    \"\"\"Exports Detectron2 config object in yaml format so that it can be used later.\n\n    Args:\n        cfg (detectron2.config.CfgNode): Detectron2 config object.\n        export_path (str): Path to export the Detectron2 config.\n    Related Detectron2 doc: https://detectron2.readthedocs.io/en/stable/modules/config.html#detectron2.config.CfgNode.dump\n    \"\"\"\n    Path(export_path).parent.mkdir(exist_ok=True, parents=True)\n\n    with open(export_path, \"w\") as f:\n        f.write(cfg.dump())\n</code></pre>"},{"location":"api/#sahi.utils.file","title":"<code>file</code>","text":"Functions\u00b6 <code>download_from_url(from_url, to_path)</code> \u00b6 <p>Downloads a file from the given URL and saves it to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>from_url</code> \u00b6 <code>str</code> <p>The URL of the file to download.</p> required <code>to_path</code> \u00b6 <code>str</code> <p>The path where the downloaded file should be saved.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def download_from_url(from_url: str, to_path: str):\n    \"\"\"Downloads a file from the given URL and saves it to the specified path.\n\n    Args:\n        from_url (str): The URL of the file to download.\n        to_path (str): The path where the downloaded file should be saved.\n\n    Returns:\n        None\n    \"\"\"\n    Path(to_path).parent.mkdir(parents=True, exist_ok=True)\n\n    if not os.path.exists(to_path):\n        urllib.request.urlretrieve(\n            from_url,\n            to_path,\n        )\n</code></pre> <code></code> <code>get_base_filename(path)</code> \u00b6 <p>Takes a file path, returns (base_filename_with_extension, base_filename_without_extension)</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def get_base_filename(path: str):\n    \"\"\"Takes a file path, returns (base_filename_with_extension, base_filename_without_extension)\"\"\"\n    base_filename_with_extension = ntpath.basename(path)\n    base_filename_without_extension, _ = os.path.splitext(base_filename_with_extension)\n    return base_filename_with_extension, base_filename_without_extension\n</code></pre> <code></code> <code>get_file_extension(path)</code> \u00b6 <p>Get the file extension from a given file path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> \u00b6 <code>str</code> <p>The file path.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The file extension.</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def get_file_extension(path: str):\n    \"\"\"Get the file extension from a given file path.\n\n    Args:\n        path (str): The file path.\n\n    Returns:\n        str: The file extension.\n    \"\"\"\n    _, file_extension = os.path.splitext(path)\n    return file_extension\n</code></pre> <code></code> <code>import_model_class(model_type, class_name)</code> \u00b6 <p>Imports a predefined detection class by class name.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> \u00b6 <p>str \"yolov5\", \"detectron2\", \"mmdet\", \"huggingface\" etc</p> required <code>model_name</code> \u00b6 <p>str Name of the detection model class (example: \"MmdetDetectionModel\")</p> required <p>Returns:     class_: class with given path</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def import_model_class(model_type, class_name):\n    \"\"\"Imports a predefined detection class by class name.\n\n    Args:\n        model_type: str\n            \"yolov5\", \"detectron2\", \"mmdet\", \"huggingface\" etc\n        model_name: str\n            Name of the detection model class (example: \"MmdetDetectionModel\")\n    Returns:\n        class_: class with given path\n    \"\"\"\n    module = __import__(f\"sahi.models.{model_type}\", fromlist=[class_name])\n    class_ = getattr(module, class_name)\n    return class_\n</code></pre> <code></code> <code>increment_path(path, exist_ok=True, sep='')</code> \u00b6 <p>Increment path, i.e. runs/exp --&gt; runs/exp{sep}0, runs/exp{sep}1 etc.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> \u00b6 <code>str | Path</code> <p>str The base path to increment.</p> required <code>exist_ok</code> \u00b6 <code>bool</code> <p>bool If True, return the path as is if it already exists. If False, increment the path.</p> <code>True</code> <code>sep</code> \u00b6 <code>str</code> <p>str The separator to use between the base path and the increment number.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The incremented path.</p> Example <p>increment_path(\"runs/exp\", sep=\"\") 'runs/exp_0' increment_path(\"runs/exp_0\", sep=\"\") 'runs/exp_1'</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def increment_path(path: str | Path, exist_ok: bool = True, sep: str = \"\") -&gt; str:\n    \"\"\"Increment path, i.e. runs/exp --&gt; runs/exp{sep}0, runs/exp{sep}1 etc.\n\n    Args:\n        path: str\n            The base path to increment.\n        exist_ok: bool\n            If True, return the path as is if it already exists. If False, increment the path.\n        sep: str\n            The separator to use between the base path and the increment number.\n\n    Returns:\n        str: The incremented path.\n\n    Example:\n        &gt;&gt;&gt; increment_path(\"runs/exp\", sep=\"_\")\n        'runs/exp_0'\n        &gt;&gt;&gt; increment_path(\"runs/exp_0\", sep=\"_\")\n        'runs/exp_1'\n    \"\"\"\n    path = Path(path)  # os-agnostic\n    if (path.exists() and exist_ok) or (not path.exists()):\n        return str(path)\n    else:\n        dirs = glob.glob(f\"{path}{sep}*\")  # similar paths\n        matches = [re.search(rf\"%s{sep}(\\d+)\" % path.stem, d) for d in dirs]\n        indices = [int(m.groups()[0]) for m in matches if m]  # indices\n        n = max(indices) + 1 if indices else 2  # increment number\n        return f\"{path}{sep}{n}\"  # update path\n</code></pre> <code></code> <code>is_colab()</code> \u00b6 <p>Check if the current environment is a Google Colab instance.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the environment is a Google Colab instance, False otherwise.</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def is_colab():\n    \"\"\"Check if the current environment is a Google Colab instance.\n\n    Returns:\n        bool: True if the environment is a Google Colab instance, False otherwise.\n    \"\"\"\n    import sys\n\n    return \"google.colab\" in sys.modules\n</code></pre> <code></code> <code>list_files(directory, contains=['.json'], verbose=1)</code> \u00b6 <p>Walk given directory and return a list of file path with desired extension.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> \u00b6 <code>str</code> <p>str \"data/coco/\"</p> required <code>contains</code> \u00b6 <code>list</code> <p>list A list of strings to check if the target file contains them, example: [\"coco.png\", \".jpg\", \"jpeg\"]</p> <code>['.json']</code> <code>verbose</code> \u00b6 <code>int</code> <p>int 0: no print 1: print number of files</p> <code>1</code> <p>Returns:</p> Name Type Description <code>filepath_list</code> <code>list[str]</code> <p>list List of file paths</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def list_files(\n    directory: str,\n    contains: list = [\".json\"],\n    verbose: int = 1,\n) -&gt; list[str]:\n    \"\"\"Walk given directory and return a list of file path with desired extension.\n\n    Args:\n        directory: str\n            \"data/coco/\"\n        contains: list\n            A list of strings to check if the target file contains them, example: [\"coco.png\", \".jpg\", \"jpeg\"]\n        verbose: int\n            0: no print\n            1: print number of files\n\n    Returns:\n        filepath_list : list\n            List of file paths\n    \"\"\"\n    # define verboseprint\n    verboseprint = print if verbose else lambda *a, **k: None\n\n    filepath_list: list[str] = []\n\n    for file in os.listdir(directory):\n        # check if filename contains any of the terms given in contains list\n        if any(strtocheck in file.lower() for strtocheck in contains):\n            filepath = str(os.path.join(directory, file))\n            filepath_list.append(filepath)\n\n    number_of_files = len(filepath_list)\n    folder_name = Path(directory).name\n\n    verboseprint(f\"There are {number_of_files!s} listed files in folder: {folder_name}/\")\n\n    return filepath_list\n</code></pre> <code></code> <code>list_files_recursively(directory, contains=['.json'], verbose=True)</code> \u00b6 <p>Walk given directory recursively and return a list of file path with desired extension.</p> <p>Parameters:</p> Name Type Description Default <code>directory </code> \u00b6 <p>str \"data/coco/\"</p> required <code>contains </code> \u00b6 <p>list A list of strings to check if the target file contains them, example: [\"coco.png\", \".jpg\", \"jpeg\"]</p> required <code>verbose </code> \u00b6 <p>bool If true, prints some results</p> required <p>Returns:</p> Name Type Description <code>relative_filepath_list</code> <code>list</code> <p>list List of file paths relative to given directory</p> <code>abs_filepath_list</code> <code>list</code> <p>list List of absolute file paths</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def list_files_recursively(directory: str, contains: list = [\".json\"], verbose: bool = True) -&gt; tuple[list, list]:\n    \"\"\"Walk given directory recursively and return a list of file path with desired extension.\n\n    Args:\n        directory : str\n            \"data/coco/\"\n        contains : list\n            A list of strings to check if the target file contains them, example: [\"coco.png\", \".jpg\", \"jpeg\"]\n        verbose : bool\n            If true, prints some results\n\n    Returns:\n        relative_filepath_list : list\n            List of file paths relative to given directory\n        abs_filepath_list : list\n            List of absolute file paths\n    \"\"\"\n\n    # define verboseprint\n    verboseprint = print if verbose else lambda *a, **k: None\n\n    # walk directories recursively and find json files\n    abs_filepath_list = []\n    relative_filepath_list = []\n\n    # r=root, d=directories, f=files\n    for r, _, f in os.walk(directory):\n        for file in f:\n            # check if filename contains any of the terms given in contains list\n            if any(strtocheck in file.lower() for strtocheck in contains):\n                abs_filepath = os.path.join(r, file)\n                abs_filepath_list.append(abs_filepath)\n                relative_filepath = abs_filepath.split(directory)[-1]\n                relative_filepath_list.append(relative_filepath)\n\n    number_of_files = len(relative_filepath_list)\n    folder_name = directory.split(os.sep)[-1]\n\n    verboseprint(f\"There are {number_of_files} listed files in folder {folder_name}.\")\n\n    return relative_filepath_list, abs_filepath_list\n</code></pre> <code></code> <code>load_json(load_path, encoding='utf-8')</code> \u00b6 <p>Loads json formatted data (given as \"data\") from load_path Encoding type can be specified with 'encoding' argument.</p> <p>Parameters:</p> Name Type Description Default <code>load_path</code> \u00b6 <code>str</code> <p>str \"dirname/coco.json\"</p> required <code>encoding</code> \u00b6 <code>str</code> <p>str Encoding type, default is 'utf-8'</p> <code>'utf-8'</code> Example inputs <p>load_path: \"dirname/coco.json\"</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def load_json(load_path: str, encoding: str = \"utf-8\"):\n    \"\"\"Loads json formatted data (given as \"data\") from load_path Encoding type can be specified with 'encoding'\n    argument.\n\n    Args:\n        load_path: str\n            \"dirname/coco.json\"\n        encoding: str\n            Encoding type, default is 'utf-8'\n\n    Example inputs:\n        load_path: \"dirname/coco.json\"\n    \"\"\"\n    # read from path\n    with open(load_path, encoding=encoding) as json_file:\n        data = json.load(json_file)\n    return data\n</code></pre> <code></code> <code>load_pickle(load_path)</code> \u00b6 <p>Loads pickle formatted data (given as \"data\") from load_path</p> <p>Parameters:</p> Name Type Description Default <code>load_path</code> \u00b6 <p>str \"dirname/coco.pickle\"</p> required Example inputs <p>load_path: \"dirname/coco.pickle\"</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def load_pickle(load_path):\n    \"\"\"\n    Loads pickle formatted data (given as \"data\") from load_path\n\n    Args:\n        load_path: str\n            \"dirname/coco.pickle\"\n\n    Example inputs:\n        load_path: \"dirname/coco.pickle\"\n    \"\"\"\n    with open(load_path, \"rb\") as json_file:\n        data = pickle.load(json_file)\n    return data\n</code></pre> <code></code> <code>save_json(data, save_path, indent=None)</code> \u00b6 <p>Saves json formatted data (given as \"data\") as save_path</p> <p>Parameters:</p> Name Type Description Default <code>data</code> \u00b6 <p>dict Data to be saved as json</p> required <code>save_path</code> \u00b6 <p>str \"dirname/coco.json\"</p> required <code>indent</code> \u00b6 <code>int | None</code> <p>int or None Indentation level for pretty-printing the JSON data. If None, the most compact representation will be used. If an integer is provided, it specifies the number of spaces to use for indentation. Example: indent=4 will format the JSON data with an indentation of 4 spaces per level.</p> <code>None</code> Example inputs <p>data: {\"image_id\": 5} save_path: \"dirname/coco.json\" indent: Train json files with indent=None, val json files with indent=4</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def save_json(data, save_path, indent: int | None = None):\n    \"\"\"\n    Saves json formatted data (given as \"data\") as save_path\n\n    Args:\n        data: dict\n            Data to be saved as json\n        save_path: str\n            \"dirname/coco.json\"\n        indent: int or None\n            Indentation level for pretty-printing the JSON data. If None, the most compact representation\n            will be used. If an integer is provided, it specifies the number of spaces to use for indentation.\n            Example: indent=4 will format the JSON data with an indentation of 4 spaces per level.\n\n    Example inputs:\n        data: {\"image_id\": 5}\n        save_path: \"dirname/coco.json\"\n        indent: Train json files with indent=None, val json files with indent=4\n    \"\"\"\n    # create dir if not present\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n\n    # export as json\n    with open(save_path, \"w\", encoding=\"utf-8\") as outfile:\n        json.dump(data, outfile, separators=(\",\", \":\"), cls=NumpyEncoder, indent=indent)\n</code></pre> <code></code> <code>save_pickle(data, save_path)</code> \u00b6 <p>Saves pickle formatted data (given as \"data\") as save_path</p> <p>Parameters:</p> Name Type Description Default <code>data</code> \u00b6 <p>dict Data to be saved as pickle</p> required <code>save_path</code> \u00b6 <p>str \"dirname/coco.pickle\"</p> required Example inputs <p>data: {\"image_id\": 5} save_path: \"dirname/coco.pickle\"</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def save_pickle(data, save_path):\n    \"\"\"\n    Saves pickle formatted data (given as \"data\") as save_path\n\n    Args:\n        data: dict\n            Data to be saved as pickle\n        save_path: str\n            \"dirname/coco.pickle\"\n\n    Example inputs:\n        data: {\"image_id\": 5}\n        save_path: \"dirname/coco.pickle\"\n    \"\"\"\n    # create dir if not present\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n\n    # export as json\n    with open(save_path, \"wb\") as outfile:\n        pickle.dump(data, outfile)\n</code></pre> <code></code> <code>unzip(file_path, dest_dir)</code> \u00b6 <p>Unzips compressed .zip file.</p> Example inputs <p>file_path: 'data/01_alb_id.zip' dest_dir: 'data/'</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def unzip(file_path: str, dest_dir: str):\n    \"\"\"Unzips compressed .zip file.\n\n    Example inputs:\n        file_path: 'data/01_alb_id.zip'\n        dest_dir: 'data/'\n    \"\"\"\n\n    # unzip file\n    with zipfile.ZipFile(file_path) as zf:\n        zf.extractall(dest_dir)\n</code></pre>"},{"location":"api/#sahi.utils.import_utils","title":"<code>import_utils</code>","text":"Functions\u00b6 <code>check_package_minimum_version(package_name, minimum_version, verbose=False)</code> \u00b6 <p>Raise error if module version is not compatible.</p> Source code in <code>sahi/utils/import_utils.py</code> <pre><code>def check_package_minimum_version(package_name: str, minimum_version: str, verbose=False):\n    \"\"\"Raise error if module version is not compatible.\"\"\"\n    from packaging import version\n\n    _is_available, _version = get_package_info(package_name, verbose=verbose)\n    if _is_available:\n        if _version == \"unknown\":\n            logger.warning(\n                f\"Could not determine version of {package_name}. Assuming version {minimum_version} is compatible.\"\n            )\n        else:\n            if version.parse(_version) &lt; version.parse(minimum_version):\n                return False\n    return True\n</code></pre> <code></code> <code>check_requirements(package_names)</code> \u00b6 <p>Raise error if module is not installed.</p> Source code in <code>sahi/utils/import_utils.py</code> <pre><code>def check_requirements(package_names):\n    \"\"\"Raise error if module is not installed.\"\"\"\n    missing_packages = []\n    for package_name in package_names:\n        if importlib.util.find_spec(package_name) is None:\n            missing_packages.append(package_name)\n    if missing_packages:\n        raise ImportError(f\"The following packages are required to use this module: {missing_packages}\")\n    yield\n</code></pre> <code></code> <code>ensure_package_minimum_version(package_name, minimum_version, verbose=False)</code> \u00b6 <p>Raise error if module version is not compatible.</p> Source code in <code>sahi/utils/import_utils.py</code> <pre><code>def ensure_package_minimum_version(package_name: str, minimum_version: str, verbose=False):\n    \"\"\"Raise error if module version is not compatible.\"\"\"\n    from packaging import version\n\n    _is_available, _version = get_package_info(package_name, verbose=verbose)\n    if _is_available:\n        if _version == \"unknown\":\n            logger.warning(\n                f\"Could not determine version of {package_name}. Assuming version {minimum_version} is compatible.\"\n            )\n        else:\n            if version.parse(_version) &lt; version.parse(minimum_version):\n                raise ImportError(\n                    f\"Please upgrade {package_name} to version {minimum_version} or higher to use this module.\"\n                )\n    yield\n</code></pre> <code></code> <code>get_package_info(package_name, verbose=True)</code> \u00b6 <p>Returns the package version as a string and the package name as a string.</p> Source code in <code>sahi/utils/import_utils.py</code> <pre><code>def get_package_info(package_name: str, verbose: bool = True):\n    \"\"\"Returns the package version as a string and the package name as a string.\"\"\"\n    _is_available = is_available(package_name)\n\n    if _is_available:\n        try:\n            import importlib.metadata as _importlib_metadata\n\n            _version = _importlib_metadata.version(package_name)\n        except (ModuleNotFoundError, AttributeError):\n            try:\n                _version = importlib.import_module(package_name).__version__\n            except AttributeError:\n                _version = \"unknown\"\n        if verbose:\n            logger.pkg_info(f\"{package_name} version {_version} is available.\")\n    else:\n        _version = \"N/A\"\n\n    return _is_available, _version\n</code></pre>"},{"location":"api/#sahi.utils.mmdet","title":"<code>mmdet</code>","text":"Functions\u00b6 <code>download_mmdet_config(model_name='cascade_rcnn', config_file_name='cascade_mask_rcnn_r50_fpn_1x_coco.py', verbose=True)</code> \u00b6 <p>Merges config files starting from given main config file name. Saves as single file.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> \u00b6 <code>str</code> <p>mmdet model name. check https://github.com/open-mmlab/mmdetection/tree/master/configs.</p> <code>'cascade_rcnn'</code> <code>config_file_name</code> \u00b6 <code>str</code> <p>mdmet config file name.</p> <code>'cascade_mask_rcnn_r50_fpn_1x_coco.py'</code> <code>verbose</code> \u00b6 <code>bool</code> <p>if True, print save path.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>(str) abs path of the downloaded config file.</p> Source code in <code>sahi/utils/mmdet.py</code> <pre><code>def download_mmdet_config(\n    model_name: str = \"cascade_rcnn\",\n    config_file_name: str = \"cascade_mask_rcnn_r50_fpn_1x_coco.py\",\n    verbose: bool = True,\n) -&gt; str:\n    \"\"\"Merges config files starting from given main config file name. Saves as single file.\n\n    Args:\n        model_name (str): mmdet model name. check https://github.com/open-mmlab/mmdetection/tree/master/configs.\n        config_file_name (str): mdmet config file name.\n        verbose (bool): if True, print save path.\n\n    Returns:\n        (str) abs path of the downloaded config file.\n    \"\"\"\n\n    # get mmdet version\n    from mmdet import __version__\n\n    mmdet_ver = \"v\" + __version__\n\n    # set main config url\n    base_config_url = (\n        \"https://raw.githubusercontent.com/open-mmlab/mmdetection/\" + mmdet_ver + \"/configs/\" + model_name + \"/\"\n    )\n    main_config_url = base_config_url + config_file_name\n\n    # set final config dirs\n    configs_dir = Path(\"mmdet_configs\") / mmdet_ver\n    model_config_dir = configs_dir / model_name\n\n    # create final config dir\n    configs_dir.mkdir(parents=True, exist_ok=True)\n    model_config_dir.mkdir(parents=True, exist_ok=True)\n\n    # get final config file name\n    filename = Path(main_config_url).name\n\n    # set final config file path\n    final_config_path = str(model_config_dir / filename)\n\n    if not Path(final_config_path).exists():\n        # set config dirs\n        temp_configs_dir = Path(\"temp_mmdet_configs\")\n        main_config_dir = temp_configs_dir / model_name\n\n        # create config dirs\n        temp_configs_dir.mkdir(parents=True, exist_ok=True)\n        main_config_dir.mkdir(parents=True, exist_ok=True)\n\n        # get main config file name\n        filename = Path(main_config_url).name\n\n        # set main config file path\n        main_config_path = str(main_config_dir / filename)\n\n        # download main config file\n        urllib.request.urlretrieve(\n            main_config_url,\n            main_config_path,\n        )\n\n        # read main config file\n        sys.path.insert(0, str(main_config_dir))\n        temp_module_name = path.splitext(filename)[0]\n        mod = import_module(temp_module_name)\n        sys.path.pop(0)\n        config_dict = {name: value for name, value in mod.__dict__.items() if not name.startswith(\"__\")}\n\n        # handle when config_dict[\"_base_\"] is string\n        if not isinstance(config_dict[\"_base_\"], list):\n            config_dict[\"_base_\"] = [config_dict[\"_base_\"]]\n\n        # iterate over secondary config files\n        for secondary_config_file_path in config_dict[\"_base_\"]:\n            # set config url\n            config_url = base_config_url + secondary_config_file_path\n            config_path = main_config_dir / secondary_config_file_path\n\n            # create secondary config dir\n            config_path.parent.mkdir(parents=True, exist_ok=True)\n\n            # download secondary config files\n            urllib.request.urlretrieve(\n                config_url,\n                str(config_path),\n            )\n\n            # read secondary config file\n            secondary_config_dir = config_path.parent\n            sys.path.insert(0, str(secondary_config_dir))\n            temp_module_name = path.splitext(Path(config_path).name)[0]\n            mod = import_module(temp_module_name)\n            sys.path.pop(0)\n            secondary_config_dict = {name: value for name, value in mod.__dict__.items() if not name.startswith(\"__\")}\n\n            # go deeper if there are more steps\n            if secondary_config_dict.get(\"_base_\") is not None:\n                # handle when config_dict[\"_base_\"] is string\n                if not isinstance(secondary_config_dict[\"_base_\"], list):\n                    secondary_config_dict[\"_base_\"] = [secondary_config_dict[\"_base_\"]]\n\n                # iterate over third config files\n                for third_config_file_path in secondary_config_dict[\"_base_\"]:\n                    # set config url\n                    config_url = base_config_url + third_config_file_path\n                    config_path = main_config_dir / third_config_file_path\n\n                    # create secondary config dir\n                    config_path.parent.mkdir(parents=True, exist_ok=True)\n                    # download secondary config files\n                    urllib.request.urlretrieve(\n                        config_url,\n                        str(config_path),\n                    )\n\n        from mmengine import Config\n        # dump final config as single file\n\n        config = Config.fromfile(main_config_path)\n        config.dump(final_config_path)\n\n        if verbose:\n            print(f\"mmdet config file has been downloaded to {path.abspath(final_config_path)}\")\n\n        # remove temp config dir\n        shutil.rmtree(temp_configs_dir)\n\n    return path.abspath(final_config_path)\n</code></pre>"},{"location":"api/#sahi.utils.shapely","title":"<code>shapely</code>","text":"Classes\u00b6 <code>ShapelyAnnotation</code> \u00b6 <p>Creates ShapelyAnnotation (as shapely MultiPolygon).</p> <p>Can convert this instance annotation to various formats.</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>class ShapelyAnnotation:\n    \"\"\"Creates ShapelyAnnotation (as shapely MultiPolygon).\n\n    Can convert this instance annotation to various formats.\n    \"\"\"\n\n    @classmethod\n    def from_coco_segmentation(cls, segmentation, slice_bbox=None):\n        \"\"\"Init ShapelyAnnotation from coco segmentation.\n\n        segmentation : List[List]\n            [[1, 1, 325, 125, 250, 200, 5, 200]]\n        slice_bbox (List[int]): [xmin, ymin, width, height]\n            Should have the same format as the output of the get_bbox_from_shapely function.\n            Is used to calculate sliced coco coordinates.\n        \"\"\"\n        shapely_multipolygon = get_shapely_multipolygon(segmentation)\n        return cls(multipolygon=shapely_multipolygon, slice_bbox=slice_bbox)\n\n    @classmethod\n    def from_coco_bbox(cls, bbox: list[int], slice_bbox: list[int] | None = None):\n        \"\"\"Init ShapelyAnnotation from coco bbox.\n\n        bbox (List[int]): [xmin, ymin, width, height] slice_bbox (List[int]): [x_min, y_min, x_max, y_max] Is used\n        to calculate sliced coco coordinates.\n        \"\"\"\n        shapely_polygon = get_shapely_box(x=bbox[0], y=bbox[1], width=bbox[2], height=bbox[3])\n        shapely_multipolygon = MultiPolygon([shapely_polygon])\n        return cls(multipolygon=shapely_multipolygon, slice_bbox=slice_bbox)\n\n    def __init__(self, multipolygon: MultiPolygon, slice_bbox=None):\n        self.multipolygon = multipolygon\n        self.slice_bbox = slice_bbox\n\n    @property\n    def multipolygon(self):\n        return self.__multipolygon\n\n    @property\n    def area(self):\n        return int(self.__area)\n\n    @multipolygon.setter\n    def multipolygon(self, multipolygon: MultiPolygon):\n        self.__multipolygon = multipolygon\n        # calculate areas of all polygons\n        area = 0\n        for shapely_polygon in multipolygon.geoms:\n            area += shapely_polygon.area\n        # set instance area\n        self.__area = area\n\n    def to_list(self):\n        \"\"\"\n        [\n            [(x1, y1), (x2, y2), (x3, y3), ...],\n            [(x1, y1), (x2, y2), (x3, y3), ...],\n            ...\n        ]\n        \"\"\"\n        list_of_list_of_points: list = []\n        for shapely_polygon in self.multipolygon.geoms:\n            # create list_of_points for selected shapely_polygon\n            if shapely_polygon.area != 0:\n                x_coords = shapely_polygon.exterior.coords.xy[0]\n                y_coords = shapely_polygon.exterior.coords.xy[1]\n                # fix coord by slice_bbox\n                if self.slice_bbox:\n                    minx = self.slice_bbox[0]\n                    miny = self.slice_bbox[1]\n                    x_coords = [x_coord - minx for x_coord in x_coords]\n                    y_coords = [y_coord - miny for y_coord in y_coords]\n                list_of_points = list(zip(x_coords, y_coords))\n            else:\n                list_of_points = []\n            # append list_of_points to list_of_list_of_points\n            list_of_list_of_points.append(list_of_points)\n        # return result\n        return list_of_list_of_points\n\n    def to_coco_segmentation(self):\n        \"\"\"\n        [\n            [x1, y1, x2, y2, x3, y3, ...],\n            [x1, y1, x2, y2, x3, y3, ...],\n            ...\n        ]\n        \"\"\"\n        coco_segmentation: list = []\n        for shapely_polygon in self.multipolygon.geoms:\n            # create list_of_points for selected shapely_polygon\n            if shapely_polygon.area != 0:\n                x_coords = shapely_polygon.exterior.coords.xy[0]\n                y_coords = shapely_polygon.exterior.coords.xy[1]\n                # fix coord by slice_bbox\n                if self.slice_bbox:\n                    minx = self.slice_bbox[0]\n                    miny = self.slice_bbox[1]\n                    x_coords = [x_coord - minx for x_coord in x_coords]\n                    y_coords = [y_coord - miny for y_coord in y_coords]\n                # convert intersection to coco style segmentation annotation\n                coco_polygon: list[None | int] = [None] * (len(x_coords) * 2)\n                coco_polygon[0::2] = [int(coord) for coord in x_coords]\n                coco_polygon[1::2] = [int(coord) for coord in y_coords]\n            else:\n                coco_polygon = []\n            # remove if first and last points are duplicate\n            if coco_polygon[:2] == coco_polygon[-2:]:\n                del coco_polygon[-2:]\n            # append coco_polygon to coco_segmentation\n            coco_polygon = [point for point in coco_polygon] if coco_polygon else coco_polygon\n            coco_segmentation.append(coco_polygon)\n        return coco_segmentation\n\n    def to_opencv_contours(self):\n        \"\"\"[ [[[1, 1]], [[325, 125]], [[250, 200]], [[5, 200]]], [[[1, 1]], [[325, 125]], [[250, 200]], [[5, 200]]] ]\"\"\"\n        opencv_contours: list = []\n        for shapely_polygon in self.multipolygon.geoms:\n            # create opencv_contour for selected shapely_polygon\n            if shapely_polygon.area != 0:\n                x_coords = shapely_polygon.exterior.coords.xy[0]\n                y_coords = shapely_polygon.exterior.coords.xy[1]\n                # fix coord by slice_bbox\n                if self.slice_bbox:\n                    minx = self.slice_bbox[0]\n                    miny = self.slice_bbox[1]\n                    x_coords = [x_coord - minx for x_coord in x_coords]\n                    y_coords = [y_coord - miny for y_coord in y_coords]\n                opencv_contour = [[[int(x_coords[ind]), int(y_coords[ind])]] for ind in range(len(x_coords))]\n            else:\n                opencv_contour: list = []\n            # append opencv_contour to opencv_contours\n            opencv_contours.append(opencv_contour)\n        # return result\n        return opencv_contours\n\n    def to_xywh(self):\n        \"\"\"[xmin, ymin, width, height]\"\"\"\n        if self.multipolygon.area != 0:\n            coco_bbox, _ = get_bbox_from_shapely(self.multipolygon)\n            # fix coord by slice box\n            if self.slice_bbox:\n                minx = self.slice_bbox[0]\n                miny = self.slice_bbox[1]\n                coco_bbox[0] = coco_bbox[0] - minx\n                coco_bbox[1] = coco_bbox[1] - miny\n        else:\n            coco_bbox: list = []\n        return coco_bbox\n\n    def to_coco_bbox(self):\n        \"\"\"[xmin, ymin, width, height]\"\"\"\n        return self.to_xywh()\n\n    def to_xyxy(self):\n        \"\"\"[xmin, ymin, xmax, ymax]\"\"\"\n        if self.multipolygon.area != 0:\n            _, voc_bbox = get_bbox_from_shapely(self.multipolygon)\n            # fix coord by slice box\n            if self.slice_bbox:\n                minx = self.slice_bbox[0]\n                miny = self.slice_bbox[1]\n                voc_bbox[0] = voc_bbox[0] - minx\n                voc_bbox[2] = voc_bbox[2] - minx\n                voc_bbox[1] = voc_bbox[1] - miny\n                voc_bbox[3] = voc_bbox[3] - miny\n        else:\n            voc_bbox = []\n        return voc_bbox\n\n    def to_voc_bbox(self):\n        \"\"\"[xmin, ymin, xmax, ymax]\"\"\"\n        return self.to_xyxy()\n\n    def get_convex_hull_shapely_annotation(self):\n        shapely_multipolygon = MultiPolygon([self.multipolygon.convex_hull])\n        shapely_annotation = ShapelyAnnotation(shapely_multipolygon)\n        return shapely_annotation\n\n    def get_simplified_shapely_annotation(self, tolerance=1):\n        shapely_multipolygon = MultiPolygon([self.multipolygon.simplify(tolerance)])\n        shapely_annotation = ShapelyAnnotation(shapely_multipolygon)\n        return shapely_annotation\n\n    def get_buffered_shapely_annotation(\n        self,\n        distance=3,\n        resolution=16,\n        quadsegs=None,\n        cap_style=CAP_STYLE.round,\n        join_style=JOIN_STYLE.round,\n        mitre_limit=5.0,\n        single_sided=False,\n    ):\n        \"\"\"Approximates the present polygon to have a valid polygon shape.\n\n        For more, check: https://shapely.readthedocs.io/en/stable/manual.html#object.buffer\n        \"\"\"\n        buffered_polygon = self.multipolygon.buffer(\n            distance=distance,\n            resolution=resolution,\n            quadsegs=quadsegs,\n            cap_style=cap_style,\n            join_style=join_style,\n            mitre_limit=mitre_limit,\n            single_sided=single_sided,\n        )\n        shapely_annotation = ShapelyAnnotation(MultiPolygon([buffered_polygon]))\n        return shapely_annotation\n\n    def get_intersection(self, polygon: Polygon):\n        \"\"\"Accepts shapely polygon object and returns the intersection in ShapelyAnnotation format.\"\"\"\n        # convert intersection polygon to list of tuples\n        intersection = self.multipolygon.intersection(polygon)\n        # if polygon is box then set slice_box property\n        if (\n            len(polygon.exterior.xy[0]) == 5\n            and polygon.exterior.xy[0][0] == polygon.exterior.xy[0][1]\n            and polygon.exterior.xy[0][2] == polygon.exterior.xy[0][3]\n        ):\n            coco_bbox, _ = get_bbox_from_shapely(polygon)\n            slice_bbox = coco_bbox\n        else:\n            slice_bbox = None\n        # convert intersection to multipolygon\n        if intersection.geom_type == \"Polygon\":\n            intersection_multipolygon = MultiPolygon([intersection])\n        elif intersection.geom_type == \"MultiPolygon\":\n            intersection_multipolygon = intersection\n        else:\n            intersection_multipolygon = MultiPolygon([])\n        # create shapely annotation from intersection multipolygon\n        intersection_shapely_annotation = ShapelyAnnotation(intersection_multipolygon, slice_bbox)\n\n        return intersection_shapely_annotation\n</code></pre> Functions\u00b6 <code></code> <code>from_coco_bbox(bbox, slice_bbox=None)</code> <code>classmethod</code> \u00b6 <p>Init ShapelyAnnotation from coco bbox.</p> <p>bbox (List[int]): [xmin, ymin, width, height] slice_bbox (List[int]): [x_min, y_min, x_max, y_max] Is used to calculate sliced coco coordinates.</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>@classmethod\ndef from_coco_bbox(cls, bbox: list[int], slice_bbox: list[int] | None = None):\n    \"\"\"Init ShapelyAnnotation from coco bbox.\n\n    bbox (List[int]): [xmin, ymin, width, height] slice_bbox (List[int]): [x_min, y_min, x_max, y_max] Is used\n    to calculate sliced coco coordinates.\n    \"\"\"\n    shapely_polygon = get_shapely_box(x=bbox[0], y=bbox[1], width=bbox[2], height=bbox[3])\n    shapely_multipolygon = MultiPolygon([shapely_polygon])\n    return cls(multipolygon=shapely_multipolygon, slice_bbox=slice_bbox)\n</code></pre> <code></code> <code>from_coco_segmentation(segmentation, slice_bbox=None)</code> <code>classmethod</code> \u00b6 <p>Init ShapelyAnnotation from coco segmentation.</p> List[List] <p>[[1, 1, 325, 125, 250, 200, 5, 200]]</p> <p>slice_bbox (List[int]): [xmin, ymin, width, height]     Should have the same format as the output of the get_bbox_from_shapely function.     Is used to calculate sliced coco coordinates.</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>@classmethod\ndef from_coco_segmentation(cls, segmentation, slice_bbox=None):\n    \"\"\"Init ShapelyAnnotation from coco segmentation.\n\n    segmentation : List[List]\n        [[1, 1, 325, 125, 250, 200, 5, 200]]\n    slice_bbox (List[int]): [xmin, ymin, width, height]\n        Should have the same format as the output of the get_bbox_from_shapely function.\n        Is used to calculate sliced coco coordinates.\n    \"\"\"\n    shapely_multipolygon = get_shapely_multipolygon(segmentation)\n    return cls(multipolygon=shapely_multipolygon, slice_bbox=slice_bbox)\n</code></pre> <code></code> <code>get_buffered_shapely_annotation(distance=3, resolution=16, quadsegs=None, cap_style=CAP_STYLE.round, join_style=JOIN_STYLE.round, mitre_limit=5.0, single_sided=False)</code> \u00b6 <p>Approximates the present polygon to have a valid polygon shape.</p> <p>For more, check: https://shapely.readthedocs.io/en/stable/manual.html#object.buffer</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>def get_buffered_shapely_annotation(\n    self,\n    distance=3,\n    resolution=16,\n    quadsegs=None,\n    cap_style=CAP_STYLE.round,\n    join_style=JOIN_STYLE.round,\n    mitre_limit=5.0,\n    single_sided=False,\n):\n    \"\"\"Approximates the present polygon to have a valid polygon shape.\n\n    For more, check: https://shapely.readthedocs.io/en/stable/manual.html#object.buffer\n    \"\"\"\n    buffered_polygon = self.multipolygon.buffer(\n        distance=distance,\n        resolution=resolution,\n        quadsegs=quadsegs,\n        cap_style=cap_style,\n        join_style=join_style,\n        mitre_limit=mitre_limit,\n        single_sided=single_sided,\n    )\n    shapely_annotation = ShapelyAnnotation(MultiPolygon([buffered_polygon]))\n    return shapely_annotation\n</code></pre> <code></code> <code>get_intersection(polygon)</code> \u00b6 <p>Accepts shapely polygon object and returns the intersection in ShapelyAnnotation format.</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>def get_intersection(self, polygon: Polygon):\n    \"\"\"Accepts shapely polygon object and returns the intersection in ShapelyAnnotation format.\"\"\"\n    # convert intersection polygon to list of tuples\n    intersection = self.multipolygon.intersection(polygon)\n    # if polygon is box then set slice_box property\n    if (\n        len(polygon.exterior.xy[0]) == 5\n        and polygon.exterior.xy[0][0] == polygon.exterior.xy[0][1]\n        and polygon.exterior.xy[0][2] == polygon.exterior.xy[0][3]\n    ):\n        coco_bbox, _ = get_bbox_from_shapely(polygon)\n        slice_bbox = coco_bbox\n    else:\n        slice_bbox = None\n    # convert intersection to multipolygon\n    if intersection.geom_type == \"Polygon\":\n        intersection_multipolygon = MultiPolygon([intersection])\n    elif intersection.geom_type == \"MultiPolygon\":\n        intersection_multipolygon = intersection\n    else:\n        intersection_multipolygon = MultiPolygon([])\n    # create shapely annotation from intersection multipolygon\n    intersection_shapely_annotation = ShapelyAnnotation(intersection_multipolygon, slice_bbox)\n\n    return intersection_shapely_annotation\n</code></pre> <code></code> <code>to_coco_bbox()</code> \u00b6 <p>[xmin, ymin, width, height]</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>def to_coco_bbox(self):\n    \"\"\"[xmin, ymin, width, height]\"\"\"\n    return self.to_xywh()\n</code></pre> <code></code> <code>to_coco_segmentation()</code> \u00b6 <p>[     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>def to_coco_segmentation(self):\n    \"\"\"\n    [\n        [x1, y1, x2, y2, x3, y3, ...],\n        [x1, y1, x2, y2, x3, y3, ...],\n        ...\n    ]\n    \"\"\"\n    coco_segmentation: list = []\n    for shapely_polygon in self.multipolygon.geoms:\n        # create list_of_points for selected shapely_polygon\n        if shapely_polygon.area != 0:\n            x_coords = shapely_polygon.exterior.coords.xy[0]\n            y_coords = shapely_polygon.exterior.coords.xy[1]\n            # fix coord by slice_bbox\n            if self.slice_bbox:\n                minx = self.slice_bbox[0]\n                miny = self.slice_bbox[1]\n                x_coords = [x_coord - minx for x_coord in x_coords]\n                y_coords = [y_coord - miny for y_coord in y_coords]\n            # convert intersection to coco style segmentation annotation\n            coco_polygon: list[None | int] = [None] * (len(x_coords) * 2)\n            coco_polygon[0::2] = [int(coord) for coord in x_coords]\n            coco_polygon[1::2] = [int(coord) for coord in y_coords]\n        else:\n            coco_polygon = []\n        # remove if first and last points are duplicate\n        if coco_polygon[:2] == coco_polygon[-2:]:\n            del coco_polygon[-2:]\n        # append coco_polygon to coco_segmentation\n        coco_polygon = [point for point in coco_polygon] if coco_polygon else coco_polygon\n        coco_segmentation.append(coco_polygon)\n    return coco_segmentation\n</code></pre> <code></code> <code>to_list()</code> \u00b6 <p>[     [(x1, y1), (x2, y2), (x3, y3), ...],     [(x1, y1), (x2, y2), (x3, y3), ...],     ... ]</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>def to_list(self):\n    \"\"\"\n    [\n        [(x1, y1), (x2, y2), (x3, y3), ...],\n        [(x1, y1), (x2, y2), (x3, y3), ...],\n        ...\n    ]\n    \"\"\"\n    list_of_list_of_points: list = []\n    for shapely_polygon in self.multipolygon.geoms:\n        # create list_of_points for selected shapely_polygon\n        if shapely_polygon.area != 0:\n            x_coords = shapely_polygon.exterior.coords.xy[0]\n            y_coords = shapely_polygon.exterior.coords.xy[1]\n            # fix coord by slice_bbox\n            if self.slice_bbox:\n                minx = self.slice_bbox[0]\n                miny = self.slice_bbox[1]\n                x_coords = [x_coord - minx for x_coord in x_coords]\n                y_coords = [y_coord - miny for y_coord in y_coords]\n            list_of_points = list(zip(x_coords, y_coords))\n        else:\n            list_of_points = []\n        # append list_of_points to list_of_list_of_points\n        list_of_list_of_points.append(list_of_points)\n    # return result\n    return list_of_list_of_points\n</code></pre> <code></code> <code>to_opencv_contours()</code> \u00b6 <p>[ [[[1, 1]], [[325, 125]], [[250, 200]], [[5, 200]]], [[[1, 1]], [[325, 125]], [[250, 200]], [[5, 200]]] ]</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>def to_opencv_contours(self):\n    \"\"\"[ [[[1, 1]], [[325, 125]], [[250, 200]], [[5, 200]]], [[[1, 1]], [[325, 125]], [[250, 200]], [[5, 200]]] ]\"\"\"\n    opencv_contours: list = []\n    for shapely_polygon in self.multipolygon.geoms:\n        # create opencv_contour for selected shapely_polygon\n        if shapely_polygon.area != 0:\n            x_coords = shapely_polygon.exterior.coords.xy[0]\n            y_coords = shapely_polygon.exterior.coords.xy[1]\n            # fix coord by slice_bbox\n            if self.slice_bbox:\n                minx = self.slice_bbox[0]\n                miny = self.slice_bbox[1]\n                x_coords = [x_coord - minx for x_coord in x_coords]\n                y_coords = [y_coord - miny for y_coord in y_coords]\n            opencv_contour = [[[int(x_coords[ind]), int(y_coords[ind])]] for ind in range(len(x_coords))]\n        else:\n            opencv_contour: list = []\n        # append opencv_contour to opencv_contours\n        opencv_contours.append(opencv_contour)\n    # return result\n    return opencv_contours\n</code></pre> <code></code> <code>to_voc_bbox()</code> \u00b6 <p>[xmin, ymin, xmax, ymax]</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>def to_voc_bbox(self):\n    \"\"\"[xmin, ymin, xmax, ymax]\"\"\"\n    return self.to_xyxy()\n</code></pre> <code></code> <code>to_xywh()</code> \u00b6 <p>[xmin, ymin, width, height]</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>def to_xywh(self):\n    \"\"\"[xmin, ymin, width, height]\"\"\"\n    if self.multipolygon.area != 0:\n        coco_bbox, _ = get_bbox_from_shapely(self.multipolygon)\n        # fix coord by slice box\n        if self.slice_bbox:\n            minx = self.slice_bbox[0]\n            miny = self.slice_bbox[1]\n            coco_bbox[0] = coco_bbox[0] - minx\n            coco_bbox[1] = coco_bbox[1] - miny\n    else:\n        coco_bbox: list = []\n    return coco_bbox\n</code></pre> <code></code> <code>to_xyxy()</code> \u00b6 <p>[xmin, ymin, xmax, ymax]</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>def to_xyxy(self):\n    \"\"\"[xmin, ymin, xmax, ymax]\"\"\"\n    if self.multipolygon.area != 0:\n        _, voc_bbox = get_bbox_from_shapely(self.multipolygon)\n        # fix coord by slice box\n        if self.slice_bbox:\n            minx = self.slice_bbox[0]\n            miny = self.slice_bbox[1]\n            voc_bbox[0] = voc_bbox[0] - minx\n            voc_bbox[2] = voc_bbox[2] - minx\n            voc_bbox[1] = voc_bbox[1] - miny\n            voc_bbox[3] = voc_bbox[3] - miny\n    else:\n        voc_bbox = []\n    return voc_bbox\n</code></pre> Functions\u00b6 <code></code> <code>get_bbox_from_shapely(shapely_object)</code> \u00b6 <p>Accepts shapely box/poly object and returns its bounding box in coco and voc formats.</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>def get_bbox_from_shapely(shapely_object):\n    \"\"\"Accepts shapely box/poly object and returns its bounding box in coco and voc formats.\"\"\"\n    minx, miny, maxx, maxy = shapely_object.bounds\n    width = maxx - minx\n    height = maxy - miny\n    coco_bbox = [minx, miny, width, height]\n    voc_bbox = [minx, miny, maxx, maxy]\n\n    return coco_bbox, voc_bbox\n</code></pre> <code></code> <code>get_shapely_box(x, y, width, height)</code> \u00b6 <p>Accepts coco style bbox coords and converts it to shapely box object.</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>def get_shapely_box(x: int, y: int, width: int, height: int) -&gt; Polygon:\n    \"\"\"Accepts coco style bbox coords and converts it to shapely box object.\"\"\"\n    minx = x\n    miny = y\n    maxx = x + width\n    maxy = y + height\n    shapely_box = box(minx, miny, maxx, maxy)\n\n    return shapely_box\n</code></pre> <code></code> <code>get_shapely_multipolygon(coco_segmentation)</code> \u00b6 <p>Accepts coco style polygon coords and converts it to valid shapely multipolygon object.</p> Source code in <code>sahi/utils/shapely.py</code> <pre><code>def get_shapely_multipolygon(coco_segmentation: list[list]) -&gt; MultiPolygon:\n    \"\"\"Accepts coco style polygon coords and converts it to valid shapely multipolygon object.\"\"\"\n\n    def filter_polygons(geometry):\n        \"\"\"Filters out and returns only Polygon or MultiPolygon components of a geometry.\n\n        If geometry is a Polygon, it converts it into a MultiPolygon. If it's a GeometryCollection, it filters to create\n        a MultiPolygon from any Polygons in the collection. Returns an empty MultiPolygon if no Polygon or MultiPolygon\n        components are found.\n\n        Args:\n            geometry: A shapely geometry object (Polygon, MultiPolygon, GeometryCollection, etc.)\n\n        Returns: MultiPolygon\n        \"\"\"\n        if isinstance(geometry, Polygon):\n            return MultiPolygon([geometry])\n        elif isinstance(geometry, MultiPolygon):\n            return geometry\n        elif isinstance(geometry, GeometryCollection):\n            polygons = [\n                geom.geoms if isinstance(geom, MultiPolygon) else geom\n                for geom in geometry.geoms\n                if isinstance(geom, (Polygon, MultiPolygon))\n            ]\n            return MultiPolygon(polygons) if polygons else MultiPolygon()\n        return MultiPolygon()\n\n    polygon_list = []\n    for coco_polygon in coco_segmentation:\n        point_list = list(zip(coco_polygon[0::2], coco_polygon[1::2]))\n        shapely_polygon = Polygon(point_list)\n        polygon_list.append(shapely_polygon)\n    shapely_multipolygon = MultiPolygon(polygon_list)\n\n    if not shapely_multipolygon.is_valid:\n        shapely_multipolygon = filter_polygons(make_valid(shapely_multipolygon))\n\n    return shapely_multipolygon\n</code></pre>"},{"location":"api/#sahi.utils.torch_utils","title":"<code>torch_utils</code>","text":"Functions\u00b6 <code>select_device(device=None)</code> \u00b6 <p>Selects torch device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> \u00b6 <code>str | None</code> <p>\"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.     When no device string is given, the order of preference     to try is: cuda:0 &gt; mps &gt; cpu</p> <code>None</code> <p>Returns:</p> Type Description <code>device</code> <p>torch.device</p> <p>Inspired by https://github.com/ultralytics/yolov5/blob/6371de8879e7ad7ec5283e8b95cc6dd85d6a5e72/utils/torch_utils.py#L107</p> Source code in <code>sahi/utils/torch_utils.py</code> <pre><code>def select_device(device: str | None = None) -&gt; torch.device:\n    \"\"\"Selects torch device.\n\n    Args:\n        device: \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n                When no device string is given, the order of preference\n                to try is: cuda:0 &gt; mps &gt; cpu\n\n    Returns:\n        torch.device\n\n    Inspired by https://github.com/ultralytics/yolov5/blob/6371de8879e7ad7ec5283e8b95cc6dd85d6a5e72/utils/torch_utils.py#L107\n    \"\"\"\n    import torch\n\n    if device == \"cuda\" or device is None:\n        device = \"cuda:0\"\n    device = str(device).strip().lower().replace(\"cuda:\", \"\").replace(\"none\", \"\")  # to string, 'cuda:0' to '0'\n    cpu = device == \"cpu\"\n    mps = device == \"mps\"  # Apple Metal Performance Shaders (MPS)\n    if cpu or mps:\n        environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # force torch.cuda.is_available() = False\n    elif device:  # non-cpu device requested\n        environ[\"CUDA_VISIBLE_DEVICES\"] = device  # set environment variable - must be before assert is_available()\n\n    cuda_id_pattern = r\"^(0|[1-9]\\d*)$\"\n    valid_cuda_id = bool(re.fullmatch(cuda_id_pattern, device))\n\n    if not cpu and not mps and torch.cuda.is_available() and valid_cuda_id:  # prefer GPU if available\n        arg = f\"cuda:{device}\" if device else \"cuda:0\"\n    elif mps and getattr(torch, \"has_mps\", False) and torch.backends.mps.is_available():  # prefer MPS if available\n        arg = \"mps\"\n    else:  # revert to CPU\n        arg = \"cpu\"\n\n    return torch.device(arg)\n</code></pre> <code></code> <code>to_float_tensor(img)</code> \u00b6 <p>Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W).</p> <p>Parameters:</p> Name Type Description Default <code>img</code> \u00b6 <code>ndarray | Image</code> <p>PIL.Image or numpy array</p> required <p>Returns:     torch.tensor</p> Source code in <code>sahi/utils/torch_utils.py</code> <pre><code>def to_float_tensor(img: np.ndarray | Image) -&gt; torch.Tensor:\n    \"\"\"Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C\n    x H x W).\n\n    Args:\n        img: PIL.Image or numpy array\n    Returns:\n        torch.tensor\n    \"\"\"\n    import torch\n\n    nparray: np.ndarray\n    if isinstance(img, np.ndarray):\n        nparray = img\n    else:\n        nparray = np.array(img)\n    nparray = nparray.transpose((2, 0, 1))\n    tensor = torch.from_numpy(np.array(nparray)).float()\n    if tensor.max() &gt; 1:\n        tensor /= 255\n    return tensor\n</code></pre>"},{"location":"auto_model/","title":"AutoModel","text":""},{"location":"auto_model/#sahi.auto_model","title":"<code>sahi.auto_model</code>","text":""},{"location":"auto_model/#sahi.auto_model-classes","title":"Classes","text":""},{"location":"auto_model/#sahi.auto_model.AutoDetectionModel","title":"<code>AutoDetectionModel</code>","text":"Source code in <code>sahi/auto_model.py</code> <pre><code>class AutoDetectionModel:\n    @staticmethod\n    def from_pretrained(\n        model_type: str,\n        model_path: str | None = None,\n        model: Any | None = None,\n        config_path: str | None = None,\n        device: str | None = None,\n        mask_threshold: float = 0.5,\n        confidence_threshold: float = 0.3,\n        category_mapping: dict | None = None,\n        category_remapping: dict | None = None,\n        load_at_init: bool = True,\n        image_size: int | None = None,\n        **kwargs,\n    ) -&gt; DetectionModel:\n        \"\"\"Loads a DetectionModel from given path.\n\n        Args:\n            model_type: str\n                Name of the detection framework (example: \"ultralytics\", \"huggingface\", \"torchvision\")\n            model_path: str\n                Path of the detection model (ex. 'model.pt')\n            model: Any\n                A pre-initialized model instance, if available\n            config_path: str\n                Path of the config file (ex. 'mmdet/configs/cascade_rcnn_r50_fpn_1x.py')\n            device: str\n                Device, \"cpu\" or \"cuda:0\"\n            mask_threshold: float\n                Value to threshold mask pixels, should be between 0 and 1\n            confidence_threshold: float\n                All predictions with score &lt; confidence_threshold will be discarded\n            category_mapping: dict: str to str\n                Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n            category_remapping: dict: str to int\n                Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n            load_at_init: bool\n                If True, automatically loads the model at initialization\n            image_size: int\n                Inference input size.\n\n        Returns:\n            Returns an instance of a DetectionModel\n\n        Raises:\n            ImportError: If given {model_type} framework is not installed\n        \"\"\"\n        if model_type in ULTRALYTICS_MODEL_NAMES:\n            model_type = \"ultralytics\"\n        model_class_name = MODEL_TYPE_TO_MODEL_CLASS_NAME[model_type]\n        DetectionModel = import_model_class(model_type, model_class_name)\n\n        return DetectionModel(\n            model_path=model_path,\n            model=model,\n            config_path=config_path,\n            device=device,\n            mask_threshold=mask_threshold,\n            confidence_threshold=confidence_threshold,\n            category_mapping=category_mapping,\n            category_remapping=category_remapping,\n            load_at_init=load_at_init,\n            image_size=image_size,\n            **kwargs,\n        )\n</code></pre>"},{"location":"auto_model/#sahi.auto_model.AutoDetectionModel-functions","title":"Functions","text":""},{"location":"auto_model/#sahi.auto_model.AutoDetectionModel.from_pretrained","title":"<code>from_pretrained(model_type, model_path=None, model=None, config_path=None, device=None, mask_threshold=0.5, confidence_threshold=0.3, category_mapping=None, category_remapping=None, load_at_init=True, image_size=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Loads a DetectionModel from given path.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> \u00b6 <code>str</code> <p>str Name of the detection framework (example: \"ultralytics\", \"huggingface\", \"torchvision\")</p> required <code>model_path</code> \u00b6 <code>str | None</code> <p>str Path of the detection model (ex. 'model.pt')</p> <code>None</code> <code>model</code> \u00b6 <code>Any | None</code> <p>Any A pre-initialized model instance, if available</p> <code>None</code> <code>config_path</code> \u00b6 <code>str | None</code> <p>str Path of the config file (ex. 'mmdet/configs/cascade_rcnn_r50_fpn_1x.py')</p> <code>None</code> <code>device</code> \u00b6 <code>str | None</code> <p>str Device, \"cpu\" or \"cuda:0\"</p> <code>None</code> <code>mask_threshold</code> \u00b6 <code>float</code> <p>float Value to threshold mask pixels, should be between 0 and 1</p> <code>0.5</code> <code>confidence_threshold</code> \u00b6 <code>float</code> <p>float All predictions with score &lt; confidence_threshold will be discarded</p> <code>0.3</code> <code>category_mapping</code> \u00b6 <code>dict | None</code> <p>dict: str to str Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}</p> <code>None</code> <code>category_remapping</code> \u00b6 <code>dict | None</code> <p>dict: str to int Remap category ids based on category names, after performing inference e.g. {\"car\": 3}</p> <code>None</code> <code>load_at_init</code> \u00b6 <code>bool</code> <p>bool If True, automatically loads the model at initialization</p> <code>True</code> <code>image_size</code> \u00b6 <code>int | None</code> <p>int Inference input size.</p> <code>None</code> <p>Returns:</p> Type Description <code>DetectionModel</code> <p>Returns an instance of a DetectionModel</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If given {model_type} framework is not installed</p> Source code in <code>sahi/auto_model.py</code> <pre><code>@staticmethod\ndef from_pretrained(\n    model_type: str,\n    model_path: str | None = None,\n    model: Any | None = None,\n    config_path: str | None = None,\n    device: str | None = None,\n    mask_threshold: float = 0.5,\n    confidence_threshold: float = 0.3,\n    category_mapping: dict | None = None,\n    category_remapping: dict | None = None,\n    load_at_init: bool = True,\n    image_size: int | None = None,\n    **kwargs,\n) -&gt; DetectionModel:\n    \"\"\"Loads a DetectionModel from given path.\n\n    Args:\n        model_type: str\n            Name of the detection framework (example: \"ultralytics\", \"huggingface\", \"torchvision\")\n        model_path: str\n            Path of the detection model (ex. 'model.pt')\n        model: Any\n            A pre-initialized model instance, if available\n        config_path: str\n            Path of the config file (ex. 'mmdet/configs/cascade_rcnn_r50_fpn_1x.py')\n        device: str\n            Device, \"cpu\" or \"cuda:0\"\n        mask_threshold: float\n            Value to threshold mask pixels, should be between 0 and 1\n        confidence_threshold: float\n            All predictions with score &lt; confidence_threshold will be discarded\n        category_mapping: dict: str to str\n            Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n        category_remapping: dict: str to int\n            Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n        load_at_init: bool\n            If True, automatically loads the model at initialization\n        image_size: int\n            Inference input size.\n\n    Returns:\n        Returns an instance of a DetectionModel\n\n    Raises:\n        ImportError: If given {model_type} framework is not installed\n    \"\"\"\n    if model_type in ULTRALYTICS_MODEL_NAMES:\n        model_type = \"ultralytics\"\n    model_class_name = MODEL_TYPE_TO_MODEL_CLASS_NAME[model_type]\n    DetectionModel = import_model_class(model_type, model_class_name)\n\n    return DetectionModel(\n        model_path=model_path,\n        model=model,\n        config_path=config_path,\n        device=device,\n        mask_threshold=mask_threshold,\n        confidence_threshold=confidence_threshold,\n        category_mapping=category_mapping,\n        category_remapping=category_remapping,\n        load_at_init=load_at_init,\n        image_size=image_size,\n        **kwargs,\n    )\n</code></pre>"},{"location":"auto_model/#sahi.auto_model-functions","title":"Functions","text":""},{"location":"changelog/","title":"\ud83d\udcdd CHANGELOG","text":""},{"location":"changelog/#sahi-v01131-release-notes","title":"\ud83d\ude80 SAHI v0.11.31 Release Notes","text":"<p>We're excited to announce SAHI v0.11.31 with important bug fixes and improvements!</p>"},{"location":"changelog/#whats-changed","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>Make Category immutable and add tests by @gboeer in https://github.com/obss/sahi/pull/1206</li> <li>Update docstring for greedy_nmm by @kikefdezl in https://github.com/obss/sahi/pull/1205</li> <li>update version by @fcakyon in https://github.com/obss/sahi/pull/1208</li> </ul>"},{"location":"changelog/#new-contributors","title":"\ud83d\ude4c New Contributors","text":"<ul> <li>@kikefdezl made their first contribution in https://github.com/obss/sahi/pull/1205</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.30...0.11.31</p>"},{"location":"changelog/#sahi-v01130-release-notes","title":"\ud83d\ude80 SAHI v0.11.30 Release Notes","text":"<p>We're excited to announce SAHI v0.11.30 with improved performance tracking, enhanced testing   infrastructure, and better developer experience!</p>"},{"location":"changelog/#milestones","title":"\ud83d\udcc8 Milestones","text":"<ul> <li>Academic papers citing SAHI reached 400! (#1168)</li> </ul>"},{"location":"changelog/#key-updates","title":"\ud83d\ude80 Key Updates","text":""},{"location":"changelog/#performance-monitoring","title":"\u26a1\ufe0f Performance &amp; Monitoring","text":"<ul> <li>Fixed postprocess duration tracking in <code>get_sliced_prediction</code> - now properly separates slice,   prediction, and postprocess timings for accurate performance monitoring   (#1201) - Thanks @Toprak2!</li> </ul>"},{"location":"changelog/#framework-updates","title":"\ud83e\udde9 Framework Updates","text":"<ul> <li>Refactored Ultralytics support with ONNX model support and better compatibility   (#1184)</li> <li>Updated TorchVision support to latest API (#1182)</li> <li>Improved Detectron2 support with better config handling to prevent KeyError issues   (#1116) - Thanks @Arnesh1411!</li> <li>Added Roboflow framework support for RF-DETR models from the Roboflow Universe   (#1161) - Thanks @nok!</li> <li>Removed deepsparse integration as the framework is no longer maintained   (#1164)</li> </ul>"},{"location":"changelog/#testing-infrastructure","title":"\ud83e\uddea Testing Infrastructure","text":"<ul> <li>Migrated test suite to pytest (#1187)</li> <li>Tests now run faster with better parallel execution</li> <li>Extended Python version coverage (3.8, 3.9, 3.10, 3.11, 3.12)</li> <li>Updated to more recent PyTorch versions for better compatibility testing</li> <li>Improved test organization and maintainability</li> <li>Refactored MMDetection tests for better reliability (#1185)</li> </ul>"},{"location":"changelog/#developer-experience","title":"\ud83d\udcbb Developer Experience","text":"<ul> <li>Added Context7 MCP integration for AI-assisted development   (#1198)</li> <li>SAHI's documentation is now indexed in Context7 MCP</li> <li>Provides AI coding assistants with up-to-date, version-specific code examples</li> <li>Includes llms.txt file for AI-readable documentation</li> <li>Check out the Context7 MCP installation   guide to integrate SAHI docs with your AI   workflow</li> </ul>"},{"location":"changelog/#improvements","title":"\ud83d\udee0\ufe0f Improvements","text":""},{"location":"changelog/#code-quality-safety","title":"\ud83e\uddf9 Code Quality &amp; Safety","text":"<ul> <li>Immutable bounding boxes for thread-safe operations (#1194,    #1191) - Thanks @gboeer!</li> <li>Enhanced type hints and docstrings throughout the codebase   (#1195) - Thanks @gboeer!</li> <li>Overloaded operators for prediction scores enabling intuitive score comparisons   (#1190) - Thanks @gboeer!</li> <li>PyTorch is now a soft dependency improving flexibility   (#1162) - Thanks @ducviet00!</li> </ul>"},{"location":"changelog/#infrastructure-stability","title":"\ud83c\udfd7\ufe0f Infrastructure &amp; Stability","text":"<ul> <li>Improved dependency management and documentation (#1183)</li> <li>Enhanced pyproject.toml configuration for better package management   (#1181)</li> <li>Optimized CI/CD workflows for MMDetection tests (#1186)</li> </ul>"},{"location":"changelog/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fixed CUDA device selection to support devices other than cuda:0   (#1158) - Thanks @0xf21!</li> <li>Corrected parameter naming from 'confidence' to 'threshold' for consistency   (#1180) - Thanks @nok!</li> <li>Fixed regex string formatting in device selection function   (#1165)</li> <li>Resolved torch import errors when PyTorch is not installed   (#1172) - Thanks @ducviet00!</li> <li>Fixed model instantiation issues with <code>AutoDetectionModel.from_pretrained</code>   (#1158)</li> </ul>"},{"location":"changelog/#dependencies","title":"\ud83d\udce6 Dependencies","text":"<ul> <li>Updated OpenCV packages from 4.10.0.84 to 4.11.0.86 (#1171) -   Thanks @ducviet00-h2!</li> <li>Removed unmaintained matplotlib-stubs dependency (#1169)</li> <li>Cleaned up unused configuration files (#1199)</li> </ul>"},{"location":"changelog/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Added context7.json for better AI tool integration (#1200)</li> <li>Updated README with new contributors (#1175,   #1179)</li> <li>Added Roboflow+SAHI Colab tutorial link (#1177)</li> </ul>"},{"location":"changelog/#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<p>Special thanks to all contributors who made this release possible: @nok, @gboeer, @Toprak2, @Arnesh1411,   @0xf21, @ducviet00, @ducviet00-h2, @p-constant, and @fcakyon!</p> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.24...0.11.30</p>"},{"location":"changelog/#sahi-v01129-release-notes","title":"\ud83d\ude80 SAHI v0.11.29 Release Notes","text":""},{"location":"changelog/#whats-changed_1","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>Make bounding box immutable by @gboeer in https://github.com/obss/sahi/pull/1194</li> <li>Improve type hints and docstrings by @gboeer in https://github.com/obss/sahi/pull/1195</li> <li>update version by @fcakyon in https://github.com/obss/sahi/pull/1196</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.28...0.11.29</p>"},{"location":"changelog/#sahi-v01128-release-notes","title":"\ud83d\ude80 SAHI v0.11.28 Release Notes","text":""},{"location":"changelog/#whats-changed_2","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>Add overloaded operators for prediction score by @gboeer in https://github.com/obss/sahi/pull/1190</li> <li>Improve detectron2 support by @Arnesh1411 in https://github.com/obss/sahi/pull/1116</li> <li>Use immutable arguments for bounding boxes by @gboeer in https://github.com/obss/sahi/pull/1191</li> <li>update version by @fcakyon in https://github.com/obss/sahi/pull/1192</li> </ul>"},{"location":"changelog/#new-contributors_1","title":"\ud83d\ude4c New Contributors","text":"<ul> <li>@Arnesh1411 made their first contribution in https://github.com/obss/sahi/pull/1116</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.27...0.11.28</p>"},{"location":"changelog/#sahi-v01127-release-notes","title":"\ud83d\ude80 SAHI v0.11.27 Release Notes","text":""},{"location":"changelog/#whats-changed_3","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>fix: Update inference method to use 'threshold' instead of 'confidence' by @nok in https://github.com/obss/sahi/pull/1180</li> <li>Update README.md by @nok in https://github.com/obss/sahi/pull/1179</li> <li>improve pyproject.toml by @fcakyon in https://github.com/obss/sahi/pull/1181</li> <li>Refactor dependency management and some docs by @fcakyon in https://github.com/obss/sahi/pull/1183</li> <li>update: refactor ultralytics support by @fcakyon in https://github.com/obss/sahi/pull/1184</li> <li>Refactor mmdet tests by @fcakyon in https://github.com/obss/sahi/pull/1185</li> <li>update torchvision support to latest api by @fcakyon in https://github.com/obss/sahi/pull/1182</li> <li>optimize mmdet workflow trigger condition by @fcakyon in https://github.com/obss/sahi/pull/1186</li> <li>Migrate tests to pytest by @fcakyon in https://github.com/obss/sahi/pull/1187</li> <li>update version by @fcakyon in https://github.com/obss/sahi/pull/1188</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.26...0.11.27</p>"},{"location":"changelog/#sahi-v01126-release-notes","title":"\ud83d\ude80 SAHI v0.11.26 Release Notes","text":""},{"location":"changelog/#whats-changed_4","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>Bump opencv packages from <code>4.10.0.84</code> to <code>4.11.0.86</code> by @ducviet00-h2 in https://github.com/obss/sahi/pull/1171</li> <li>Add new framework Roboflow (RFDETR models) by @nok in https://github.com/obss/sahi/pull/1161</li> <li>add new contributors to readme by @fcakyon in https://github.com/obss/sahi/pull/1175</li> <li>add roboflow+sahi colab url to readme by @fcakyon in https://github.com/obss/sahi/pull/1177</li> <li>update version by @fcakyon in https://github.com/obss/sahi/pull/1176</li> </ul>"},{"location":"changelog/#new-contributors_2","title":"\ud83d\ude4c New Contributors","text":"<ul> <li>@ducviet00-h2 made their first contribution in https://github.com/obss/sahi/pull/1171</li> <li>@nok made their first contribution in https://github.com/obss/sahi/pull/1161</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.25...0.11.26</p>"},{"location":"changelog/#sahi-v01125-release-notes","title":"\ud83d\ude80 SAHI v0.11.25 Release Notes","text":""},{"location":"changelog/#whats-changed_5","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>update sahi citation in readme by @fcakyon in https://github.com/obss/sahi/pull/1168</li> <li>remove matplotlib-stubs as its not maintained by @fcakyon in https://github.com/obss/sahi/pull/1169</li> <li>Fix torch import errors by @ducviet00 in https://github.com/obss/sahi/pull/1172</li> <li>update version by @fcakyon in https://github.com/obss/sahi/pull/1173</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.24...0.11.25</p>"},{"location":"changelog/#sahi-v01124-release-notes","title":"\ud83d\ude80 SAHI v0.11.24 Release Notes","text":""},{"location":"changelog/#whats-changed_6","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>Fix typo and scripts URL by @gboeer in https://github.com/obss/sahi/pull/1155</li> <li>fix ci workflow bug by @Dronakurl in https://github.com/obss/sahi/pull/1156</li> <li>[DOC] Fix typos by @gboeer in https://github.com/obss/sahi/pull/1157</li> <li>Remove deepsparse integration by @fcakyon in https://github.com/obss/sahi/pull/1164</li> <li>Fix: Make pytorch is not a hard dependency by @ducviet00 in https://github.com/obss/sahi/pull/1162</li> <li>fix: specify a device other than cuda:0 by @0xf21 in https://github.com/obss/sahi/pull/1158</li> <li>fix: correct regex string formatting in select_device function by @fcakyon in https://github.com/obss/sahi/pull/1165</li> <li>add TensorrtExecutionProvider to yolov8onnx by @p-constant in https://github.com/obss/sahi/pull/1091</li> <li>update version by @fcakyon in https://github.com/obss/sahi/pull/1166</li> </ul>"},{"location":"changelog/#new-contributors_3","title":"\ud83d\ude4c New Contributors","text":"<ul> <li>@gboeer made their first contribution in https://github.com/obss/sahi/pull/1155</li> <li>@ducviet00 made their first contribution in https://github.com/obss/sahi/pull/1162</li> <li>@0xf21 made their first contribution in https://github.com/obss/sahi/pull/1158</li> <li>@p-constant made their first contribution in https://github.com/obss/sahi/pull/1091</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.23...0.11.24</p>"},{"location":"changelog/#sahi-v01123-release-notes","title":"\ud83d\ude80 SAHI v0.11.23 Release Notes","text":""},{"location":"changelog/#whats-changed_7","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>fix(CI): numpy dependency fixes #1119 by @Dronakurl in https://github.com/obss/sahi/pull/1144</li> <li>Fix: Predict cannot find TIF files in source directory by @dibunker in https://github.com/obss/sahi/pull/1142</li> <li>Fixed typos in demo Notebooks by @picjul in https://github.com/obss/sahi/pull/1150</li> <li>fix: Fix Polygon Repair and Empty Polygon Issues, see #1118 by @mario-dg in https://github.com/obss/sahi/pull/1138</li> <li>improve package ci logging by @fcakyon in https://github.com/obss/sahi/pull/1151</li> </ul>"},{"location":"changelog/#new-contributors_4","title":"\ud83d\ude4c New Contributors","text":"<ul> <li>@dibunker made their first contribution in https://github.com/obss/sahi/pull/1142</li> <li>@picjul made their first contribution in https://github.com/obss/sahi/pull/1150</li> <li>@mario-dg made their first contribution in https://github.com/obss/sahi/pull/1138</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.22...0.11.23</p>"},{"location":"changelog/#sahi-v01122-release-notes","title":"\ud83d\ude80 SAHI v0.11.22 Release Notes","text":""},{"location":"changelog/#whats-changed_8","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>Improve support for latest mmdet (v3.3.0) by @fcakyon in https://github.com/obss/sahi/pull/1129</li> <li>Improve support for latest yolov5-pip and ultralytics versions by @fcakyon in https://github.com/obss/sahi/pull/1130</li> <li>support latest huggingface/transformers models by @fcakyon in https://github.com/obss/sahi/pull/1131</li> <li>refctor coco to yolo conversion, update docs by @fcakyon in https://github.com/obss/sahi/pull/1132</li> <li>bump version by @fcakyon in https://github.com/obss/sahi/pull/1134</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.21...0.11.22</p>"},{"location":"changelog/#core-documentation-files","title":"\ud83d\udcda Core Documentation Files","text":""},{"location":"changelog/#prediction-utilities","title":"\ud83d\udce6 Prediction Utilities","text":"<ul> <li>Detailed guide for performing object detection inference</li> <li>Standard and sliced inference examples</li> <li>Batch prediction usage</li> <li>Class exclusion during inference</li> <li>Visualization parameters and export formats</li> <li>Interactive examples with various model integrations (YOLOv8, MMDetection, etc.)</li> </ul>"},{"location":"changelog/#slicing-utilities","title":"\u2702\ufe0f Slicing Utilities","text":"<ul> <li>Guide for slicing large images and datasets</li> <li>Image slicing examples</li> <li>COCO dataset slicing examples</li> <li>Interactive demo notebook reference</li> </ul>"},{"location":"changelog/#coco-utilities","title":"\ud83d\udc12 COCO Utilities","text":"<ul> <li>Comprehensive guide for working with COCO format datasets</li> <li>Dataset creation and manipulation</li> <li>Slicing COCO datasets</li> <li>Dataset splitting (train/val)</li> <li>Category filtering and updates</li> <li>Area-based filtering</li> <li>Dataset merging</li> <li>Format conversion (COCO \u2194 YOLO)</li> <li>Dataset sampling utilities</li> <li>Statistics calculation</li> <li>Result validation</li> </ul>"},{"location":"changelog/#cli-commands","title":"\ud83d\udcbb CLI Commands","text":"<ul> <li>Complete reference for SAHI command-line interface</li> <li>Prediction commands</li> <li>FiftyOne integration</li> <li>COCO dataset operations</li> <li>Environment information</li> <li>Version checking</li> <li>Custom script usage</li> </ul>"},{"location":"changelog/#fiftyone-integration","title":"\ud83d\udc41\ufe0f FiftyOne Integration","text":"<ul> <li>Guide for visualizing and analyzing predictions with FiftyOne</li> <li>Dataset visualization</li> <li>Result exploration</li> <li>Interactive analysis</li> </ul>"},{"location":"changelog/#interactive-examples","title":"\ud83d\udcd3 Interactive Examples","text":"<p>All documentation files are complemented by interactive Jupyter notebooks in the demo directory: * <code>slicing.ipynb</code> - Slicing operations demonstration * <code>inference_for_ultralytics.ipynb</code> - YOLOv8/YOLO11/YOLO12 integration * <code>inference_for_yolov5.ipynb</code> - YOLOv5 integration * <code>inference_for_mmdetection.ipynb</code> - MMDetection integration * <code>inference_for_huggingface.ipynb</code> - HuggingFace models integration * <code>inference_for_torchvision.ipynb</code> - TorchVision models integration * <code>inference_for_rtdetr.ipynb</code> - RT-DETR integration * <code>inference_for_sparse_yolov5.ipynb</code> - DeepSparse optimized inference</p>"},{"location":"changelog/#getting-started","title":"\ud83d\udea6 Getting Started","text":"<p>If you're new to SAHI:</p> <ol> <li>Start with the prediction utilities to understand basic inference</li> <li>Explore the slicing utilities to learn about processing large images</li> <li>Check out the CLI commands for command-line usage</li> <li>Dive into COCO utilities for dataset operations</li> <li>Try the interactive notebooks in the demo directory for hands-on experience</li> </ol>"},{"location":"changelog/#sahi-v01121-release-notes","title":"\ud83d\ude80 SAHI v0.11.21 Release Notes","text":""},{"location":"changelog/#whats-changed_9","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>Exclude classes from inference using pretrained or custom models by @gguzzy in https://github.com/obss/sahi/pull/1104</li> <li>pyproject.toml, pre-commit, ruff, uv and typing issues, fixes #1119 by @Dronakurl in https://github.com/obss/sahi/pull/1120</li> <li>add class exclusion example into predict docs by @gguzzy in https://github.com/obss/sahi/pull/1125</li> <li>Add OBB demo by @fcakyon in https://github.com/obss/sahi/pull/1126</li> <li>fix a type hint typo in predict func by @fcakyon in https://github.com/obss/sahi/pull/1111</li> <li>Remove numpy&lt;2 upper pin by @weiji14 in https://github.com/obss/sahi/pull/1112</li> <li>fix ci badge on readme by @fcakyon in https://github.com/obss/sahi/pull/1124</li> <li>fix version in pyproject.toml by @fcakyon in https://github.com/obss/sahi/pull/1127</li> </ul>"},{"location":"changelog/#new-contributors_5","title":"\ud83d\ude4c New Contributors","text":"<ul> <li>@Dronakurl made their first contribution in https://github.com/obss/sahi/pull/1120</li> <li>@gguzzy made their first contribution in https://github.com/obss/sahi/pull/1104</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.20...0.11.21</p>"},{"location":"changelog/#sahi-v01120-release-notes","title":"\ud83d\ude80 SAHI v0.11.20 Release Notes","text":""},{"location":"changelog/#whats-changed_10","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>add yolo11 and ultralytics obb task support by @fcakyon in https://github.com/obss/sahi/pull/1109</li> <li>support latest opencv version by @fcakyon in https://github.com/obss/sahi/pull/1106</li> <li>simplify yolo detection model code by @fcakyon in https://github.com/obss/sahi/pull/1107</li> <li>Pin shapely&gt;2.0.0 by @weiji14 in https://github.com/obss/sahi/pull/1101</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.19...0.11.20</p>"},{"location":"changelog/#sahi-v01119-release-notes","title":"\ud83d\ude80 SAHI v0.11.19 Release Notes","text":""},{"location":"changelog/#whats-changed_11","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>fix ci actions by @fcakyon in https://github.com/obss/sahi/pull/1073</li> <li>Update has_mask method for mmdet models (handle an edge case) by @ccomkhj in https://github.com/obss/sahi/pull/1066</li> <li>Another self-intersection corner case handling by @sergiev in https://github.com/obss/sahi/pull/982</li> <li>Update README.md by @fcakyon in https://github.com/obss/sahi/pull/1077</li> <li>drop non-working yolonas support by @fcakyon in https://github.com/obss/sahi/pull/1097</li> <li>drop yolonas support part2 by @fcakyon in https://github.com/obss/sahi/pull/1098</li> <li>Update has_mask method for mmdet models (handle ConcatDataset) by @ccomkhj in https://github.com/obss/sahi/pull/1092</li> </ul>"},{"location":"changelog/#new-contributors_6","title":"\ud83d\ude4c New Contributors","text":"<ul> <li>@ccomkhj made their first contribution in https://github.com/obss/sahi/pull/1066</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.18...0.11.19</p>"},{"location":"changelog/#sahi-v01118-release-notes","title":"\ud83d\ude80 SAHI v0.11.18 Release Notes","text":""},{"location":"changelog/#whats-changed_12","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>add yolov8 mask support, improve mask processing speed by 4-5x by @mayrajeo in https://github.com/obss/sahi/pull/1039</li> <li>fix has_mask method for mmdet models by @Alias-z in https://github.com/obss/sahi/pull/1054</li> <li>Fix <code>TypeError: 'GeometryCollection' object is not subscriptable</code> when slicing COCO by @Alias-z in https://github.com/obss/sahi/pull/1047</li> <li>support opencv-python version 4.9 by @iokarkan in https://github.com/obss/sahi/pull/1041</li> <li>add upperlimit to numpy dep by @fcakyon in https://github.com/obss/sahi/pull/1057</li> <li>add more unit tests by @MMerling in https://github.com/obss/sahi/pull/1048</li> <li>upgrade ci actions by @fcakyon in https://github.com/obss/sahi/pull/1049</li> </ul>"},{"location":"changelog/#new-contributors_7","title":"\ud83d\ude4c New Contributors","text":"<ul> <li>@iokarkan made their first contribution in https://github.com/obss/sahi/pull/1041</li> <li>@MMerling made their first contribution in https://github.com/obss/sahi/pull/1048</li> <li>@Alias-z made their first contribution in https://github.com/obss/sahi/pull/1047</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.16...0.11.18</p>"},{"location":"changelog/#sahi-v01116-release-notes","title":"\ud83d\ude80 SAHI v0.11.16 Release Notes","text":""},{"location":"changelog/#sahi-v01115-release-notes","title":"\ud83d\ude80 SAHI v0.11.15 Release Notes","text":""},{"location":"changelog/#sahi-v01114-release-notes","title":"\ud83d\ude80 SAHI v0.11.14 Release Notes","text":""},{"location":"changelog/#whats-changed_13","title":"\ud83c\udd95 What's Changed","text":"<ul> <li>support Deci-AI YOLO-NAS models by @ssahinnkadir in https://github.com/obss/sahi/pull/874</li> <li>Significant speed improvement for Detectron2 models by @MyosQ in https://github.com/obss/sahi/pull/865</li> <li>support ultralytics&gt;=8.0.99 by @eVen-gits in https://github.com/obss/sahi/pull/873</li> <li>Documentation typo, and missing value by @Hamzalopode in https://github.com/obss/sahi/pull/859</li> <li>update version by @fcakyon in https://github.com/obss/sahi/pull/876</li> <li>update black version by @fcakyon in https://github.com/obss/sahi/pull/877</li> </ul>"},{"location":"changelog/#new-contributors_8","title":"\ud83d\ude4c New Contributors","text":"<ul> <li>@Hamzalopode made their first contribution in https://github.com/obss/sahi/pull/859</li> <li>@eVen-gits made their first contribution in https://github.com/obss/sahi/pull/873</li> <li>@MyosQ made their first contribution in https://github.com/obss/sahi/pull/865</li> </ul> <p>Full Changelog: https://github.com/obss/sahi/compare/0.11.13...0.11.14</p>"},{"location":"changelog/#sahi-v01113-release-notes","title":"\ud83d\ude80 SAHI v0.11.13 Release Notes","text":""},{"location":"changelog/#sahi-v01112-release-notes","title":"\ud83d\ude80 SAHI v0.11.12 Release Notes","text":""},{"location":"changelog/#sahi-v01111-release-notes","title":"\ud83d\ude80 SAHI v0.11.11 Release Notes","text":""},{"location":"changelog/#sahi-v01110-release-notes","title":"\ud83d\ude80 SAHI v0.11.10 Release Notes","text":""},{"location":"changelog/#sahi-v0119-release-notes","title":"\ud83d\ude80 SAHI v0.11.9 Release Notes","text":""},{"location":"changelog/#sahi-v0118-release-notes","title":"\ud83d\ude80 SAHI v0.11.8 Release Notes","text":""},{"location":"changelog/#sahi-v0117-release-notes","title":"\ud83d\ude80 SAHI v0.11.7 Release Notes","text":""},{"location":"changelog/#sahi-v0116-release-notes","title":"\ud83d\ude80 SAHI v0.11.6 Release Notes","text":""},{"location":"changelog/#sahi-v0115-release-notes","title":"\ud83d\ude80 SAHI v0.11.5 Release Notes","text":""},{"location":"changelog/#sahi-v0114-release-notes","title":"\ud83d\ude80 SAHI v0.11.4 Release Notes","text":""},{"location":"changelog/#sahi-v0113-release-notes","title":"\ud83d\ude80 SAHI v0.11.3 Release Notes","text":""},{"location":"changelog/#sahi-v0112-release-notes","title":"\ud83d\ude80 SAHI v0.11.2 Release Notes","text":""},{"location":"changelog/#sahi-v0111-release-notes","title":"\ud83d\ude80 SAHI v0.11.1 Release Notes","text":""},{"location":"cli/","title":"CLI Commands","text":""},{"location":"cli/#predict-command-usage","title":"<code>predict</code> command usage","text":"<pre><code>sahi predict --source image/file/or/folder --model_path path/to/model --model_config_path path/to/config\n</code></pre> <p>will perform sliced inference on default parameters and export the prediction visuals to runs/predict/exp folder.</p> <ul> <li>It also supports video input:</li> </ul> <pre><code>sahi predict --model_path yolo11s.pt --model_type ultralytics --source video.mp4\n</code></pre> <p>You can also view video render during video inference with <code>--view_video</code>:</p> <pre><code>sahi predict --model_path yolo11s.pt --model_type ultralytics --source video.mp4 --view_video\n</code></pre> <ul> <li>To <code>forward 100 frames</code>, on opened window press key <code>D</code></li> <li>To <code>revert 100 frames</code>, on opened window press key <code>A</code></li> <li>To <code>forward 20 frames</code>, on opened window press key <code>G</code></li> <li>To <code>revert 20 frames</code>, on opened window press key <code>F</code></li> <li>To <code>exit</code>, on opened window press key <code>Esc</code></li> </ul> <p>Note: If <code>--view_video</code> is slow, you can add <code>--frame_skip_interval=20</code> argument to skip interval of 20 frames each time.</p> <p>You can specify additional sliced prediction parameters as:</p> <pre><code>sahi predict --slice_width 512 --slice_height 512 --overlap_height_ratio 0.1 --overlap_width_ratio 0.1 --model_confidence_threshold 0.25 --source image/file/or/folder --model_path path/to/model --model_config_path path/to/config\n</code></pre> <ul> <li> <p>Specify detection framework as <code>--model_type mmdet</code> for MMDetection or <code>--model_type ultralytics</code> for Ultralytics, to match with your model weight file</p> </li> <li> <p>Specify postprocess type as <code>--postprocess_type GREEDYNMM</code> or <code>--postprocess_type NMS</code> to be applied over sliced predictions</p> </li> <li> <p>Specify postprocess match metric as <code>--postprocess_match_metric IOS</code> for intersection over smaller area or <code>--postprocess_match_metric IOU</code> for intersection over union</p> </li> <li> <p>Specify postprocess match threshold as <code>--postprocess_match_threshold 0.5</code></p> </li> <li> <p>Add <code>--postprocess_class_agnostic</code> argument to ignore category ids of the predictions during postprocess (merging/nms)</p> </li> <li> <p>If you want to export prediction pickles and cropped predictions add <code>--export_pickle</code> and <code>--export_crop</code> arguments. If you want to change crop extension type, set it as <code>--visual_export_format JPG</code>.</p> </li> <li> <p>If you don't want to export prediction visuals, add <code>--novisual</code> argument.</p> </li> <li> <p>By default, scripts apply both standard and sliced prediction (multi-stage inference). If you don't want to perform sliced prediction add <code>--no_sliced_prediction</code> argument. If you don't want to perform standard prediction add <code>--no_standard_prediction</code> argument.</p> </li> <li> <p>If you want to perform prediction using a COCO annotation file, provide COCO json path as <code>--dataset_json_path dataset.json</code> and coco image folder as <code>--source path/to/coco/image/folder</code>, predictions will be exported as a coco json file to runs/predict/exp/results.json. Then you can use coco_evaluation command to calculate COCO evaluation results or coco_error_analysis command to calculate detailed COCO error plots.</p> </li> <li> <p>Progress reporting: If you want a terminal progress bar while sliced inference runs, use the <code>--progress_bar</code> flag. This enables a tqdm progress bar that shows how many slice groups have been processed. Example:</p> </li> </ul> <pre><code>sahi predict --model_path path/to/model --source images/ --slice_width 512 --slice_height 512 --progress_bar\n</code></pre> <p>Note: The <code>--progress_bar</code> flag controls only the CLI visual progress (tqdm). The <code>progress_callback</code> parameter available in the Python API (<code>get_sliced_prediction</code>) is a programmatic hook (callable) and is not exposed as a CLI option.</p>"},{"location":"cli/#predict-fiftyone-command-usage","title":"<code>predict-fiftyone</code> command usage","text":"<pre><code>sahi predict-fiftyone --image_dir image/file/or/folder --dataset_json_path dataset.json --model_path path/to/model --model_config_path path/to/config\n</code></pre> <p>will perform sliced inference on default parameters and show the inference result on FiftyOne App.</p> <p>You can specify additional all extra parameters of the sahi predict command.</p>"},{"location":"cli/#coco-fiftyone-command-usage","title":"<code>coco fiftyone</code> command usage","text":"<p>You need to convert your predictions into COCO result json, sahi predict command can be used to create that.</p> <pre><code>sahi coco fiftyone --image_dir dir/to/images --dataset_json_path dataset.json cocoresult1.json cocoresult2.json\n</code></pre> <p>will open a FiftyOne app that visualizes the given dataset and 2 detection results.</p> <p>Specify IOU threshold for FP/TP by <code>--iou_threshold 0.5</code> argument</p>"},{"location":"cli/#coco-slice-command-usage","title":"<code>coco slice</code> command usage","text":"<pre><code>sahi coco slice --image_dir dir/to/images --dataset_json_path dataset.json\n</code></pre> <p>will slice the given images and COCO formatted annotations and export them to given output folder directory.</p> <p>Specify slice height/width size as <code>--slice_size 512</code>.</p> <p>Specify slice overlap ratio for height/width size as <code>--overlap_ratio 0.2</code>.</p> <p>If you want to ignore images with annotations set it add <code>--ignore_negative_samples</code> argument.</p>"},{"location":"cli/#coco-yolo-command-usage","title":"<code>coco yolo</code> command usage","text":"<p>(In Windows be sure to open anaconda cmd prompt/windows cmd <code>as admin</code> to be able to create symlinks properly.)</p> <pre><code>sahi coco yolo --image_dir dir/to/images --dataset_json_path dataset.json  --train_split 0.9\n</code></pre> <p>will convert given coco dataset to yolo format and export to runs/coco2yolo/exp folder.</p>"},{"location":"cli/#coco-evaluate-command-usage","title":"<code>coco evaluate</code> command usage","text":"<p>You need to convert your predictions into COCO result json, sahi predict command can be used to create that.</p> <pre><code>sahi coco evaluate --dataset_json_path dataset.json --result_json_path result.json\n</code></pre> <p>will calculate coco evaluation and export them to given output folder directory.</p> <p>If you want to specify mAP metric type, set it as <code>--type bbox</code> or <code>--type mask</code>.</p> <p>If you want to also calculate classwise scores add <code>--classwise</code> argument.</p> <p>If you want to specify max detections, set it as <code>--proposal_nums \"[10 100 500]\"</code>.</p> <p>If you want to specify a specific IOU threshold, set it as <code>--iou_thrs 0.5</code>. Default includes <code>0.50:0.95</code> and <code>0.5</code> scores.</p> <p>If you want to specify an export directory, set it as <code>--out_dir output/folder/directory</code>.</p>"},{"location":"cli/#coco-analyse-command-usage","title":"<code>coco analyse</code> command usage","text":"<p>You need to convert your predictions into COCO result json, sahi predict command can be used to create that.</p> <pre><code>sahi coco analyse --dataset_json_path dataset.json --result_json_path result.json --out_dir output/directory\n</code></pre> <p>will calculate coco error plots and export them to given output folder directory.</p> <p>If you want to specify mAP result type, set it as <code>--type bbox</code> or <code>--type segm</code>.</p> <p>If you want to export extra mAP bar plots and annotation area stats add <code>--extraplots</code> argument.</p> <p>If you want to specify area regions, set it as <code>--areas \"[1024 9216 10000000000]\"</code>.</p>"},{"location":"cli/#env-command-usage","title":"<code>env</code> command usage","text":"<p>Print related package versions in the current env as:</p> <pre><code>sahi env\n06/19/2022 21:24:52 - INFO - sahi.utils.import_utils -   torch version 2.1.2 is available.\n06/19/2022 21:24:52 - INFO - sahi.utils.import_utils -   torchvision version 0.16.2 is available.\n06/19/2022 21:24:52 - INFO - sahi.utils.import_utils -   ultralytics version 8.3.86 is available.\n06/19/2022 21:24:52 - INFO - sahi.utils.import_utils -   transformers version 4.49.0 is available.\n06/19/2022 21:24:52 - INFO - sahi.utils.import_utils -   timm version 0.9.1 is available.\n06/19/2022 21:24:52 - INFO - sahi.utils.import_utils -   fiftyone version 0.14.2 is available.\n</code></pre>"},{"location":"cli/#version-command-usage","title":"<code>version</code> command usage","text":"<p>Print your SAHI version as:</p> <pre><code>sahi version\n0.11.22\n</code></pre>"},{"location":"cli/#custom-scripts","title":"Custom scripts","text":"<p>All scripts can be downloaded from scripts directory and modified by your needs. After installing <code>sahi</code> by pip, all scripts can be called from any directory as:</p> <pre><code>python script_name.py\n</code></pre>"},{"location":"cli/#additional-resources","title":"Additional Resources","text":"<p>Looking to dive deeper? Here are some helpful resources:</p> <ul> <li>For a detailed walkthrough of prediction parameters and visualization, check out our prediction utilities documentation</li> <li>To understand slicing operations in depth, explore our slicing utilities guide</li> <li>For hands-on examples with COCO format operations, see our COCO utilities documentation</li> <li>Want to see these CLI commands in action? Try our interactive notebooks in the demo directory</li> </ul> <p>These resources provide comprehensive examples and explanations to help you make the most of SAHI's command-line interface.</p>"},{"location":"coco/","title":"COCO Utilities","text":"COCO dataset creation:   - import required classes:  <pre><code>from sahi.utils.coco import Coco, CocoCategory, CocoImage, CocoAnnotation\n</code></pre>  - init Coco object:  <pre><code>coco = Coco()\n</code></pre>  - add categories starting from id 0:  <pre><code>coco.add_category(CocoCategory(id=0, name='human'))\ncoco.add_category(CocoCategory(id=1, name='vehicle'))\n</code></pre>  - create a coco image:  <pre><code>coco_image = CocoImage(file_name=\"image1.jpg\", height=1080, width=1920)\n</code></pre>  - add annotations to coco image:  <pre><code>coco_image.add_annotation(\n  CocoAnnotation(\n    bbox=[x_min, y_min, width, height],\n    category_id=0,\n    category_name='human'\n  )\n)\ncoco_image.add_annotation(\n  CocoAnnotation(\n    bbox=[x_min, y_min, width, height],\n    category_id=1,\n    category_name='vehicle'\n  )\n)\n</code></pre>  - add predictions to coco image:  <pre><code>coco_image.add_prediction(\n  CocoPrediction(\n    score=0.864434,\n    bbox=[x_min, y_min, width, height],\n    category_id=0,\n    category_name='human'\n  )\n)\ncoco_image.add_prediction(\n  CocoPrediction(\n    score=0.653424,\n    bbox=[x_min, y_min, width, height],\n    category_id=1,\n    category_name='vehicle'\n  )\n)\n</code></pre>  - add coco image to Coco object:  <pre><code>coco.add_image(coco_image)\n</code></pre>  - after adding all images, convert coco object to coco json:  <pre><code>coco_json = coco.json\n</code></pre>  - you can export it as json file:  <pre><code>from sahi.utils.file import save_json\n\nsave_json(coco_json, \"coco_dataset.json\")\n</code></pre>  - you can also export prediction array in coco prediction format and save it as json :  <pre><code>from sahi.utils.file import save_json\n\npredictions_array = coco.prediction_array\nsave_json = save_json(predictions_array, \"coco_predictions.json\")\n</code></pre>  - this prediction array can be used to get standard coco metrics for the predictions using official pycocotool api :  <pre><code># note:- pycocotools need to be installed separately\nfrom pycocotools.cocoeval import COCOeval\nfrom pycocotools.coco import COCO\n\ncoco_ground_truth = COCO(annotation_file=\"coco_dataset.json\")\ncoco_predictions = coco_ground_truth.loadRes(\"coco_predictions.json\")\n\ncoco_evaluator = COCOeval(coco_ground_truth, coco_predictions, \"bbox\")\ncoco_evaluator.evaluate()\ncoco_evaluator.accumulate()\ncoco_evaluator.summarize()\n</code></pre> Slice COCO dataset images and annotations into grids: <pre><code>from sahi.slicing import slice_coco\n\ncoco_dict, coco_path = slice_coco(\n    coco_annotation_file_path=\"coco.json\",\n    image_dir=\"source/coco/image/dir\",\n    slice_height=256,\n    slice_width=256,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n</code></pre> Split COCO dataset into train/val: <pre><code>from sahi.utils.coco import Coco\nfrom sahi.utils.file import save_json\n\n# specify coco dataset path\ncoco_path = \"coco.json\"\n\n# init Coco object\ncoco = Coco.from_coco_dict_or_path(coco_path)\n\n# split COCO dataset with a 85% train/15% val split\nresult = coco.split_coco_as_train_val(\n  train_split_rate=0.85\n)\n\n# export train val split files\nsave_json(result[\"train_coco\"].json, \"train_split.json\")\nsave_json(result[\"val_coco\"].json, \"val_split.json\")\n</code></pre> Filter/Update COCO dataset by categories: <pre><code>from sahi.utils.coco import Coco\nfrom sahi.utils.file import save_json\n\n# init Coco objects by specifying coco dataset paths and image folder directories\ncoco = Coco.from_coco_dict_or_path(\"coco.json\")\n\n# select only 3 categories; and map them to ids 1, 2 and 3\ndesired_name2id = {\n  \"big_vehicle\": 1,\n  \"car\": 2,\n  \"human\": 3\n}\ncoco.update_categories(desired_name2id)\n\n# export updated/filtered COCO dataset\nsave_json(coco.json, \"updated_coco.json\")\n</code></pre> Filter COCO dataset by annotation area: <pre><code>from sahi.utils.coco import Coco\nfrom sahi.utils.file import save_json\n\n# init Coco objects by specifying coco dataset paths and image folder directories\ncoco = Coco.from_coco_dict_or_path(\"coco.json\")\n\n# filter out images that contain annotations with smaller area than 50\narea_filtered_coco = coco.get_area_filtered_coco(min=50)\n# filter out images that contain annotations with smaller area than 50 and larger area than 10000\narea_filtered_coco = coco.get_area_filtered_coco(min=50, max_val=10000)\n# filter out images with separate area intervals per category\nintervals_per_category = {\n  \"human\": {\"min\": 20, \"max\": 10000},\n  \"vehicle\": {\"min\": 50, \"max\": 15000},\n}\narea_filtered_coco = coco.get_area_filtered_coco(intervals_per_category=intervals_per_category)\n\n# export filtered COCO dataset\nsave_json(area_filtered_coco.json, \"area_filtered_coco.json\")\n</code></pre> Filter out images that does not contain any annotation: <pre><code>from sahi.utils.coco import Coco\n\n# set ignore_negative_samples as False if you want images without annotations present in json and YOLO exports\ncoco = Coco.from_coco_dict_or_path(\"coco.json\", ignore_negative_samples=False)\n</code></pre> Merge COCO dataset files: <pre><code>from sahi.utils.coco import Coco\nfrom sahi.utils.file import save_json\n\n# init Coco objects by specifying coco dataset paths and image folder directories\ncoco_1 = Coco.from_coco_dict_or_path(\"coco1.json\", image_dir=\"images_1/\")\ncoco_2 = Coco.from_coco_dict_or_path(\"coco2.json\", image_dir=\"images_2/\")\n\n# merge Coco datasets\ncoco_1.merge(coco_2)\n\n# export merged COCO dataset\nsave_json(coco_1.json, \"merged_coco.json\")\n</code></pre> Convert COCO dataset to ultralytics/YOLO format: <pre><code>from sahi.utils.coco import Coco\n\n# init Coco object\ncoco = Coco.from_coco_dict_or_path(\"coco.json\", image_dir=\"coco_images/\")\n\n# export converted YOLO formatted dataset into given output_dir with a 85% train/15% val split\ncoco.export_as_yolo(\n  output_dir=\"output/folder/dir\",\n  train_split_rate=0.85\n)\n</code></pre> Convert train/val COCO dataset to ultralytics/YOLO format: <pre><code>from sahi.utils.coco import Coco, export_coco_as_yolo\n\n# init Coco object\ntrain_coco = Coco.from_coco_dict_or_path(\"train_coco.json\", image_dir=\"coco_images/\")\nval_coco = Coco.from_coco_dict_or_path(\"val_coco.json\", image_dir=\"coco_images/\")\n\n# export converted YOLO formatted dataset into given output_dir with given train/val split\ndata_yml_path = export_coco_as_yolo(\n  output_dir=\"output/folder/dir\",\n  train_coco=train_coco,\n  val_coco=val_coco\n)\n</code></pre> Subsample COCO dataset file: <pre><code>from sahi.utils.coco import Coco\n\n# specify coco dataset path\ncoco_path = \"coco.json\"\n\n# init Coco object\ncoco = Coco.from_coco_dict_or_path(coco_path)\n\n# create a Coco object with 1/10 of total images\nsubsampled_coco = coco.get_subsampled_coco(subsample_ratio=10)\n\n# export subsampled COCO dataset\nsave_json(subsampled_coco.json, \"subsampled_coco.json\")\n\n# bonus: create a Coco object with 1/10 of total images that contain first category\nsubsampled_coco = coco.get_subsampled_coco(subsample_ratio=10, category_id=0)\n\n# bonus2: create a Coco object with negative samples reduced to 1/10\nsubsampled_coco = coco.get_subsampled_coco(subsample_ratio=10, category_id=-1)\n</code></pre> Upsample COCO dataset file: <pre><code>from sahi.utils.coco import Coco\n\n# specify coco dataset path\ncoco_path = \"coco.json\"\n\n# init Coco object\ncoco = Coco.from_coco_dict_or_path(coco_path)\n\n# create a Coco object with each sample is repeated 10 times\nupsampled_coco = coco.get_upsampled_coco(upsample_ratio=10)\n\n# export upsampled COCO dataset\nsave_json(upsampled_coco.json, \"upsampled_coco.json\")\n\n# bonus: create a Coco object with images that contain first category repeated 10 times\nsubsampled_coco = coco.get_subsampled_coco(upsample_ratio=10, category_id=0)\n\n# bonus2: create a Coco object with negative samples upsampled by 10 times\nupsampled_coco = coco.get_upsampled_coco(upsample_ratio=10, category_id=-1)\n</code></pre> Get dataset stats: <pre><code>from sahi.utils.coco import Coco\n\n# init Coco object\ncoco = Coco.from_coco_dict_or_path(\"coco.json\")\n\n# get dataset stats\ncoco.stats\n{\n  'num_images': 6471,\n  'num_annotations': 343204,\n  'num_categories': 2,\n  'num_negative_images': 0,\n  'num_images_per_category': {'human': 5684, 'vehicle': 6323},\n  'num_annotations_per_category': {'human': 106396, 'vehicle': 236808},\n  'min_num_annotations_in_image': 1,\n  'max_num_annotations_in_image': 902,\n  'avg_num_annotations_in_image': 53.037243084530985,\n  'min_annotation_area': 3,\n  'max_annotation_area': 328640,\n  'avg_annotation_area': 2448.405738278109,\n  'min_annotation_area_per_category': {'human': 3, 'vehicle': 3},\n  'max_annotation_area_per_category': {'human': 72670, 'vehicle': 328640},\n}\n</code></pre> Remove invalid coco results: <pre><code>from sahi.utils.file import save_json\nfrom sahi.utils.coco import remove_invalid_coco_results\n\n# remove invalid predictions from COCO results JSON\ncoco_results = remove_invalid_coco_results(\"coco_result.json\")\n\n# export processed COCO results\nsave_json(coco_results, \"fixed_coco_result.json\")\n\n# bonus: remove invalid predictions from COCO results JSON by giving COCO\n# dataset path to also filter out bbox results exceeding image height&amp;width\ncoco_results = remove_invalid_coco_results(\"coco_result.json\", \"coco_dataset.json\")\n</code></pre> Get COCO with clipped bounding boxes:   - import required classes:  <pre><code>from sahi.utils.coco import Coco\nfrom sahi.utils.file import save_json\n</code></pre> Usage:  <pre><code># Clip overflowing bounding boxes to image width &amp; height\ncoco = Coco.from_coco_dict_or_path(coco_path, clip_bboxes_to_img_dims=True)\n</code></pre> or,  <pre><code># apply to your already created coco object\ncoco = coco.get_coco_with_clipped_bboxes()\n</code></pre> - Export your clipped_bboxed_coco: <pre><code>save_json(coco.json, \"coco.json\")\n</code></pre>"},{"location":"coco/#interactive-examples-and-additional-resources","title":"Interactive Examples and Additional Resources","text":"<p>Want to see these COCO utilities in action? Here are some helpful resources:</p> <ul> <li>For hands-on examples of COCO dataset slicing, check out our slicing demo notebook</li> <li>To learn about prediction and visualization with COCO datasets, explore our model-specific notebooks in the demo directory</li> <li>For command-line operations with COCO datasets, refer to our CLI documentation</li> </ul> <p>These resources provide practical examples and detailed explanations to help you work effectively with COCO datasets using SAHI.</p>"},{"location":"contributing/","title":"Contributing to SAHI","text":"<p>Thank you for your interest in contributing to SAHI! This guide will help you get started.</p>"},{"location":"contributing/#setting-up-development-environment","title":"Setting Up Development Environment","text":""},{"location":"contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code>git clone https://github.com/YOUR_USERNAME/sahi.git\ncd sahi\n</code></pre>"},{"location":"contributing/#2-create-environment","title":"2. Create Environment","text":"<p>We recommend Python 3.10 for development:</p> <pre><code>pip install uv\nuv venv --python 3.10\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre>"},{"location":"contributing/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code># Install core + dev dependencies\nuv sync --extra dev\n\n# For testing specific models, install their dependencies.\n</code></pre>"},{"location":"contributing/#code-formatting","title":"Code Formatting","text":"<p>We use <code>ruff</code> for code formatting and linting. To format your code:</p> <pre><code># Check formatting\nuv run ruff check .\nuv run ruff format --check .\n\n# Fix formatting\nuv run ruff check --fix .\nuv run ruff format .\n</code></pre> <p>Or use the convenience script:</p> <pre><code># Check formatting\npython scripts/format_code.py check\n\n# Fix formatting\npython scripts/format_code.py fix\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run specific test file\nuv run pytest tests/test_predict.py\n\n# Run with coverage\nuv run pytest --cov=sahi\n</code></pre>"},{"location":"contributing/#submitting-pull-requests","title":"Submitting Pull Requests","text":"<ol> <li>Create a new branch: <code>git checkout -b feature-name</code></li> <li>Make your changes</li> <li>Format your code: <code>python scripts/format_code.py fix</code></li> <li>Run tests: <code>uv run pytest</code></li> <li>Commit with clear message: <code>git commit -m \"Add feature X\"</code></li> <li>Push and create PR: <code>git push origin feature-name</code></li> </ol>"},{"location":"contributing/#ci-build-failures","title":"CI Build Failures","text":"<p>If the CI build fails due to formatting:</p> <ol> <li>Check the CI output for the specific Python version that failed</li> <li>Create environment with that Python version:</li> </ol> <pre><code>uv venv --python 3.X  # Replace X with the version from CI\nsource .venv/bin/activate\n</code></pre> <ol> <li>Install dev dependencies:</li> </ol> <pre><code>uv sync --extra dev\n</code></pre> <ol> <li>Fix formatting:</li> </ol> <pre><code>python scripts/format_code.py fix\n</code></pre> <ol> <li>Commit and push the changes</li> </ol>"},{"location":"contributing/#adding-new-model-support","title":"Adding New Model Support","text":"<p>To add support for a new detection framework:</p> <ol> <li>Create a new file under <code>sahi/models/your_framework.py</code></li> <li>Implement a class that inherits from <code>DetectionModel</code></li> <li>Add your framework to <code>MODEL_TYPE_TO_MODEL_CLASS_NAME</code> in <code>sahi/auto_model.py</code></li> <li>Add tests under <code>tests/test_yourframework.py</code></li> <li>Add a demo notebook under <code>docs/notebooks/inference_for_your_framework.ipynb</code></li> <li>Update <code>README.md</code> and related docs under <code>docs/</code> to include your new model</li> </ol> <p>See existing implementations like <code>sahi/models/ultralytics.py</code> for reference.</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Feel free to start a discussion if you have questions!</p>"},{"location":"fiftyone/","title":"Fiftyone Utilities","text":"<ul> <li>Explore COCO dataset via FiftyOne app:</li> </ul> <p>Supported version: <code>pip install fiftyone&gt;=0.14.2&lt;0.15.0</code></p> <pre><code>from sahi.utils.fiftyone import launch_fiftyone_app\n\n# launch fiftyone app:\nsession = launch_fiftyone_app(coco_image_dir, coco_json_path)\n\n# close fiftyone app:\nsession.close()\n</code></pre> <ul> <li>Convert predictions to FiftyOne detection:</li> </ul> <pre><code>from sahi import get_sliced_prediction\n\n# perform sliced prediction\nresult = get_sliced_prediction(\n    image,\n    detection_model,\n    slice_height = 256,\n    slice_width = 256,\n    overlap_height_ratio = 0.2,\n    overlap_width_ratio = 0.2\n)\n\n# convert detections into fiftyone detection format\nfiftyone_detections = result.to_fiftyone_detections()\n</code></pre> <ul> <li>Explore detection results in Fiftyone UI:</li> </ul> <pre><code>sahi coco fiftyone --image_dir dir/to/images --dataset_json_path dataset.json cocoresult1.json cocoresult2.json\n</code></pre> <p>will open a FiftyOne app that visualizes the given dataset and 2 detection results.</p> <p>Specify IOU threshold for FP/TP by <code>--iou_threshold 0.5</code> argument</p>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2020 obss</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"predict/","title":"Prediction Utilities","text":"<ul> <li>Sliced inference:</li> </ul> <pre><code>from sahi.predict import get_sliced_prediction\nfrom sahi import AutoDetectionModel\n\n# init any model\ndetection_model = AutoDetectionModel.from_pretrained(model_type='mmdet',...) # for MMDetection models\ndetection_model = AutoDetectionModel.from_pretrained(model_type='ultralytics',...) # for YOLOv8/YOLO11/YOLO12 models\ndetection_model = AutoDetectionModel.from_pretrained(model_type='huggingface',...) # for HuggingFace detection models\ndetection_model = AutoDetectionModel.from_pretrained(model_type='torchvision',...) # for Torchvision detection models\n\n# get sliced prediction result\nresult = get_sliced_prediction(\n    image,\n    detection_model,\n    slice_height = 256,\n    slice_width = 256,\n    overlap_height_ratio = 0.2,\n    overlap_width_ratio = 0.2\n)\n</code></pre> <ul> <li>Standard inference:</li> </ul> <pre><code>from sahi.predict import get_prediction\nfrom sahi import AutoDetectionModel\n\n# init a model\ndetection_model = AutoDetectionModel.from_pretrained(...)\n\n# get standard prediction result\nresult = get_prediction(\n    image,\n    detection_model,\n)\n</code></pre> <ul> <li>Batch inference:</li> </ul> <pre><code>from sahi.predict import predict\nfrom sahi import AutoDetectionModel\n\n# init a model\ndetection_model = AutoDetectionModel.from_pretrained(...)\n\n# get batch predict result\nresult = predict(\n    model_type=..., # one of 'ultralytics', 'mmdet', 'huggingface'\n    model_path=..., # path to model weight file\n    model_config_path=..., # for mmdet models\n    model_confidence_threshold=0.5,\n    model_device='cpu', # or 'cuda:0'\n    source=..., # image or folder path\n    no_standard_prediction=True,\n    no_sliced_prediction=False,\n    slice_height=512,\n    slice_width=512,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n    export_pickle=False,\n    export_crop=False,\n    progress_bar=False,\n)\n</code></pre>"},{"location":"predict/#progress-bar","title":"Progress-Bar","text":"<p>Two options were added to control and receive progress updates when running sliced inference over many slices:</p> <ul> <li><code>progress_bar</code> (bool): When True, shows a tqdm progress bar during slice processing. Useful for visual feedback in terminals and notebooks. Default is False.</li> <li><code>progress_callback</code> (callable): A callback function that will be called after each slice (or slice group) is processed. The callback receives two integer arguments: <code>(current_slice_index, total_slices)</code>. Use this to integrate custom progress reporting (for example, update a GUI element or log progress to a file).</li> </ul> <p>Example using the callback:</p> <pre><code>from sahi.predict import get_sliced_prediction\nfrom sahi import AutoDetectionModel\n\n# init model\ndetection_model = AutoDetectionModel.from_pretrained(...)\n\ndef my_progress_callback(current, total):\n    print(f\"Processed {current}/{total} slices\")\n\nresult = get_sliced_prediction(\n    image,\n    detection_model,\n    slice_height=512,\n    slice_width=512,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n    progress_bar=False,           # disable tqdm bar\n    progress_callback=my_progress_callback,  # use callback to receive updates\n)\n</code></pre>"},{"location":"predict/#notes","title":"Notes","text":"<ul> <li><code>progress_bar</code> and <code>progress_callback</code> can be used together. When both are provided, the tqdm bar will display and the callback will be called after each slice group is processed.</li> <li>The <code>progress_callback</code> is called with 1-based indices (i.e. first call will be <code>(1, total)</code>).</li> </ul> <pre><code>- Exclude custom classes on inference:\n\n```python\nfrom sahi.predict import get_sliced_prediction\nfrom sahi import AutoDetectionModel\n\n# init a model\ndetection_model = AutoDetectionModel.from_pretrained(...)\n\n# define the class names to exclude from custom model inference\nexclude_classes_by_name = [\"car\"]\n\n# or exclude classes by its custom id\nexclude_classes_by_id = [0]\n\nresult = get_sliced_prediction(\n    image,\n    detection_model,\n    slice_height = 256,\n    slice_width = 256,\n    overlap_height_ratio = 0.2,\n    overlap_width_ratio = 0.2,\n    exclude_classes_by_name = exclude_classes_by_name\n    # exclude_classes_by_id = exclude_classes_by_id\n)\n</code></pre> <ul> <li>Visualization parameters and export formats:</li> </ul> <pre><code>from sahi.predict import get_prediction\nfrom sahi import AutoDetectionModel\nfrom PIL import Image\n\n# init a model\ndetection_model = AutoDetectionModel.from_pretrained(...)\n\n# get prediction result\nresult = get_prediction(\n    image,\n    detection_model,\n)\n\n# Export with custom visualization parameters\nresult.export_visuals(\n    export_dir=\"outputs/\",\n    text_size=1.0,  # Size of the class label text\n    rect_th=2,      # Thickness of bounding box lines\n    text_th=2,      # Thickness of the text\n    hide_labels=False,  # Set True to hide class labels\n    hide_conf=False,    # Set True to hide confidence scores\n    color=(255, 0, 0),  # Custom color in RGB format (red in this example)\n    file_name=\"custom_visualization\",\n    export_format=\"jpg\"  # Supports 'jpg' and 'png'\n)\n\n# Export as COCO format annotations\ncoco_annotations = result.to_coco_annotations()\n# Example output: [{'image_id': None, 'bbox': [x, y, width, height], 'category_id': 0, 'area': width*height, ...}]\n\n# Export as COCO predictions (includes confidence scores)\ncoco_predictions = result.to_coco_predictions(image_id=1)\n# Example output: [{'image_id': 1, 'bbox': [x, y, width, height], 'score': 0.98, 'category_id': 0, ...}]\n\n# Export as imantics format\nimantics_annotations = result.to_imantics_annotations()\n# For use with imantics library: https://github.com/jsbroks/imantics\n\n# Export for FiftyOne visualization\nfiftyone_detections = result.to_fiftyone_detections()\n# For use with FiftyOne: https://github.com/voxel51/fiftyone\n</code></pre>"},{"location":"predict/#interactive-demos-and-examples","title":"Interactive Demos and Examples","text":"<p>Want to see these prediction utilities in action? We have several interactive notebooks that demonstrate different model integrations:</p> <ul> <li>For YOLOv8/YOLO11/YOLO12 models, explore our Ultralytics integration notebook</li> <li>For YOLOv5 models, check out our YOLOv5 integration notebook</li> <li>For MMDetection models, try our MMDetection integration notebook</li> <li>For HuggingFace models, see our HuggingFace integration notebook</li> <li>For TorchVision models, explore our TorchVision integration notebook</li> <li>For RT-DETR models, check out our RT-DETR integration notebook</li> </ul> <p>These notebooks provide hands-on examples and allow you to experiment with different parameters and settings.</p>"},{"location":"prediction/","title":"Prediction","text":""},{"location":"prediction/#sahi.prediction","title":"<code>sahi.prediction</code>","text":""},{"location":"prediction/#sahi.prediction-classes","title":"Classes","text":""},{"location":"prediction/#sahi.prediction.ObjectPrediction","title":"<code>ObjectPrediction</code>","text":"<p>               Bases: <code>ObjectAnnotation</code></p> <p>Class for handling detection model predictions.</p> Source code in <code>sahi/prediction.py</code> <pre><code>class ObjectPrediction(ObjectAnnotation):\n    \"\"\"Class for handling detection model predictions.\"\"\"\n\n    def __init__(\n        self,\n        bbox: list[int] | None = None,\n        category_id: int | None = None,\n        category_name: str | None = None,\n        segmentation: list[list[float]] | None = None,\n        score: float = 0.0,\n        shift_amount: list[int] | None = [0, 0],\n        full_shape: list[int] | None = None,\n    ):\n        \"\"\"Creates ObjectPrediction from bbox, score, category_id, category_name, segmentation.\n\n        Args:\n            bbox: list\n                [minx, miny, maxx, maxy]\n            score: float\n                Prediction score between 0 and 1\n            category_id: int\n                ID of the object category\n            category_name: str\n                Name of the object category\n            segmentation: List[List]\n                [\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    [x1, y1, x2, y2, x3, y3, ...],\n                    ...\n                ]\n            shift_amount: list\n                To shift the box and mask predictions from sliced image\n                to full sized image, should be in the form of [shift_x, shift_y]\n            full_shape: list\n                Size of the full image after shifting, should be in\n                the form of [height, width]\n        \"\"\"\n        self.score = PredictionScore(score)\n        super().__init__(\n            bbox=bbox,\n            category_id=category_id,\n            segmentation=segmentation,\n            category_name=category_name,\n            shift_amount=shift_amount,\n            full_shape=full_shape,\n        )\n\n    def get_shifted_object_prediction(self):\n        \"\"\"Returns shifted version ObjectPrediction.\n\n        Shifts bbox and mask coords. Used for mapping sliced predictions over full image.\n        \"\"\"\n        if self.mask:\n            shifted_mask = self.mask.get_shifted_mask()\n            return ObjectPrediction(\n                bbox=self.bbox.get_shifted_box().to_xyxy(),\n                category_id=self.category.id,\n                score=self.score.value,\n                segmentation=shifted_mask.segmentation,\n                category_name=self.category.name,\n                shift_amount=[0, 0],\n                full_shape=shifted_mask.full_shape,\n            )\n        else:\n            return ObjectPrediction(\n                bbox=self.bbox.get_shifted_box().to_xyxy(),\n                category_id=self.category.id,\n                score=self.score.value,\n                segmentation=None,\n                category_name=self.category.name,\n                shift_amount=[0, 0],\n                full_shape=None,\n            )\n\n    def to_coco_prediction(self, image_id=None):\n        \"\"\"Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.\"\"\"\n        if self.mask:\n            coco_prediction = CocoPrediction.from_coco_segmentation(\n                segmentation=self.mask.segmentation,\n                category_id=self.category.id,\n                category_name=self.category.name,\n                score=self.score.value,\n                image_id=image_id,\n            )\n        else:\n            coco_prediction = CocoPrediction.from_coco_bbox(\n                bbox=self.bbox.to_xywh(),\n                category_id=self.category.id,\n                category_name=self.category.name,\n                score=self.score.value,\n                image_id=image_id,\n            )\n        return coco_prediction\n\n    def to_fiftyone_detection(self, image_height: int, image_width: int):\n        \"\"\"Returns fiftyone.Detection representation of ObjectPrediction.\"\"\"\n        try:\n            import fiftyone as fo\n        except ImportError:\n            raise ImportError('Please run \"pip install -U fiftyone\" to install fiftyone first for fiftyone conversion.')\n\n        x1, y1, x2, y2 = self.bbox.to_xyxy()\n        rel_box = [x1 / image_width, y1 / image_height, (x2 - x1) / image_width, (y2 - y1) / image_height]\n        fiftyone_detection = fo.Detection(label=self.category.name, bounding_box=rel_box, confidence=self.score.value)\n        return fiftyone_detection\n\n    def __repr__(self):\n        return f\"\"\"ObjectPrediction&lt;\n    bbox: {self.bbox},\n    mask: {self.mask},\n    score: {self.score},\n    category: {self.category}&gt;\"\"\"\n</code></pre>"},{"location":"prediction/#sahi.prediction.ObjectPrediction-functions","title":"Functions","text":""},{"location":"prediction/#sahi.prediction.ObjectPrediction.__init__","title":"<code>__init__(bbox=None, category_id=None, category_name=None, segmentation=None, score=0.0, shift_amount=[0, 0], full_shape=None)</code>","text":"<p>Creates ObjectPrediction from bbox, score, category_id, category_name, segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> \u00b6 <code>list[int] | None</code> <p>list [minx, miny, maxx, maxy]</p> <code>None</code> <code>score</code> \u00b6 <code>float</code> <p>float Prediction score between 0 and 1</p> <code>0.0</code> <code>category_id</code> \u00b6 <code>int | None</code> <p>int ID of the object category</p> <code>None</code> <code>category_name</code> \u00b6 <code>str | None</code> <p>str Name of the object category</p> <code>None</code> <code>segmentation</code> \u00b6 <code>list[list[float]] | None</code> <p>List[List] [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> <code>None</code> <code>shift_amount</code> \u00b6 <code>list[int] | None</code> <p>list To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]</p> <code>[0, 0]</code> <code>full_shape</code> \u00b6 <code>list[int] | None</code> <p>list Size of the full image after shifting, should be in the form of [height, width]</p> <code>None</code> Source code in <code>sahi/prediction.py</code> <pre><code>def __init__(\n    self,\n    bbox: list[int] | None = None,\n    category_id: int | None = None,\n    category_name: str | None = None,\n    segmentation: list[list[float]] | None = None,\n    score: float = 0.0,\n    shift_amount: list[int] | None = [0, 0],\n    full_shape: list[int] | None = None,\n):\n    \"\"\"Creates ObjectPrediction from bbox, score, category_id, category_name, segmentation.\n\n    Args:\n        bbox: list\n            [minx, miny, maxx, maxy]\n        score: float\n            Prediction score between 0 and 1\n        category_id: int\n            ID of the object category\n        category_name: str\n            Name of the object category\n        segmentation: List[List]\n            [\n                [x1, y1, x2, y2, x3, y3, ...],\n                [x1, y1, x2, y2, x3, y3, ...],\n                ...\n            ]\n        shift_amount: list\n            To shift the box and mask predictions from sliced image\n            to full sized image, should be in the form of [shift_x, shift_y]\n        full_shape: list\n            Size of the full image after shifting, should be in\n            the form of [height, width]\n    \"\"\"\n    self.score = PredictionScore(score)\n    super().__init__(\n        bbox=bbox,\n        category_id=category_id,\n        segmentation=segmentation,\n        category_name=category_name,\n        shift_amount=shift_amount,\n        full_shape=full_shape,\n    )\n</code></pre>"},{"location":"prediction/#sahi.prediction.ObjectPrediction.get_shifted_object_prediction","title":"<code>get_shifted_object_prediction()</code>","text":"<p>Returns shifted version ObjectPrediction.</p> <p>Shifts bbox and mask coords. Used for mapping sliced predictions over full image.</p> Source code in <code>sahi/prediction.py</code> <pre><code>def get_shifted_object_prediction(self):\n    \"\"\"Returns shifted version ObjectPrediction.\n\n    Shifts bbox and mask coords. Used for mapping sliced predictions over full image.\n    \"\"\"\n    if self.mask:\n        shifted_mask = self.mask.get_shifted_mask()\n        return ObjectPrediction(\n            bbox=self.bbox.get_shifted_box().to_xyxy(),\n            category_id=self.category.id,\n            score=self.score.value,\n            segmentation=shifted_mask.segmentation,\n            category_name=self.category.name,\n            shift_amount=[0, 0],\n            full_shape=shifted_mask.full_shape,\n        )\n    else:\n        return ObjectPrediction(\n            bbox=self.bbox.get_shifted_box().to_xyxy(),\n            category_id=self.category.id,\n            score=self.score.value,\n            segmentation=None,\n            category_name=self.category.name,\n            shift_amount=[0, 0],\n            full_shape=None,\n        )\n</code></pre>"},{"location":"prediction/#sahi.prediction.ObjectPrediction.to_coco_prediction","title":"<code>to_coco_prediction(image_id=None)</code>","text":"<p>Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.</p> Source code in <code>sahi/prediction.py</code> <pre><code>def to_coco_prediction(self, image_id=None):\n    \"\"\"Returns sahi.utils.coco.CocoPrediction representation of ObjectAnnotation.\"\"\"\n    if self.mask:\n        coco_prediction = CocoPrediction.from_coco_segmentation(\n            segmentation=self.mask.segmentation,\n            category_id=self.category.id,\n            category_name=self.category.name,\n            score=self.score.value,\n            image_id=image_id,\n        )\n    else:\n        coco_prediction = CocoPrediction.from_coco_bbox(\n            bbox=self.bbox.to_xywh(),\n            category_id=self.category.id,\n            category_name=self.category.name,\n            score=self.score.value,\n            image_id=image_id,\n        )\n    return coco_prediction\n</code></pre>"},{"location":"prediction/#sahi.prediction.ObjectPrediction.to_fiftyone_detection","title":"<code>to_fiftyone_detection(image_height, image_width)</code>","text":"<p>Returns fiftyone.Detection representation of ObjectPrediction.</p> Source code in <code>sahi/prediction.py</code> <pre><code>def to_fiftyone_detection(self, image_height: int, image_width: int):\n    \"\"\"Returns fiftyone.Detection representation of ObjectPrediction.\"\"\"\n    try:\n        import fiftyone as fo\n    except ImportError:\n        raise ImportError('Please run \"pip install -U fiftyone\" to install fiftyone first for fiftyone conversion.')\n\n    x1, y1, x2, y2 = self.bbox.to_xyxy()\n    rel_box = [x1 / image_width, y1 / image_height, (x2 - x1) / image_width, (y2 - y1) / image_height]\n    fiftyone_detection = fo.Detection(label=self.category.name, bounding_box=rel_box, confidence=self.score.value)\n    return fiftyone_detection\n</code></pre>"},{"location":"prediction/#sahi.prediction.PredictionResult","title":"<code>PredictionResult</code>","text":"Source code in <code>sahi/prediction.py</code> <pre><code>class PredictionResult:\n    def __init__(\n        self,\n        object_prediction_list: list[ObjectPrediction],\n        image: Image.Image | str | np.ndarray,\n        durations_in_seconds: dict[str, Any] = dict(),\n    ):\n        self.image: Image.Image = read_image_as_pil(image)\n        self.image_width, self.image_height = self.image.size\n        self.object_prediction_list: list[ObjectPrediction] = object_prediction_list\n        self.durations_in_seconds = durations_in_seconds\n\n    def export_visuals(\n        self,\n        export_dir: str,\n        text_size: float | None = None,\n        rect_th: int | None = None,\n        hide_labels: bool = False,\n        hide_conf: bool = False,\n        file_name: str = \"prediction_visual\",\n    ):\n        \"\"\"\n\n        Args:\n            export_dir: directory for resulting visualization to be exported\n            text_size: size of the category name over box\n            rect_th: rectangle thickness\n            hide_labels: hide labels\n            hide_conf: hide confidence\n            file_name: saving name\n        Returns:\n\n        \"\"\"\n        Path(export_dir).mkdir(parents=True, exist_ok=True)\n        visualize_object_predictions(\n            image=np.ascontiguousarray(self.image),\n            object_prediction_list=self.object_prediction_list,\n            rect_th=rect_th,\n            text_size=text_size,\n            text_th=None,\n            color=None,\n            hide_labels=hide_labels,\n            hide_conf=hide_conf,\n            output_dir=export_dir,\n            file_name=file_name,\n            export_format=\"png\",\n        )\n\n    def to_coco_annotations(self):\n        coco_annotation_list = []\n        for object_prediction in self.object_prediction_list:\n            coco_annotation_list.append(object_prediction.to_coco_prediction().json)\n        return coco_annotation_list\n\n    def to_coco_predictions(self, image_id: int | None = None):\n        coco_prediction_list = []\n        for object_prediction in self.object_prediction_list:\n            coco_prediction_list.append(object_prediction.to_coco_prediction(image_id=image_id).json)\n        return coco_prediction_list\n\n    def to_imantics_annotations(self):\n        imantics_annotation_list = []\n        for object_prediction in self.object_prediction_list:\n            imantics_annotation_list.append(object_prediction.to_imantics_annotation())\n        return imantics_annotation_list\n\n    def to_fiftyone_detections(self):\n        try:\n            import fiftyone as fo\n        except ImportError:\n            raise ImportError('Please run \"uv pip install -U fiftyone\" to install fiftyone for conversion.')\n\n        fiftyone_detection_list: list[fo.Detection] = []\n        for object_prediction in self.object_prediction_list:\n            fiftyone_detection_list.append(\n                object_prediction.to_fiftyone_detection(image_height=self.image_height, image_width=self.image_width)\n            )\n        return fiftyone_detection_list\n</code></pre>"},{"location":"prediction/#sahi.prediction.PredictionResult-functions","title":"Functions","text":""},{"location":"prediction/#sahi.prediction.PredictionResult.export_visuals","title":"<code>export_visuals(export_dir, text_size=None, rect_th=None, hide_labels=False, hide_conf=False, file_name='prediction_visual')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>export_dir</code> \u00b6 <code>str</code> <p>directory for resulting visualization to be exported</p> required <code>text_size</code> \u00b6 <code>float | None</code> <p>size of the category name over box</p> <code>None</code> <code>rect_th</code> \u00b6 <code>int | None</code> <p>rectangle thickness</p> <code>None</code> <code>hide_labels</code> \u00b6 <code>bool</code> <p>hide labels</p> <code>False</code> <code>hide_conf</code> \u00b6 <code>bool</code> <p>hide confidence</p> <code>False</code> <code>file_name</code> \u00b6 <code>str</code> <p>saving name</p> <code>'prediction_visual'</code> <p>Returns:</p> Source code in <code>sahi/prediction.py</code> <pre><code>def export_visuals(\n    self,\n    export_dir: str,\n    text_size: float | None = None,\n    rect_th: int | None = None,\n    hide_labels: bool = False,\n    hide_conf: bool = False,\n    file_name: str = \"prediction_visual\",\n):\n    \"\"\"\n\n    Args:\n        export_dir: directory for resulting visualization to be exported\n        text_size: size of the category name over box\n        rect_th: rectangle thickness\n        hide_labels: hide labels\n        hide_conf: hide confidence\n        file_name: saving name\n    Returns:\n\n    \"\"\"\n    Path(export_dir).mkdir(parents=True, exist_ok=True)\n    visualize_object_predictions(\n        image=np.ascontiguousarray(self.image),\n        object_prediction_list=self.object_prediction_list,\n        rect_th=rect_th,\n        text_size=text_size,\n        text_th=None,\n        color=None,\n        hide_labels=hide_labels,\n        hide_conf=hide_conf,\n        output_dir=export_dir,\n        file_name=file_name,\n        export_format=\"png\",\n    )\n</code></pre>"},{"location":"prediction/#sahi.prediction.PredictionScore","title":"<code>PredictionScore</code>","text":"Source code in <code>sahi/prediction.py</code> <pre><code>class PredictionScore:\n    def __init__(self, value: float | np.ndarray):\n        \"\"\"\n        Args:\n            score: prediction score between 0 and 1\n        \"\"\"\n        # if score is a numpy object, convert it to python variable\n        if type(value).__module__ == \"numpy\":\n            value = copy.deepcopy(value).tolist()\n        # set score\n        self.value = value\n\n    def is_greater_than_threshold(self, threshold):\n        \"\"\"Check if score is greater than threshold.\"\"\"\n        return self.value &gt; threshold\n\n    def __eq__(self, threshold):\n        return self.value == threshold\n\n    def __gt__(self, threshold):\n        return self.value &gt; threshold\n\n    def __lt__(self, threshold):\n        return self.value &lt; threshold\n\n    def __repr__(self):\n        return f\"PredictionScore: &lt;value: {self.value}&gt;\"\n</code></pre>"},{"location":"prediction/#sahi.prediction.PredictionScore-functions","title":"Functions","text":""},{"location":"prediction/#sahi.prediction.PredictionScore.__init__","title":"<code>__init__(value)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>score</code> \u00b6 <p>prediction score between 0 and 1</p> required Source code in <code>sahi/prediction.py</code> <pre><code>def __init__(self, value: float | np.ndarray):\n    \"\"\"\n    Args:\n        score: prediction score between 0 and 1\n    \"\"\"\n    # if score is a numpy object, convert it to python variable\n    if type(value).__module__ == \"numpy\":\n        value = copy.deepcopy(value).tolist()\n    # set score\n    self.value = value\n</code></pre>"},{"location":"prediction/#sahi.prediction.PredictionScore.is_greater_than_threshold","title":"<code>is_greater_than_threshold(threshold)</code>","text":"<p>Check if score is greater than threshold.</p> Source code in <code>sahi/prediction.py</code> <pre><code>def is_greater_than_threshold(self, threshold):\n    \"\"\"Check if score is greater than threshold.\"\"\"\n    return self.value &gt; threshold\n</code></pre>"},{"location":"prediction/#sahi.prediction-functions","title":"Functions","text":""},{"location":"quick-start/","title":"Quick Start","text":"<p>Welcome to SAHI! This guide will get you up and running with the core features of the library, including installation, performing predictions, and using the command-line interface.</p>"},{"location":"quick-start/#1-installation","title":"1. Installation","text":"<p>Install SAHI using pip. For object detection, it's recommended to also install <code>ultralytics</code>.</p> <p>Install</p> <p><p><p></p> Pip install (recommended)Conda installGit clone <p>Install or update the <code>sahi</code> package using pip by running <code>pip install -U sahi</code>. For more details on the <code>sahi</code> package, visit the Python Package Index (PyPI).</p> <p> </p> <pre><code># Install the sahi package from PyPI\npip install sahi\n</code></pre> <p>You can also install <code>sahi</code> directly from the Sahi GitHub repository. This can be useful if you want the latest development version. Ensure you have the Git command-line tool installed, and then run:</p> <pre><code># Install the sahi package from GitHub\npip install git+https://github.com/obss/sahi.git@main\n</code></pre> <p>Conda can be used as an alternative package manager to pip. For more details, visit Anaconda. The Sahi feedstock repository for updating the conda package is available at GitHub.</p> <p> </p> <pre><code># Install the sahi package using conda\nconda install -c conda-forge sahi\n</code></pre> <p>Note</p> <p>If you are installing in a CUDA environment, it is best practice to install <code>ultralytics</code>, <code>pytorch</code>, and <code>pytorch-cuda</code> in the same command. This allows the conda package manager to resolve any conflicts. Alternatively, install <code>pytorch-cuda</code> last to override the CPU-specific <code>pytorch</code> package if necessary. <pre><code># Install all packages together using conda\nconda install -c pytorch -c nvidia -c conda-forge pytorch torchvision pytorch-cuda=11.8 ultralytics\n</code></pre></p> <p>Clone the Sahi GitHub repository if you are interested in contributing to development or wish to experiment with the latest source code. After cloning, navigate into the directory and install the package in editable mode <code>-e</code> using pip.</p> <p> </p> <pre><code># Clone the sahi repository\ngit clone https://github.com/obss/sahi\n\n# Navigate to the cloned directory\ncd sahi\n\n# Install the package in editable mode for development\npip install -e .\n</code></pre> <p>See the <code>sahi</code> pyproject.toml file for a list of dependencies.</p>"},{"location":"quick-start/#2-sliced-prediction-with-python","title":"2. Sliced Prediction with Python","text":"<p>Sliced inference is the core feature of SAHI, allowing you to detect small objects in large images. Here's a simple example using the Python API:</p> <pre><code>from sahi import AutoDetectionModel\nfrom sahi.predict import get_sliced_prediction\n\n# Initialize a YOLOv8 model\ndetection_model = AutoDetectionModel.from_pretrained(\n    model_type='yolov8',\n    model_path='yolov8n.pt', # or any other YOLOv8 model\n    confidence_threshold=0.25,\n    device=\"cuda:0\", # or \"cpu\"\n)\n\n# Run sliced prediction\nresult = get_sliced_prediction(\n    \"path/to/your/image.jpg\",\n    detection_model,\n    slice_height=512,\n    slice_width=512,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2\n)\n\n# Export visualizations\nresult.export_visuals(export_dir=\"demo_data/\")\n\n# Get predictions as a list of objects\npredictions = result.object_prediction_list\n</code></pre>"},{"location":"quick-start/#3-prediction-with-the-cli","title":"3. Prediction with the CLI","text":"<p>SAHI also provides a powerful command-line interface for quick predictions without writing any Python code.</p> <pre><code>sahi predict --model_path yolov8n.pt --model_type yolov8 --source /path/to/images/ --slice_height 512 --slice_width 512\n</code></pre> <p>This command will run sliced inference on all images in the specified directory and save the results.</p>"},{"location":"quick-start/#next-steps","title":"Next Steps","text":"<p>You've now seen the basics of SAHI! To dive deeper, check out these resources:</p> <ul> <li>Prediction In-Depth: For advanced prediction options, see the Prediction Utilities guide.</li> <li>Demos: Explore our interactive notebooks in the demo directory for hands-on examples with different models.</li> <li>COCO Tools: Learn how to create, manipulate, and convert datasets in the COCO Utilities guide.</li> <li>All CLI Commands: See the full list of commands in the CLI documentation.</li> </ul>"},{"location":"slicing/","title":"Slicing","text":""},{"location":"slicing/#sahi.slicing","title":"<code>sahi.slicing</code>","text":""},{"location":"slicing/#sahi.slicing-classes","title":"Classes","text":""},{"location":"slicing/#sahi.slicing.SliceImageResult","title":"<code>SliceImageResult</code>","text":"Source code in <code>sahi/slicing.py</code> <pre><code>class SliceImageResult:\n    def __init__(self, original_image_size: list[int], image_dir: str | None = None):\n        \"\"\"\n        image_dir: str\n            Directory of the sliced image exports.\n        original_image_size: list of int\n            Size of the unsliced original image in [height, width]\n        \"\"\"\n        self.original_image_height = original_image_size[0]\n        self.original_image_width = original_image_size[1]\n        self.image_dir = image_dir\n\n        self._sliced_image_list: list[SlicedImage] = []\n\n    def add_sliced_image(self, sliced_image: SlicedImage):\n        if not isinstance(sliced_image, SlicedImage):\n            raise TypeError(\"sliced_image must be a SlicedImage instance\")\n\n        self._sliced_image_list.append(sliced_image)\n\n    @property\n    def sliced_image_list(self):\n        return self._sliced_image_list\n\n    @property\n    def images(self):\n        \"\"\"Returns sliced images.\n\n        Returns:\n            images: a list of np.array\n        \"\"\"\n        images = []\n        for sliced_image in self._sliced_image_list:\n            images.append(sliced_image.image)\n        return images\n\n    @property\n    def coco_images(self) -&gt; list[CocoImage]:\n        \"\"\"Returns CocoImage representation of SliceImageResult.\n\n        Returns:\n            coco_images: a list of CocoImage\n        \"\"\"\n        coco_images: list = []\n        for sliced_image in self._sliced_image_list:\n            coco_images.append(sliced_image.coco_image)\n        return coco_images\n\n    @property\n    def starting_pixels(self) -&gt; list[int]:\n        \"\"\"Returns a list of starting pixels for each slice.\n\n        Returns:\n            starting_pixels: a list of starting pixel coords [x,y]\n        \"\"\"\n        starting_pixels = []\n        for sliced_image in self._sliced_image_list:\n            starting_pixels.append(sliced_image.starting_pixel)\n        return starting_pixels\n\n    @property\n    def filenames(self) -&gt; list[int]:\n        \"\"\"Returns a list of filenames for each slice.\n\n        Returns:\n            filenames: a list of filenames as str\n        \"\"\"\n        filenames = []\n        for sliced_image in self._sliced_image_list:\n            filenames.append(sliced_image.coco_image.file_name)\n        return filenames\n\n    def __getitem__(self, i):\n        def _prepare_ith_dict(i):\n            return {\n                \"image\": self.images[i],\n                \"coco_image\": self.coco_images[i],\n                \"starting_pixel\": self.starting_pixels[i],\n                \"filename\": self.filenames[i],\n            }\n\n        if isinstance(i, np.ndarray):\n            i = i.tolist()\n\n        if isinstance(i, int):\n            return _prepare_ith_dict(i)\n        elif isinstance(i, slice):\n            start, stop, step = i.indices(len(self))\n            return [_prepare_ith_dict(i) for i in range(start, stop, step)]\n        elif isinstance(i, (tuple, list)):\n            accessed_mapping = map(_prepare_ith_dict, i)\n            return list(accessed_mapping)\n        else:\n            raise NotImplementedError(f\"{type(i)}\")\n\n    def __len__(self):\n        return len(self._sliced_image_list)\n</code></pre>"},{"location":"slicing/#sahi.slicing.SliceImageResult-attributes","title":"Attributes","text":""},{"location":"slicing/#sahi.slicing.SliceImageResult.coco_images","title":"<code>coco_images</code>  <code>property</code>","text":"<p>Returns CocoImage representation of SliceImageResult.</p> <p>Returns:</p> Name Type Description <code>coco_images</code> <code>list[CocoImage]</code> <p>a list of CocoImage</p>"},{"location":"slicing/#sahi.slicing.SliceImageResult.filenames","title":"<code>filenames</code>  <code>property</code>","text":"<p>Returns a list of filenames for each slice.</p> <p>Returns:</p> Name Type Description <code>filenames</code> <code>list[int]</code> <p>a list of filenames as str</p>"},{"location":"slicing/#sahi.slicing.SliceImageResult.images","title":"<code>images</code>  <code>property</code>","text":"<p>Returns sliced images.</p> <p>Returns:</p> Name Type Description <code>images</code> <p>a list of np.array</p>"},{"location":"slicing/#sahi.slicing.SliceImageResult.starting_pixels","title":"<code>starting_pixels</code>  <code>property</code>","text":"<p>Returns a list of starting pixels for each slice.</p> <p>Returns:</p> Name Type Description <code>starting_pixels</code> <code>list[int]</code> <p>a list of starting pixel coords [x,y]</p>"},{"location":"slicing/#sahi.slicing.SliceImageResult-functions","title":"Functions","text":""},{"location":"slicing/#sahi.slicing.SliceImageResult.__init__","title":"<code>__init__(original_image_size, image_dir=None)</code>","text":"str <p>Directory of the sliced image exports.</p> <p>original_image_size: list of int     Size of the unsliced original image in [height, width]</p> Source code in <code>sahi/slicing.py</code> <pre><code>def __init__(self, original_image_size: list[int], image_dir: str | None = None):\n    \"\"\"\n    image_dir: str\n        Directory of the sliced image exports.\n    original_image_size: list of int\n        Size of the unsliced original image in [height, width]\n    \"\"\"\n    self.original_image_height = original_image_size[0]\n    self.original_image_width = original_image_size[1]\n    self.image_dir = image_dir\n\n    self._sliced_image_list: list[SlicedImage] = []\n</code></pre>"},{"location":"slicing/#sahi.slicing.SlicedImage","title":"<code>SlicedImage</code>","text":"Source code in <code>sahi/slicing.py</code> <pre><code>class SlicedImage:\n    def __init__(self, image, coco_image, starting_pixel):\n        \"\"\"\n        image: np.array\n            Sliced image.\n        coco_image: CocoImage\n            Coco styled image object that belong to sliced image.\n        starting_pixel: list of list of int\n            Starting pixel coordinates of the sliced image.\n        \"\"\"\n        self.image = image\n        self.coco_image = coco_image\n        self.starting_pixel = starting_pixel\n</code></pre>"},{"location":"slicing/#sahi.slicing.SlicedImage-functions","title":"Functions","text":""},{"location":"slicing/#sahi.slicing.SlicedImage.__init__","title":"<code>__init__(image, coco_image, starting_pixel)</code>","text":"np.array <p>Sliced image.</p> <p>coco_image: CocoImage     Coco styled image object that belong to sliced image. starting_pixel: list of list of int     Starting pixel coordinates of the sliced image.</p> Source code in <code>sahi/slicing.py</code> <pre><code>def __init__(self, image, coco_image, starting_pixel):\n    \"\"\"\n    image: np.array\n        Sliced image.\n    coco_image: CocoImage\n        Coco styled image object that belong to sliced image.\n    starting_pixel: list of list of int\n        Starting pixel coordinates of the sliced image.\n    \"\"\"\n    self.image = image\n    self.coco_image = coco_image\n    self.starting_pixel = starting_pixel\n</code></pre>"},{"location":"slicing/#sahi.slicing-functions","title":"Functions","text":""},{"location":"slicing/#sahi.slicing.annotation_inside_slice","title":"<code>annotation_inside_slice(annotation, slice_bbox)</code>","text":"<p>Check whether annotation coordinates lie inside slice coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>Single annotation entry in COCO format.</p> required <code>List[int]</code> <p>Generated from <code>get_slice_bboxes</code>. Format for each slice bbox: [x_min, y_min, x_max, y_max].</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if any annotation coordinate lies inside slice.</p> Source code in <code>sahi/slicing.py</code> <pre><code>def annotation_inside_slice(annotation: dict, slice_bbox: list[int]) -&gt; bool:\n    \"\"\"Check whether annotation coordinates lie inside slice coordinates.\n\n    Args:\n        annotation (dict): Single annotation entry in COCO format.\n        slice_bbox (List[int]): Generated from `get_slice_bboxes`.\n            Format for each slice bbox: [x_min, y_min, x_max, y_max].\n\n    Returns:\n        (bool): True if any annotation coordinate lies inside slice.\n    \"\"\"\n    left, top, width, height = annotation[\"bbox\"]\n\n    right = left + width\n    bottom = top + height\n\n    if left &gt;= slice_bbox[2]:\n        return False\n    if top &gt;= slice_bbox[3]:\n        return False\n    if right &lt;= slice_bbox[0]:\n        return False\n    if bottom &lt;= slice_bbox[1]:\n        return False\n\n    return True\n</code></pre>"},{"location":"slicing/#sahi.slicing.annotation_inside_slice(annotation)","title":"<code>annotation</code>","text":""},{"location":"slicing/#sahi.slicing.annotation_inside_slice(slice_bbox)","title":"<code>slice_bbox</code>","text":""},{"location":"slicing/#sahi.slicing.calc_aspect_ratio_orientation","title":"<code>calc_aspect_ratio_orientation(width, height)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>int</code> required <code>int</code> required <p>Returns:</p> Type Description <code>str</code> <p>image capture orientation</p> Source code in <code>sahi/slicing.py</code> <pre><code>def calc_aspect_ratio_orientation(width: int, height: int) -&gt; str:\n    \"\"\"\n\n    Args:\n        width:\n        height:\n\n    Returns:\n        image capture orientation\n    \"\"\"\n\n    if width &lt; height:\n        return \"vertical\"\n    elif width &gt; height:\n        return \"horizontal\"\n    else:\n        return \"square\"\n</code></pre>"},{"location":"slicing/#sahi.slicing.calc_aspect_ratio_orientation(width)","title":"<code>width</code>","text":""},{"location":"slicing/#sahi.slicing.calc_aspect_ratio_orientation(height)","title":"<code>height</code>","text":""},{"location":"slicing/#sahi.slicing.calc_ratio_and_slice","title":"<code>calc_ratio_and_slice(orientation, slide=1, ratio=0.1)</code>","text":"<p>According to image resolution calculation overlap params Args:     orientation: image capture angle     slide: sliding window     ratio: buffer value</p> <p>Returns:</p> Type Description <p>overlap params</p> Source code in <code>sahi/slicing.py</code> <pre><code>def calc_ratio_and_slice(orientation: Literal[\"vertical\", \"horizontal\", \"square\"], slide: int = 1, ratio: float = 0.1):\n    \"\"\"\n    According to image resolution calculation overlap params\n    Args:\n        orientation: image capture angle\n        slide: sliding window\n        ratio: buffer value\n\n    Returns:\n        overlap params\n    \"\"\"\n    if orientation == \"vertical\":\n        slice_row, slice_col, overlap_height_ratio, overlap_width_ratio = slide, slide * 2, ratio, ratio\n    elif orientation == \"horizontal\":\n        slice_row, slice_col, overlap_height_ratio, overlap_width_ratio = slide * 2, slide, ratio, ratio\n    elif orientation == \"square\":\n        slice_row, slice_col, overlap_height_ratio, overlap_width_ratio = slide, slide, ratio, ratio\n    else:\n        raise ValueError(f\"Invalid orientation: {orientation}. Must be one of 'vertical', 'horizontal', or 'square'.\")\n\n    return slice_row, slice_col, overlap_height_ratio, overlap_width_ratio\n</code></pre>"},{"location":"slicing/#sahi.slicing.calc_resolution_factor","title":"<code>calc_resolution_factor(resolution)</code>","text":"<p>According to image resolution calculate power(2,n) and return the closest smaller <code>n</code>. Args:     resolution: the width and height of the image multiplied. such as 1024x720 = 737280</p> <p>Returns:</p> Source code in <code>sahi/slicing.py</code> <pre><code>def calc_resolution_factor(resolution: int) -&gt; int:\n    \"\"\"\n    According to image resolution calculate power(2,n) and return the closest smaller `n`.\n    Args:\n        resolution: the width and height of the image multiplied. such as 1024x720 = 737280\n\n    Returns:\n\n    \"\"\"\n    expo = 0\n    while np.power(2, expo) &lt; resolution:\n        expo += 1\n\n    return expo - 1\n</code></pre>"},{"location":"slicing/#sahi.slicing.calc_slice_and_overlap_params","title":"<code>calc_slice_and_overlap_params(resolution, height, width, orientation)</code>","text":"<p>This function calculate according to image resolution slice and overlap params. Args:     resolution: str     height: int     width: int     orientation: str</p> <p>Returns:</p> Type Description <code>tuple[int, int, int, int]</code> <p>x_overlap, y_overlap, slice_width, slice_height</p> Source code in <code>sahi/slicing.py</code> <pre><code>def calc_slice_and_overlap_params(\n    resolution: str, height: int, width: int, orientation: str\n) -&gt; tuple[int, int, int, int]:\n    \"\"\"\n    This function calculate according to image resolution slice and overlap params.\n    Args:\n        resolution: str\n        height: int\n        width: int\n        orientation: str\n\n    Returns:\n        x_overlap, y_overlap, slice_width, slice_height\n    \"\"\"\n\n    if resolution == \"medium\":\n        split_row, split_col, overlap_height_ratio, overlap_width_ratio = calc_ratio_and_slice(\n            orientation, slide=1, ratio=0.8\n        )\n\n    elif resolution == \"high\":\n        split_row, split_col, overlap_height_ratio, overlap_width_ratio = calc_ratio_and_slice(\n            orientation, slide=2, ratio=0.4\n        )\n\n    elif resolution == \"ultra-high\":\n        split_row, split_col, overlap_height_ratio, overlap_width_ratio = calc_ratio_and_slice(\n            orientation, slide=4, ratio=0.4\n        )\n    else:  # low condition\n        split_col = 1\n        split_row = 1\n        overlap_width_ratio = 1\n        overlap_height_ratio = 1\n\n    slice_height = height // split_col\n    slice_width = width // split_row\n\n    x_overlap = int(slice_width * overlap_width_ratio)\n    y_overlap = int(slice_height * overlap_height_ratio)\n\n    return x_overlap, y_overlap, slice_width, slice_height\n</code></pre>"},{"location":"slicing/#sahi.slicing.get_auto_slice_params","title":"<code>get_auto_slice_params(height, width)</code>","text":"<p>According to Image HxW calculate overlap sliding window and buffer params factor is the power value of 2 closest to the image resolution.     factor &lt;= 18: low resolution image such as 300x300, 640x640     18 &lt; factor &lt;= 21: medium resolution image such as 1024x1024, 1336x960     21 &lt; factor &lt;= 24: high resolution image such as 2048x2048, 2048x4096, 4096x4096     factor &gt; 24: ultra-high resolution image such as 6380x6380, 4096x8192 Args:     height:     width:</p> <p>Returns:</p> Type Description <code>tuple[int, int, int, int]</code> <p>slicing overlap params x_overlap, y_overlap, slice_width, slice_height</p> Source code in <code>sahi/slicing.py</code> <pre><code>def get_auto_slice_params(height: int, width: int) -&gt; tuple[int, int, int, int]:\n    \"\"\"\n    According to Image HxW calculate overlap sliding window and buffer params\n    factor is the power value of 2 closest to the image resolution.\n        factor &lt;= 18: low resolution image such as 300x300, 640x640\n        18 &lt; factor &lt;= 21: medium resolution image such as 1024x1024, 1336x960\n        21 &lt; factor &lt;= 24: high resolution image such as 2048x2048, 2048x4096, 4096x4096\n        factor &gt; 24: ultra-high resolution image such as 6380x6380, 4096x8192\n    Args:\n        height:\n        width:\n\n    Returns:\n        slicing overlap params x_overlap, y_overlap, slice_width, slice_height\n    \"\"\"\n    resolution = height * width\n    factor = calc_resolution_factor(resolution)\n    if factor &lt;= 18:\n        return get_resolution_selector(\"low\", height=height, width=width)\n    elif 18 &lt;= factor &lt; 21:\n        return get_resolution_selector(\"medium\", height=height, width=width)\n    elif 21 &lt;= factor &lt; 24:\n        return get_resolution_selector(\"high\", height=height, width=width)\n    else:\n        return get_resolution_selector(\"ultra-high\", height=height, width=width)\n</code></pre>"},{"location":"slicing/#sahi.slicing.get_resolution_selector","title":"<code>get_resolution_selector(res, height, width)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>str</code> <p>resolution of image such as low, medium</p> required <code>int</code> required <code>int</code> required <p>Returns:</p> Type Description <code>tuple[int, int, int, int]</code> <p>trigger slicing params function and return overlap params</p> Source code in <code>sahi/slicing.py</code> <pre><code>def get_resolution_selector(res: str, height: int, width: int) -&gt; tuple[int, int, int, int]:\n    \"\"\"\n\n    Args:\n        res: resolution of image such as low, medium\n        height:\n        width:\n\n    Returns:\n        trigger slicing params function and return overlap params\n    \"\"\"\n    orientation = calc_aspect_ratio_orientation(width=width, height=height)\n    x_overlap, y_overlap, slice_width, slice_height = calc_slice_and_overlap_params(\n        resolution=res, height=height, width=width, orientation=orientation\n    )\n\n    return x_overlap, y_overlap, slice_width, slice_height\n</code></pre>"},{"location":"slicing/#sahi.slicing.get_resolution_selector(res)","title":"<code>res</code>","text":""},{"location":"slicing/#sahi.slicing.get_resolution_selector(height)","title":"<code>height</code>","text":""},{"location":"slicing/#sahi.slicing.get_resolution_selector(width)","title":"<code>width</code>","text":""},{"location":"slicing/#sahi.slicing.get_slice_bboxes","title":"<code>get_slice_bboxes(image_height, image_width, slice_height=None, slice_width=None, auto_slice_resolution=True, overlap_height_ratio=0.2, overlap_width_ratio=0.2)</code>","text":"<p>Generate bounding boxes for slicing an image into crops.</p> <p>The function calculates the coordinates for each slice based on the provided image dimensions, slice size, and overlap ratios. If slice size is not provided and auto_slice_resolution is True, the function will automatically determine appropriate slice parameters.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>Height of the original image.</p> required <code>int</code> <p>Width of the original image.</p> required <code>int</code> <p>Height of each slice. Default None.</p> <code>None</code> <code>int</code> <p>Width of each slice. Default None.</p> <code>None</code> <code>float</code> <p>Fractional overlap in height of each slice (e.g. an overlap of 0.2 for a slice of size 100 yields an overlap of 20 pixels). Default 0.2.</p> <code>0.2</code> <code>float</code> <p>Fractional overlap in width of each slice (e.g. an overlap of 0.2 for a slice of size 100 yields an overlap of 20 pixels). Default 0.2.</p> <code>0.2</code> <code>bool</code> <p>if not set slice parameters such as slice_height and slice_width, it enables automatically calculate these parameters from image resolution and orientation.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[list[int]]</code> <p>List[List[int]]: List of 4 corner coordinates for each N slices. [     [slice_0_left, slice_0_top, slice_0_right, slice_0_bottom],     ...     [slice_N_left, slice_N_top, slice_N_right, slice_N_bottom] ]</p> Source code in <code>sahi/slicing.py</code> <pre><code>def get_slice_bboxes(\n    image_height: int,\n    image_width: int,\n    slice_height: int | None = None,\n    slice_width: int | None = None,\n    auto_slice_resolution: bool | None = True,\n    overlap_height_ratio: float | None = 0.2,\n    overlap_width_ratio: float | None = 0.2,\n) -&gt; list[list[int]]:\n    \"\"\"Generate bounding boxes for slicing an image into crops.\n\n    The function calculates the coordinates for each slice based on the provided\n    image dimensions, slice size, and overlap ratios. If slice size is not provided\n    and auto_slice_resolution is True, the function will automatically determine\n    appropriate slice parameters.\n\n    Args:\n        image_height (int): Height of the original image.\n        image_width (int): Width of the original image.\n        slice_height (int, optional): Height of each slice. Default None.\n        slice_width (int, optional): Width of each slice. Default None.\n        overlap_height_ratio (float, optional): Fractional overlap in height of each\n            slice (e.g. an overlap of 0.2 for a slice of size 100 yields an\n            overlap of 20 pixels). Default 0.2.\n        overlap_width_ratio(float, optional): Fractional overlap in width of each\n            slice (e.g. an overlap of 0.2 for a slice of size 100 yields an\n            overlap of 20 pixels). Default 0.2.\n        auto_slice_resolution (bool, optional): if not set slice parameters such as slice_height and slice_width,\n            it enables automatically calculate these parameters from image resolution and orientation.\n\n    Returns:\n        List[List[int]]: List of 4 corner coordinates for each N slices.\n            [\n                [slice_0_left, slice_0_top, slice_0_right, slice_0_bottom],\n                ...\n                [slice_N_left, slice_N_top, slice_N_right, slice_N_bottom]\n            ]\n    \"\"\"\n    slice_bboxes = []\n    y_max = y_min = 0\n\n    if slice_height and slice_width:\n        y_overlap = int(overlap_height_ratio * slice_height)\n        x_overlap = int(overlap_width_ratio * slice_width)\n    elif auto_slice_resolution:\n        x_overlap, y_overlap, slice_width, slice_height = get_auto_slice_params(height=image_height, width=image_width)\n    else:\n        raise ValueError(\"Compute type is not auto and slice width and height are not provided.\")\n\n    while y_max &lt; image_height:\n        x_min = x_max = 0\n        y_max = y_min + slice_height\n        while x_max &lt; image_width:\n            x_max = x_min + slice_width\n            if y_max &gt; image_height or x_max &gt; image_width:\n                xmax = min(image_width, x_max)\n                ymax = min(image_height, y_max)\n                xmin = max(0, xmax - slice_width)\n                ymin = max(0, ymax - slice_height)\n                slice_bboxes.append([xmin, ymin, xmax, ymax])\n            else:\n                slice_bboxes.append([x_min, y_min, x_max, y_max])\n            x_min = x_max - x_overlap\n        y_min = y_max - y_overlap\n    return slice_bboxes\n</code></pre>"},{"location":"slicing/#sahi.slicing.get_slice_bboxes(image_height)","title":"<code>image_height</code>","text":""},{"location":"slicing/#sahi.slicing.get_slice_bboxes(image_width)","title":"<code>image_width</code>","text":""},{"location":"slicing/#sahi.slicing.get_slice_bboxes(slice_height)","title":"<code>slice_height</code>","text":""},{"location":"slicing/#sahi.slicing.get_slice_bboxes(slice_width)","title":"<code>slice_width</code>","text":""},{"location":"slicing/#sahi.slicing.get_slice_bboxes(overlap_height_ratio)","title":"<code>overlap_height_ratio</code>","text":""},{"location":"slicing/#sahi.slicing.get_slice_bboxes(overlap_width_ratio)","title":"<code>overlap_width_ratio</code>","text":""},{"location":"slicing/#sahi.slicing.get_slice_bboxes(auto_slice_resolution)","title":"<code>auto_slice_resolution</code>","text":""},{"location":"slicing/#sahi.slicing.process_coco_annotations","title":"<code>process_coco_annotations(coco_annotation_list, slice_bbox, min_area_ratio)</code>","text":"<p>Slices and filters given list of CocoAnnotation objects with given 'slice_bbox' and 'min_area_ratio'.</p> <p>Parameters:</p> Name Type Description Default <code>List[int]</code> <p>Generated from <code>get_slice_bboxes</code>. Format for each slice bbox: [x_min, y_min, x_max, y_max].</p> required <code>float</code> <p>If the cropped annotation area to original annotation ratio is smaller than this value, the annotation is filtered out. Default 0.1.</p> required <p>Returns:</p> Type Description <code>List[CocoAnnotation]</code> <p>Sliced annotations.</p> Source code in <code>sahi/slicing.py</code> <pre><code>def process_coco_annotations(\n    coco_annotation_list: list[CocoAnnotation], slice_bbox: list[int], min_area_ratio\n) -&gt; list[CocoAnnotation]:\n    \"\"\"Slices and filters given list of CocoAnnotation objects with given 'slice_bbox' and 'min_area_ratio'.\n\n    Args:\n        coco_annotation_list (List[CocoAnnotation])\n        slice_bbox (List[int]): Generated from `get_slice_bboxes`.\n            Format for each slice bbox: [x_min, y_min, x_max, y_max].\n        min_area_ratio (float): If the cropped annotation area to original\n            annotation ratio is smaller than this value, the annotation is\n            filtered out. Default 0.1.\n\n    Returns:\n        (List[CocoAnnotation]): Sliced annotations.\n    \"\"\"\n\n    sliced_coco_annotation_list: list[CocoAnnotation] = []\n    for coco_annotation in coco_annotation_list:\n        if annotation_inside_slice(coco_annotation.json, slice_bbox):\n            sliced_coco_annotation = coco_annotation.get_sliced_coco_annotation(slice_bbox)\n            if sliced_coco_annotation.area / coco_annotation.area &gt;= min_area_ratio:\n                sliced_coco_annotation_list.append(sliced_coco_annotation)\n    return sliced_coco_annotation_list\n</code></pre>"},{"location":"slicing/#sahi.slicing.process_coco_annotations(slice_bbox)","title":"<code>slice_bbox</code>","text":""},{"location":"slicing/#sahi.slicing.process_coco_annotations(min_area_ratio)","title":"<code>min_area_ratio</code>","text":""},{"location":"slicing/#sahi.slicing.shift_bboxes","title":"<code>shift_bboxes(bboxes, offset)</code>","text":"<p>Shift bboxes w.r.t offset.</p> <p>Suppo</p> <p>Parameters:</p> Name Type Description Default <code>(Tensor, ndarray, list)</code> <p>The bboxes need to be translated. Its shape can be (n, 4), which means (x, y, x, y).</p> required <code>Sequence[int]</code> <p>The translation offsets with shape of (2, ).</p> required <p>Returns:     Tensor, np.ndarray, list: Shifted bboxes.</p> Source code in <code>sahi/slicing.py</code> <pre><code>def shift_bboxes(bboxes, offset: Sequence[int]):\n    \"\"\"Shift bboxes w.r.t offset.\n\n    Suppo\n\n    Args:\n        bboxes (Tensor, np.ndarray, list): The bboxes need to be translated. Its shape can\n            be (n, 4), which means (x, y, x, y).\n        offset (Sequence[int]): The translation offsets with shape of (2, ).\n    Returns:\n        Tensor, np.ndarray, list: Shifted bboxes.\n    \"\"\"\n    shifted_bboxes = []\n\n    if type(bboxes).__module__ == \"torch\":\n        bboxes_is_torch_tensor = True\n    else:\n        bboxes_is_torch_tensor = False\n\n    for bbox in bboxes:\n        if bboxes_is_torch_tensor or isinstance(bbox, np.ndarray):\n            bbox = bbox.tolist()\n        bbox = BoundingBox(bbox, shift_amount=offset)\n        bbox = bbox.get_shifted_box()\n        shifted_bboxes.append(bbox.to_xyxy())\n\n    if isinstance(bboxes, np.ndarray):\n        return np.stack(shifted_bboxes, axis=0)\n    elif bboxes_is_torch_tensor:\n        return bboxes.new_tensor(shifted_bboxes)\n    else:\n        return shifted_bboxes\n</code></pre>"},{"location":"slicing/#sahi.slicing.shift_bboxes(bboxes)","title":"<code>bboxes</code>","text":""},{"location":"slicing/#sahi.slicing.shift_bboxes(offset)","title":"<code>offset</code>","text":""},{"location":"slicing/#sahi.slicing.shift_masks","title":"<code>shift_masks(masks, offset, full_shape)</code>","text":"<p>Shift masks to the original image.</p> <p>Parameters:</p> Name Type Description Default <code>ndarray</code> <p>masks that need to be shifted.</p> required <code>Sequence[int]</code> <p>The offset to translate with shape of (2, ).</p> required <code>Sequence[int]</code> <p>A (height, width) tuple of the huge image's shape.</p> required <p>Returns:     np.ndarray: Shifted masks.</p> Source code in <code>sahi/slicing.py</code> <pre><code>def shift_masks(masks: np.ndarray, offset: Sequence[int], full_shape: Sequence[int]) -&gt; np.ndarray:\n    \"\"\"Shift masks to the original image.\n\n    Args:\n        masks (np.ndarray): masks that need to be shifted.\n        offset (Sequence[int]): The offset to translate with shape of (2, ).\n        full_shape (Sequence[int]): A (height, width) tuple of the huge image's shape.\n    Returns:\n        np.ndarray: Shifted masks.\n    \"\"\"\n    # empty masks\n    if masks is None:\n        return masks\n\n    shifted_masks = []\n    for mask in masks:\n        mask = Mask(segmentation=mask, shift_amount=offset, full_shape=full_shape)\n        mask = mask.get_shifted_mask()\n        shifted_masks.append(mask.bool_mask)\n\n    return np.stack(shifted_masks, axis=0)\n</code></pre>"},{"location":"slicing/#sahi.slicing.shift_masks(masks)","title":"<code>masks</code>","text":""},{"location":"slicing/#sahi.slicing.shift_masks(offset)","title":"<code>offset</code>","text":""},{"location":"slicing/#sahi.slicing.shift_masks(full_shape)","title":"<code>full_shape</code>","text":""},{"location":"slicing/#sahi.slicing.slice_coco","title":"<code>slice_coco(coco_annotation_file_path, image_dir, output_coco_annotation_file_name, output_dir=None, ignore_negative_samples=False, slice_height=512, slice_width=512, overlap_height_ratio=0.2, overlap_width_ratio=0.2, min_area_ratio=0.1, out_ext=None, verbose=False, exif_fix=True)</code>","text":"<p>Slice large images given in a directory, into smaller windows. If output_dir is given, export sliced images and coco file.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Location of the coco annotation file</p> required <code>str</code> <p>Base directory for the images</p> required <code>str</code> <p>File name of the exported coco dataset json.</p> required <code>str</code> <p>Output directory</p> <code>None</code> <code>bool</code> <p>If True, images without annotations are ignored. Defaults to False.</p> <code>False</code> <code>int</code> <p>Height of each slice. Default 512.</p> <code>512</code> <code>int</code> <p>Width of each slice. Default 512.</p> <code>512</code> <code>float</code> <p>Fractional overlap in height of each slice (e.g. an overlap of 0.2 for a slice of size 100 yields an overlap of 20 pixels). Default 0.2.</p> <code>0.2</code> <code>float</code> <p>Fractional overlap in width of each slice (e.g. an overlap of 0.2 for a slice of size 100 yields an overlap of 20 pixels). Default 0.2.</p> <code>0.2</code> <code>float</code> <p>If the cropped annotation area to original annotation ratio is smaller than this value, the annotation is filtered out. Default 0.1.</p> <code>0.1</code> <code>str</code> <p>Extension of saved images. Default is the original suffix.</p> <code>None</code> <code>bool</code> <p>Switch to print relevant values to screen.</p> <code>False</code> <code>bool</code> <p>Whether to apply an EXIF fix to the image.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>coco_dict</code> <code>list[dict | str]</code> <p>dict COCO dict for sliced images and annotations</p> <code>save_path</code> <code>list[dict | str]</code> <p>str Path to the saved coco file</p> Source code in <code>sahi/slicing.py</code> <pre><code>def slice_coco(\n    coco_annotation_file_path: str,\n    image_dir: str,\n    output_coco_annotation_file_name: str,\n    output_dir: str | None = None,\n    ignore_negative_samples: bool | None = False,\n    slice_height: int | None = 512,\n    slice_width: int | None = 512,\n    overlap_height_ratio: float | None = 0.2,\n    overlap_width_ratio: float | None = 0.2,\n    min_area_ratio: float | None = 0.1,\n    out_ext: str | None = None,\n    verbose: bool | None = False,\n    exif_fix: bool = True,\n) -&gt; list[dict | str]:\n    \"\"\"Slice large images given in a directory, into smaller windows. If output_dir is given, export sliced images and\n    coco file.\n\n    Args:\n        coco_annotation_file_path (str): Location of the coco annotation file\n        image_dir (str): Base directory for the images\n        output_coco_annotation_file_name (str): File name of the exported coco\n            dataset json.\n        output_dir (str, optional): Output directory\n        ignore_negative_samples (bool, optional): If True, images without annotations\n            are ignored. Defaults to False.\n        slice_height (int, optional): Height of each slice. Default 512.\n        slice_width (int, optional): Width of each slice. Default 512.\n        overlap_height_ratio (float, optional): Fractional overlap in height of each\n            slice (e.g. an overlap of 0.2 for a slice of size 100 yields an\n            overlap of 20 pixels). Default 0.2.\n        overlap_width_ratio (float, optional): Fractional overlap in width of each\n            slice (e.g. an overlap of 0.2 for a slice of size 100 yields an\n            overlap of 20 pixels). Default 0.2.\n        min_area_ratio (float): If the cropped annotation area to original annotation\n            ratio is smaller than this value, the annotation is filtered out. Default 0.1.\n        out_ext (str, optional): Extension of saved images. Default is the\n            original suffix.\n        verbose (bool, optional): Switch to print relevant values to screen.\n        exif_fix (bool, optional): Whether to apply an EXIF fix to the image.\n\n    Returns:\n        coco_dict: dict\n            COCO dict for sliced images and annotations\n        save_path: str\n            Path to the saved coco file\n    \"\"\"\n\n    # read coco file\n    coco_dict: dict = load_json(coco_annotation_file_path)\n    # create image_id_to_annotation_list mapping\n    coco = Coco.from_coco_dict_or_path(coco_dict)\n    # init sliced coco_utils.CocoImage list\n    sliced_coco_images: list = []\n\n    # iterate over images and slice\n    for idx, coco_image in enumerate(tqdm(coco.images)):\n        # get image path\n        image_path: str = os.path.join(image_dir, coco_image.file_name)\n        # get annotation json list corresponding to selected coco image\n        # slice image\n        try:\n            slice_image_result = slice_image(\n                image=image_path,\n                coco_annotation_list=coco_image.annotations,\n                output_file_name=f\"{Path(coco_image.file_name).stem}_{idx}\",\n                output_dir=output_dir,\n                slice_height=slice_height,\n                slice_width=slice_width,\n                overlap_height_ratio=overlap_height_ratio,\n                overlap_width_ratio=overlap_width_ratio,\n                min_area_ratio=min_area_ratio,\n                out_ext=out_ext,\n                verbose=verbose,\n                exif_fix=exif_fix,\n            )\n            # append slice outputs\n            sliced_coco_images.extend(slice_image_result.coco_images)\n        except TopologicalError:\n            logger.warning(f\"Invalid annotation found, skipping this image: {image_path}\")\n\n    # create and save coco dict\n    coco_dict = create_coco_dict(\n        sliced_coco_images, coco_dict[\"categories\"], ignore_negative_samples=ignore_negative_samples\n    )\n    save_path = \"\"\n    if output_coco_annotation_file_name and output_dir:\n        save_path = Path(output_dir) / (output_coco_annotation_file_name + \"_coco.json\")\n        save_json(coco_dict, save_path)\n\n    return coco_dict, save_path\n</code></pre>"},{"location":"slicing/#sahi.slicing.slice_coco(coco_annotation_file_path)","title":"<code>coco_annotation_file_path</code>","text":""},{"location":"slicing/#sahi.slicing.slice_coco(image_dir)","title":"<code>image_dir</code>","text":""},{"location":"slicing/#sahi.slicing.slice_coco(output_coco_annotation_file_name)","title":"<code>output_coco_annotation_file_name</code>","text":""},{"location":"slicing/#sahi.slicing.slice_coco(output_dir)","title":"<code>output_dir</code>","text":""},{"location":"slicing/#sahi.slicing.slice_coco(ignore_negative_samples)","title":"<code>ignore_negative_samples</code>","text":""},{"location":"slicing/#sahi.slicing.slice_coco(slice_height)","title":"<code>slice_height</code>","text":""},{"location":"slicing/#sahi.slicing.slice_coco(slice_width)","title":"<code>slice_width</code>","text":""},{"location":"slicing/#sahi.slicing.slice_coco(overlap_height_ratio)","title":"<code>overlap_height_ratio</code>","text":""},{"location":"slicing/#sahi.slicing.slice_coco(overlap_width_ratio)","title":"<code>overlap_width_ratio</code>","text":""},{"location":"slicing/#sahi.slicing.slice_coco(min_area_ratio)","title":"<code>min_area_ratio</code>","text":""},{"location":"slicing/#sahi.slicing.slice_coco(out_ext)","title":"<code>out_ext</code>","text":""},{"location":"slicing/#sahi.slicing.slice_coco(verbose)","title":"<code>verbose</code>","text":""},{"location":"slicing/#sahi.slicing.slice_coco(exif_fix)","title":"<code>exif_fix</code>","text":""},{"location":"slicing/#sahi.slicing.slice_image","title":"<code>slice_image(image, coco_annotation_list=None, output_file_name=None, output_dir=None, slice_height=None, slice_width=None, overlap_height_ratio=0.2, overlap_width_ratio=0.2, auto_slice_resolution=True, min_area_ratio=0.1, out_ext=None, verbose=False, exif_fix=True)</code>","text":"<p>Slice a large image into smaller windows. If output_file_name and output_dir is given, export sliced images.</p> <p>Parameters:</p> Name Type Description Default <code>str or Image</code> <p>File path of image or Pillow Image to be sliced.</p> required <code>List[CocoAnnotation]</code> <p>List of CocoAnnotation objects.</p> <code>None</code> <code>str</code> <p>Root name of output files (coordinates will be appended to this)</p> <code>None</code> <code>str</code> <p>Output directory</p> <code>None</code> <code>int</code> <p>Height of each slice. Default None.</p> <code>None</code> <code>int</code> <p>Width of each slice. Default None.</p> <code>None</code> <code>float</code> <p>Fractional overlap in height of each slice (e.g. an overlap of 0.2 for a slice of size 100 yields an overlap of 20 pixels). Default 0.2.</p> <code>0.2</code> <code>float</code> <p>Fractional overlap in width of each slice (e.g. an overlap of 0.2 for a slice of size 100 yields an overlap of 20 pixels). Default 0.2.</p> <code>0.2</code> <code>bool</code> <p>if not set slice parameters such as slice_height and slice_width, it enables automatically calculate these params from image resolution and orientation.</p> <code>True</code> <code>float</code> <p>If the cropped annotation area to original annotation ratio is smaller than this value, the annotation is filtered out. Default 0.1.</p> <code>0.1</code> <code>str</code> <p>Extension of saved images. Default is the original suffix for lossless image formats and png for lossy formats ('.jpg','.jpeg').</p> <code>None</code> <code>bool</code> <p>Switch to print relevant values to screen. Default 'False'.</p> <code>False</code> <code>bool</code> <p>Whether to apply an EXIF fix to the image.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>sliced_image_result</code> <code>SliceImageResult</code> <p>SliceImageResult:                     sliced_image_list: list of SlicedImage                     image_dir: str                         Directory of the sliced image exports.                     original_image_size: list of int                         Size of the unsliced original image in [height, width]</p> Source code in <code>sahi/slicing.py</code> <pre><code>def slice_image(\n    image: str | Image.Image,\n    coco_annotation_list: list[CocoAnnotation] | None = None,\n    output_file_name: str | None = None,\n    output_dir: str | None = None,\n    slice_height: int | None = None,\n    slice_width: int | None = None,\n    overlap_height_ratio: float | None = 0.2,\n    overlap_width_ratio: float | None = 0.2,\n    auto_slice_resolution: bool | None = True,\n    min_area_ratio: float | None = 0.1,\n    out_ext: str | None = None,\n    verbose: bool | None = False,\n    exif_fix: bool = True,\n) -&gt; SliceImageResult:\n    \"\"\"Slice a large image into smaller windows. If output_file_name and output_dir is given, export sliced images.\n\n    Args:\n        image (str or PIL.Image): File path of image or Pillow Image to be sliced.\n        coco_annotation_list (List[CocoAnnotation], optional): List of CocoAnnotation objects.\n        output_file_name (str, optional): Root name of output files (coordinates will\n            be appended to this)\n        output_dir (str, optional): Output directory\n        slice_height (int, optional): Height of each slice. Default None.\n        slice_width (int, optional): Width of each slice. Default None.\n        overlap_height_ratio (float, optional): Fractional overlap in height of each\n            slice (e.g. an overlap of 0.2 for a slice of size 100 yields an\n            overlap of 20 pixels). Default 0.2.\n        overlap_width_ratio (float, optional): Fractional overlap in width of each\n            slice (e.g. an overlap of 0.2 for a slice of size 100 yields an\n            overlap of 20 pixels). Default 0.2.\n        auto_slice_resolution (bool, optional): if not set slice parameters such as slice_height and slice_width,\n            it enables automatically calculate these params from image resolution and orientation.\n        min_area_ratio (float, optional): If the cropped annotation area to original annotation\n            ratio is smaller than this value, the annotation is filtered out. Default 0.1.\n        out_ext (str, optional): Extension of saved images. Default is the\n            original suffix for lossless image formats and png for lossy formats ('.jpg','.jpeg').\n        verbose (bool, optional): Switch to print relevant values to screen.\n            Default 'False'.\n        exif_fix (bool): Whether to apply an EXIF fix to the image.\n\n    Returns:\n        sliced_image_result: SliceImageResult:\n                                sliced_image_list: list of SlicedImage\n                                image_dir: str\n                                    Directory of the sliced image exports.\n                                original_image_size: list of int\n                                    Size of the unsliced original image in [height, width]\n    \"\"\"\n\n    # define verboseprint\n    verboselog = logger.info if verbose else lambda *a, **k: None\n\n    def _export_single_slice(image: np.ndarray, output_dir: str, slice_file_name: str):\n        image_pil = read_image_as_pil(image, exif_fix=exif_fix)\n        slice_file_path = str(Path(output_dir) / slice_file_name)\n        # export sliced image\n        image_pil.save(slice_file_path)\n        image_pil.close()  # to fix https://github.com/obss/sahi/issues/565\n        verboselog(\"sliced image path: \" + slice_file_path)\n\n    # create outdir if not present\n    if output_dir is not None:\n        Path(output_dir).mkdir(parents=True, exist_ok=True)\n\n    # read image\n    image_pil = read_image_as_pil(image, exif_fix=exif_fix)\n    verboselog(\"image.shape: \" + str(image_pil.size))\n\n    image_width, image_height = image_pil.size\n    if not (image_width != 0 and image_height != 0):\n        raise RuntimeError(f\"invalid image size: {image_pil.size} for 'slice_image'.\")\n    slice_bboxes = get_slice_bboxes(\n        image_height=image_height,\n        image_width=image_width,\n        auto_slice_resolution=auto_slice_resolution,\n        slice_height=slice_height,\n        slice_width=slice_width,\n        overlap_height_ratio=overlap_height_ratio,\n        overlap_width_ratio=overlap_width_ratio,\n    )\n\n    n_ims = 0\n\n    # init images and annotations lists\n    sliced_image_result = SliceImageResult(original_image_size=[image_height, image_width], image_dir=output_dir)\n\n    image_pil_arr = np.asarray(image_pil)\n    # iterate over slices\n    for slice_bbox in slice_bboxes:\n        n_ims += 1\n\n        # extract image\n        tlx = slice_bbox[0]\n        tly = slice_bbox[1]\n        brx = slice_bbox[2]\n        bry = slice_bbox[3]\n        image_pil_slice = image_pil_arr[tly:bry, tlx:brx]\n\n        # set image file suffixes\n        slice_suffixes = \"_\".join(map(str, slice_bbox))\n        if out_ext:\n            suffix = out_ext\n        elif hasattr(image_pil, \"filename\"):\n            suffix = Path(getattr(image_pil, \"filename\")).suffix\n            if suffix in IMAGE_EXTENSIONS_LOSSY:\n                suffix = \".png\"\n            elif suffix in IMAGE_EXTENSIONS_LOSSLESS:\n                suffix = Path(image_pil.filename).suffix\n        else:\n            suffix = \".png\"\n\n        # set image file name and path\n        slice_file_name = f\"{output_file_name}_{slice_suffixes}{suffix}\"\n\n        # create coco image\n        slice_width = slice_bbox[2] - slice_bbox[0]\n        slice_height = slice_bbox[3] - slice_bbox[1]\n        coco_image = CocoImage(file_name=slice_file_name, height=slice_height, width=slice_width)\n\n        # append coco annotations (if present) to coco image\n        if coco_annotation_list is not None:\n            for sliced_coco_annotation in process_coco_annotations(coco_annotation_list, slice_bbox, min_area_ratio):\n                coco_image.add_annotation(sliced_coco_annotation)\n\n        # create sliced image and append to sliced_image_result\n        sliced_image = SlicedImage(\n            image=image_pil_slice, coco_image=coco_image, starting_pixel=[slice_bbox[0], slice_bbox[1]]\n        )\n        sliced_image_result.add_sliced_image(sliced_image)\n\n    # export slices if output directory is provided\n    if output_file_name and output_dir:\n        conc_exec = concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS)\n        conc_exec.map(\n            _export_single_slice,\n            sliced_image_result.images,\n            [output_dir] * len(sliced_image_result),\n            sliced_image_result.filenames,\n        )\n\n    verboselog(\n        \"Num slices: \" + str(n_ims) + \" slice_height: \" + str(slice_height) + \" slice_width: \" + str(slice_width)\n    )\n\n    return sliced_image_result\n</code></pre>"},{"location":"slicing/#sahi.slicing.slice_image(image)","title":"<code>image</code>","text":""},{"location":"slicing/#sahi.slicing.slice_image(coco_annotation_list)","title":"<code>coco_annotation_list</code>","text":""},{"location":"slicing/#sahi.slicing.slice_image(output_file_name)","title":"<code>output_file_name</code>","text":""},{"location":"slicing/#sahi.slicing.slice_image(output_dir)","title":"<code>output_dir</code>","text":""},{"location":"slicing/#sahi.slicing.slice_image(slice_height)","title":"<code>slice_height</code>","text":""},{"location":"slicing/#sahi.slicing.slice_image(slice_width)","title":"<code>slice_width</code>","text":""},{"location":"slicing/#sahi.slicing.slice_image(overlap_height_ratio)","title":"<code>overlap_height_ratio</code>","text":""},{"location":"slicing/#sahi.slicing.slice_image(overlap_width_ratio)","title":"<code>overlap_width_ratio</code>","text":""},{"location":"slicing/#sahi.slicing.slice_image(auto_slice_resolution)","title":"<code>auto_slice_resolution</code>","text":""},{"location":"slicing/#sahi.slicing.slice_image(min_area_ratio)","title":"<code>min_area_ratio</code>","text":""},{"location":"slicing/#sahi.slicing.slice_image(out_ext)","title":"<code>out_ext</code>","text":""},{"location":"slicing/#sahi.slicing.slice_image(verbose)","title":"<code>verbose</code>","text":""},{"location":"slicing/#sahi.slicing.slice_image(exif_fix)","title":"<code>exif_fix</code>","text":""},{"location":"slicing/#slicing-utilities","title":"Slicing Utilities","text":"<ul> <li>Slice an image:</li> </ul> <pre><code>from sahi.slicing import slice_image\n\nslice_image_result = slice_image(\n    image=image_path,\n    output_file_name=output_file_name,\n    output_dir=output_dir,\n    slice_height=256,\n    slice_width=256,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n</code></pre> <ul> <li>Slice a COCO formatted dataset:</li> </ul> <pre><code>from sahi.slicing import slice_coco\n\ncoco_dict, coco_path = slice_coco(\n    coco_annotation_file_path=coco_annotation_file_path,\n    image_dir=image_dir,\n    slice_height=256,\n    slice_width=256,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n</code></pre>"},{"location":"slicing/#interactive-demo","title":"Interactive Demo","text":"<p>Want to experiment with different slicing parameters and see their effects? Check out our interactive Jupyter notebook that demonstrates these slicing operations in action.</p>"},{"location":"models/base/","title":"BaseModel","text":""},{"location":"models/base/#sahi.models.base","title":"<code>sahi.models.base</code>","text":""},{"location":"models/base/#sahi.models.base-classes","title":"Classes","text":""},{"location":"models/base/#sahi.models.base.DetectionModel","title":"<code>DetectionModel</code>","text":"Source code in <code>sahi/models/base.py</code> <pre><code>class DetectionModel:\n    required_packages: list[str] | None = None\n\n    def __init__(\n        self,\n        model_path: str | None = None,\n        model: Any | None = None,\n        config_path: str | None = None,\n        device: str | None = None,\n        mask_threshold: float = 0.5,\n        confidence_threshold: float = 0.3,\n        category_mapping: dict | None = None,\n        category_remapping: dict | None = None,\n        load_at_init: bool = True,\n        image_size: int | None = None,\n    ):\n        \"\"\"Init object detection/instance segmentation model.\n\n        Args:\n            model_path: str\n                Path for the instance segmentation model weight\n            config_path: str\n                Path for the mmdetection instance segmentation model config file\n            device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n            mask_threshold: float\n                Value to threshold mask pixels, should be between 0 and 1\n            confidence_threshold: float\n                All predictions with score &lt; confidence_threshold will be discarded\n            category_mapping: dict: str to str\n                Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n            category_remapping: dict: str to int\n                Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n            load_at_init: bool\n                If True, automatically loads the model at initialization\n            image_size: int\n                Inference input size.\n        \"\"\"\n\n        self.model_path = model_path\n        self.config_path = config_path\n        self.model = None\n        self.mask_threshold = mask_threshold\n        self.confidence_threshold = confidence_threshold\n        self.category_mapping = category_mapping\n        self.category_remapping = category_remapping\n        self.image_size = image_size\n        self._original_predictions = None\n        self._object_prediction_list_per_image = None\n        self.set_device(device)\n\n        # automatically ensure dependencies\n        self.check_dependencies()\n\n        # automatically load model if load_at_init is True\n        if load_at_init:\n            if model:\n                self.set_model(model)\n            else:\n                self.load_model()\n\n    def check_dependencies(self, packages: list[str] | None = None) -&gt; None:\n        \"\"\"Ensures required dependencies are installed.\n\n        If 'packages' is None, uses self.required_packages. Subclasses may still call with a custom list for dynamic\n        needs.\n        \"\"\"\n        pkgs = packages if packages is not None else getattr(self, \"required_packages\", [])\n        if pkgs:\n            check_requirements(pkgs)\n\n    def load_model(self):\n        \"\"\"This function should be implemented in a way that detection model should be initialized and set to\n        self.model.\n\n        (self.model_path, self.config_path, and self.device should be utilized)\n        \"\"\"\n        raise NotImplementedError()\n\n    def set_model(self, model: Any, **kwargs):\n        \"\"\"\n        This function should be implemented to instantiate a DetectionModel out of an already loaded model\n        Args:\n            model: Any\n                Loaded model\n        \"\"\"\n        raise NotImplementedError()\n\n    def set_device(self, device: str | None = None):\n        \"\"\"Sets the device pytorch should use for the model.\n\n        Args:\n            device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n        \"\"\"\n\n        self.device = select_device(device)\n\n    def unload_model(self):\n        \"\"\"Unloads the model from CPU/GPU.\"\"\"\n        self.model = None\n        empty_cuda_cache()\n\n    def perform_inference(self, image: np.ndarray):\n        \"\"\"This function should be implemented in a way that prediction should be performed using self.model and the\n        prediction result should be set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"This function should be implemented in a way that self._original_predictions should be converted to a list of\n        prediction.ObjectPrediction and set to self._object_prediction_list.\n\n        self.mask_threshold can also be utilized.\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        raise NotImplementedError()\n\n    def _apply_category_remapping(self):\n        \"\"\"Applies category remapping based on mapping given in self.category_remapping.\"\"\"\n        # confirm self.category_remapping is not None\n        if self.category_remapping is None:\n            raise ValueError(\"self.category_remapping cannot be None\")\n        # remap categories\n        if not isinstance(self._object_prediction_list_per_image, list):\n            logger.error(\n                f\"Unknown type for self._object_prediction_list_per_image: \"\n                f\"{type(self._object_prediction_list_per_image)}\"\n            )\n            return\n        for object_prediction_list in self._object_prediction_list_per_image:  # type: ignore\n            for object_prediction in object_prediction_list:\n                old_category_id_str = str(object_prediction.category.id)\n                new_category_id_int = self.category_remapping[old_category_id_str]\n                object_prediction.category = Category(id=new_category_id_int, name=object_prediction.category.name)\n\n    def convert_original_predictions(\n        self,\n        shift_amount: list[list[int]] | None = [[0, 0]],\n        full_shape: list[list[int]] | None = None,\n    ):\n        \"\"\"Converts original predictions of the detection model to a list of prediction.ObjectPrediction object.\n\n        Should be called after perform_inference().\n        Args:\n            shift_amount: list\n                To shift the box and mask predictions from sliced image to full sized image,\n                    should be in the form of [shift_x, shift_y]\n            full_shape: list\n                Size of the full image after shifting, should be in the form of [height, width]\n        \"\"\"\n        self._create_object_prediction_list_from_original_predictions(\n            shift_amount_list=shift_amount,\n            full_shape_list=full_shape,\n        )\n        if self.category_remapping:\n            self._apply_category_remapping()\n\n    @property\n    def object_prediction_list(self) -&gt; list[list[ObjectPrediction]]:\n        if self._object_prediction_list_per_image is None:\n            return []\n        if len(self._object_prediction_list_per_image) == 0:\n            return []\n        return self._object_prediction_list_per_image[0]\n\n    @property\n    def object_prediction_list_per_image(self) -&gt; list[list[ObjectPrediction]]:\n        return self._object_prediction_list_per_image or []\n\n    @property\n    def original_predictions(self):\n        return self._original_predictions\n</code></pre>"},{"location":"models/base/#sahi.models.base.DetectionModel-functions","title":"Functions","text":""},{"location":"models/base/#sahi.models.base.DetectionModel.__init__","title":"<code>__init__(model_path=None, model=None, config_path=None, device=None, mask_threshold=0.5, confidence_threshold=0.3, category_mapping=None, category_remapping=None, load_at_init=True, image_size=None)</code>","text":"<p>Init object detection/instance segmentation model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> \u00b6 <code>str | None</code> <p>str Path for the instance segmentation model weight</p> <code>None</code> <code>config_path</code> \u00b6 <code>str | None</code> <p>str Path for the mmdetection instance segmentation model config file</p> <code>None</code> <code>device</code> \u00b6 <code>str | None</code> <p>Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.</p> <code>None</code> <code>mask_threshold</code> \u00b6 <code>float</code> <p>float Value to threshold mask pixels, should be between 0 and 1</p> <code>0.5</code> <code>confidence_threshold</code> \u00b6 <code>float</code> <p>float All predictions with score &lt; confidence_threshold will be discarded</p> <code>0.3</code> <code>category_mapping</code> \u00b6 <code>dict | None</code> <p>dict: str to str Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}</p> <code>None</code> <code>category_remapping</code> \u00b6 <code>dict | None</code> <p>dict: str to int Remap category ids based on category names, after performing inference e.g. {\"car\": 3}</p> <code>None</code> <code>load_at_init</code> \u00b6 <code>bool</code> <p>bool If True, automatically loads the model at initialization</p> <code>True</code> <code>image_size</code> \u00b6 <code>int | None</code> <p>int Inference input size.</p> <code>None</code> Source code in <code>sahi/models/base.py</code> <pre><code>def __init__(\n    self,\n    model_path: str | None = None,\n    model: Any | None = None,\n    config_path: str | None = None,\n    device: str | None = None,\n    mask_threshold: float = 0.5,\n    confidence_threshold: float = 0.3,\n    category_mapping: dict | None = None,\n    category_remapping: dict | None = None,\n    load_at_init: bool = True,\n    image_size: int | None = None,\n):\n    \"\"\"Init object detection/instance segmentation model.\n\n    Args:\n        model_path: str\n            Path for the instance segmentation model weight\n        config_path: str\n            Path for the mmdetection instance segmentation model config file\n        device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n        mask_threshold: float\n            Value to threshold mask pixels, should be between 0 and 1\n        confidence_threshold: float\n            All predictions with score &lt; confidence_threshold will be discarded\n        category_mapping: dict: str to str\n            Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n        category_remapping: dict: str to int\n            Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n        load_at_init: bool\n            If True, automatically loads the model at initialization\n        image_size: int\n            Inference input size.\n    \"\"\"\n\n    self.model_path = model_path\n    self.config_path = config_path\n    self.model = None\n    self.mask_threshold = mask_threshold\n    self.confidence_threshold = confidence_threshold\n    self.category_mapping = category_mapping\n    self.category_remapping = category_remapping\n    self.image_size = image_size\n    self._original_predictions = None\n    self._object_prediction_list_per_image = None\n    self.set_device(device)\n\n    # automatically ensure dependencies\n    self.check_dependencies()\n\n    # automatically load model if load_at_init is True\n    if load_at_init:\n        if model:\n            self.set_model(model)\n        else:\n            self.load_model()\n</code></pre>"},{"location":"models/base/#sahi.models.base.DetectionModel.check_dependencies","title":"<code>check_dependencies(packages=None)</code>","text":"<p>Ensures required dependencies are installed.</p> <p>If 'packages' is None, uses self.required_packages. Subclasses may still call with a custom list for dynamic needs.</p> Source code in <code>sahi/models/base.py</code> <pre><code>def check_dependencies(self, packages: list[str] | None = None) -&gt; None:\n    \"\"\"Ensures required dependencies are installed.\n\n    If 'packages' is None, uses self.required_packages. Subclasses may still call with a custom list for dynamic\n    needs.\n    \"\"\"\n    pkgs = packages if packages is not None else getattr(self, \"required_packages\", [])\n    if pkgs:\n        check_requirements(pkgs)\n</code></pre>"},{"location":"models/base/#sahi.models.base.DetectionModel.convert_original_predictions","title":"<code>convert_original_predictions(shift_amount=[[0, 0]], full_shape=None)</code>","text":"<p>Converts original predictions of the detection model to a list of prediction.ObjectPrediction object.</p> <p>Should be called after perform_inference(). Args:     shift_amount: list         To shift the box and mask predictions from sliced image to full sized image,             should be in the form of [shift_x, shift_y]     full_shape: list         Size of the full image after shifting, should be in the form of [height, width]</p> Source code in <code>sahi/models/base.py</code> <pre><code>def convert_original_predictions(\n    self,\n    shift_amount: list[list[int]] | None = [[0, 0]],\n    full_shape: list[list[int]] | None = None,\n):\n    \"\"\"Converts original predictions of the detection model to a list of prediction.ObjectPrediction object.\n\n    Should be called after perform_inference().\n    Args:\n        shift_amount: list\n            To shift the box and mask predictions from sliced image to full sized image,\n                should be in the form of [shift_x, shift_y]\n        full_shape: list\n            Size of the full image after shifting, should be in the form of [height, width]\n    \"\"\"\n    self._create_object_prediction_list_from_original_predictions(\n        shift_amount_list=shift_amount,\n        full_shape_list=full_shape,\n    )\n    if self.category_remapping:\n        self._apply_category_remapping()\n</code></pre>"},{"location":"models/base/#sahi.models.base.DetectionModel.load_model","title":"<code>load_model()</code>","text":"<p>This function should be implemented in a way that detection model should be initialized and set to self.model.</p> <p>(self.model_path, self.config_path, and self.device should be utilized)</p> Source code in <code>sahi/models/base.py</code> <pre><code>def load_model(self):\n    \"\"\"This function should be implemented in a way that detection model should be initialized and set to\n    self.model.\n\n    (self.model_path, self.config_path, and self.device should be utilized)\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"models/base/#sahi.models.base.DetectionModel.perform_inference","title":"<code>perform_inference(image)</code>","text":"<p>This function should be implemented in a way that prediction should be performed using self.model and the prediction result should be set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted.</p> required Source code in <code>sahi/models/base.py</code> <pre><code>def perform_inference(self, image: np.ndarray):\n    \"\"\"This function should be implemented in a way that prediction should be performed using self.model and the\n    prediction result should be set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"models/base/#sahi.models.base.DetectionModel.set_device","title":"<code>set_device(device=None)</code>","text":"<p>Sets the device pytorch should use for the model.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> \u00b6 <code>str | None</code> <p>Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.</p> <code>None</code> Source code in <code>sahi/models/base.py</code> <pre><code>def set_device(self, device: str | None = None):\n    \"\"\"Sets the device pytorch should use for the model.\n\n    Args:\n        device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n    \"\"\"\n\n    self.device = select_device(device)\n</code></pre>"},{"location":"models/base/#sahi.models.base.DetectionModel.set_model","title":"<code>set_model(model, **kwargs)</code>","text":"<p>This function should be implemented to instantiate a DetectionModel out of an already loaded model Args:     model: Any         Loaded model</p> Source code in <code>sahi/models/base.py</code> <pre><code>def set_model(self, model: Any, **kwargs):\n    \"\"\"\n    This function should be implemented to instantiate a DetectionModel out of an already loaded model\n    Args:\n        model: Any\n            Loaded model\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"models/base/#sahi.models.base.DetectionModel.unload_model","title":"<code>unload_model()</code>","text":"<p>Unloads the model from CPU/GPU.</p> Source code in <code>sahi/models/base.py</code> <pre><code>def unload_model(self):\n    \"\"\"Unloads the model from CPU/GPU.\"\"\"\n    self.model = None\n    empty_cuda_cache()\n</code></pre>"},{"location":"models/base/#sahi.models.base-functions","title":"Functions","text":""},{"location":"models/detectron2/","title":"Detectron2Model","text":""},{"location":"models/detectron2/#sahi.models.detectron2","title":"<code>sahi.models.detectron2</code>","text":""},{"location":"models/detectron2/#sahi.models.detectron2-classes","title":"Classes","text":""},{"location":"models/detectron2/#sahi.models.detectron2.Detectron2DetectionModel","title":"<code>Detectron2DetectionModel</code>","text":"<p>               Bases: <code>DetectionModel</code></p> Source code in <code>sahi/models/detectron2.py</code> <pre><code>class Detectron2DetectionModel(DetectionModel):\n    def __init__(self, *args, **kwargs):\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"torch\", \"detectron2\"]\n        super().__init__(*args, **kwargs)\n\n    def load_model(self):\n        from detectron2.config import get_cfg\n        from detectron2.data import MetadataCatalog\n        from detectron2.engine import DefaultPredictor\n        from detectron2.model_zoo import model_zoo\n\n        cfg = get_cfg()\n\n        try:  # try to load from model zoo\n            config_file = model_zoo.get_config_file(self.config_path)\n            cfg.set_new_allowed(True)\n            cfg.merge_from_file(config_file)\n            cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(self.config_path)\n        except Exception as e:  # try to load from local\n            print(e)\n            if self.config_path is not None:\n                cfg.set_new_allowed(True)\n                cfg.merge_from_file(self.config_path)\n            cfg.MODEL.WEIGHTS = self.model_path\n\n        # set model device\n        cfg.MODEL.DEVICE = self.device.type\n        # set input image size\n        if self.image_size is not None:\n            cfg.INPUT.MIN_SIZE_TEST = self.image_size\n            cfg.INPUT.MAX_SIZE_TEST = self.image_size\n        # init predictor\n        model = DefaultPredictor(cfg)\n\n        self.model = model\n\n        # detectron2 category mapping\n        if self.category_mapping is None:\n            try:  # try to parse category names from metadata\n                metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n                category_names = metadata.thing_classes\n                self.category_names = category_names\n                self.category_mapping = {\n                    str(ind): category_name for ind, category_name in enumerate(self.category_names)\n                }\n            except Exception as e:\n                logger.warning(e)\n                # https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html#update-the-config-for-new-datasets\n                if cfg.MODEL.META_ARCHITECTURE == \"RetinaNet\":\n                    num_categories = cfg.MODEL.RETINANET.NUM_CLASSES\n                else:  # fasterrcnn/maskrcnn etc\n                    num_categories = cfg.MODEL.ROI_HEADS.NUM_CLASSES\n                self.category_names = [str(category_id) for category_id in range(num_categories)]\n                self.category_mapping = {\n                    str(ind): category_name for ind, category_name in enumerate(self.category_names)\n                }\n        else:\n            self.category_names = list(self.category_mapping.values())\n\n    def perform_inference(self, image: np.ndarray):\n        \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        \"\"\"\n\n        # Confirm model is loaded\n        if self.model is None:\n            raise RuntimeError(\"Model is not loaded, load it by calling .load_model()\")\n\n        if isinstance(image, np.ndarray) and self.model.input_format == \"BGR\":\n            # convert RGB image to BGR format\n            image = image[:, :, ::-1]\n\n        prediction_result = self.model(image)\n\n        self._original_predictions = prediction_result\n\n    @property\n    def num_categories(self):\n        \"\"\"Returns number of categories.\"\"\"\n        num_categories = len(self.category_mapping)\n        return num_categories\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n        self._object_prediction_list_per_image.\n\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n\n        original_predictions = self._original_predictions\n\n        # compatilibty for sahi v0.8.15\n        if isinstance(shift_amount_list[0], int):\n            shift_amount_list = [shift_amount_list]\n        if full_shape_list is not None and isinstance(full_shape_list[0], int):\n            full_shape_list = [full_shape_list]\n\n        # detectron2 DefaultPredictor supports single image\n        shift_amount = shift_amount_list[0]\n        full_shape = None if full_shape_list is None else full_shape_list[0]\n\n        # parse boxes, masks, scores, category_ids from predictions\n        boxes = original_predictions[\"instances\"].pred_boxes.tensor\n        scores = original_predictions[\"instances\"].scores\n        category_ids = original_predictions[\"instances\"].pred_classes\n\n        # check if predictions contain mask\n        try:\n            masks = original_predictions[\"instances\"].pred_masks\n        except AttributeError:\n            masks = None\n\n        # filter predictions with low confidence\n        high_confidence_mask = scores &gt;= self.confidence_threshold\n        boxes = boxes[high_confidence_mask]\n        scores = scores[high_confidence_mask]\n        category_ids = category_ids[high_confidence_mask]\n        if masks is not None:\n            masks = masks[high_confidence_mask]\n        if masks is not None:\n            object_prediction_list = [\n                ObjectPrediction(\n                    bbox=box.tolist() if mask is None else None,\n                    segmentation=(\n                        get_coco_segmentation_from_bool_mask(mask.detach().cpu().numpy()) if mask is not None else None\n                    ),\n                    category_id=category_id.item(),\n                    category_name=self.category_mapping[str(category_id.item())],\n                    shift_amount=shift_amount,\n                    score=score.item(),\n                    full_shape=full_shape,\n                )\n                for box, score, category_id, mask in zip(boxes, scores, category_ids, masks)\n                if mask is None or get_bbox_from_bool_mask(mask.detach().cpu().numpy()) is not None\n            ]\n        else:\n            object_prediction_list = [\n                ObjectPrediction(\n                    bbox=box.tolist(),\n                    segmentation=None,\n                    category_id=category_id.item(),\n                    category_name=self.category_mapping[str(category_id.item())],\n                    shift_amount=shift_amount,\n                    score=score.item(),\n                    full_shape=full_shape,\n                )\n                for box, score, category_id in zip(boxes, scores, category_ids)\n            ]\n\n        # detectron2 DefaultPredictor supports single image\n        object_prediction_list_per_image = [object_prediction_list]\n\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre>"},{"location":"models/detectron2/#sahi.models.detectron2.Detectron2DetectionModel-attributes","title":"Attributes","text":""},{"location":"models/detectron2/#sahi.models.detectron2.Detectron2DetectionModel.num_categories","title":"<code>num_categories</code>  <code>property</code>","text":"<p>Returns number of categories.</p>"},{"location":"models/detectron2/#sahi.models.detectron2.Detectron2DetectionModel-functions","title":"Functions","text":""},{"location":"models/detectron2/#sahi.models.detectron2.Detectron2DetectionModel.perform_inference","title":"<code>perform_inference(image)</code>","text":"<p>Prediction is performed using self.model and the prediction result is set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.</p> required Source code in <code>sahi/models/detectron2.py</code> <pre><code>def perform_inference(self, image: np.ndarray):\n    \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n    \"\"\"\n\n    # Confirm model is loaded\n    if self.model is None:\n        raise RuntimeError(\"Model is not loaded, load it by calling .load_model()\")\n\n    if isinstance(image, np.ndarray) and self.model.input_format == \"BGR\":\n        # convert RGB image to BGR format\n        image = image[:, :, ::-1]\n\n    prediction_result = self.model(image)\n\n    self._original_predictions = prediction_result\n</code></pre>"},{"location":"models/detectron2/#sahi.models.detectron2-functions","title":"Functions","text":""},{"location":"models/huggingface/","title":"HuggingfaceModel","text":""},{"location":"models/huggingface/#sahi.models.huggingface","title":"<code>sahi.models.huggingface</code>","text":""},{"location":"models/huggingface/#sahi.models.huggingface-classes","title":"Classes","text":""},{"location":"models/huggingface/#sahi.models.huggingface.HuggingfaceDetectionModel","title":"<code>HuggingfaceDetectionModel</code>","text":"<p>               Bases: <code>DetectionModel</code></p> Source code in <code>sahi/models/huggingface.py</code> <pre><code>class HuggingfaceDetectionModel(DetectionModel):\n    def __init__(\n        self,\n        model_path: str | None = None,\n        model: Any | None = None,\n        processor: Any | None = None,\n        config_path: str | None = None,\n        device: str | None = None,\n        mask_threshold: float = 0.5,\n        confidence_threshold: float = 0.3,\n        category_mapping: dict | None = None,\n        category_remapping: dict | None = None,\n        load_at_init: bool = True,\n        image_size: int | None = None,\n        token: str | None = None,\n    ):\n        self._processor = processor\n        self._image_shapes: list = []\n        self._token = token\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"torch\", \"transformers\"]\n        ensure_package_minimum_version(\"transformers\", \"4.42.0\")\n        super().__init__(\n            model_path,\n            model,\n            config_path,\n            device,\n            mask_threshold,\n            confidence_threshold,\n            category_mapping,\n            category_remapping,\n            load_at_init,\n            image_size,\n        )\n\n    @property\n    def processor(self):\n        return self._processor\n\n    @property\n    def image_shapes(self):\n        return self._image_shapes\n\n    @property\n    def num_categories(self) -&gt; int:\n        \"\"\"Returns number of categories.\"\"\"\n        return self.model.config.num_labels\n\n    def load_model(self):\n        from transformers import AutoModelForObjectDetection, AutoProcessor\n\n        hf_token = os.getenv(\"HF_TOKEN\", self._token)\n        model = AutoModelForObjectDetection.from_pretrained(self.model_path, token=hf_token)\n        if self.image_size is not None:\n            if model.base_model_prefix == \"rt_detr_v2\":\n                size = {\"height\": self.image_size, \"width\": self.image_size}\n            else:\n                size = {\"shortest_edge\": self.image_size, \"longest_edge\": None}\n            # use_fast=True raises error: AttributeError: 'SizeDict' object has no attribute 'keys'\n            processor = AutoProcessor.from_pretrained(\n                self.model_path, size=size, do_resize=True, use_fast=False, token=hf_token\n            )\n        else:\n            processor = AutoProcessor.from_pretrained(self.model_path, use_fast=False, token=hf_token)\n        self.set_model(model, processor)\n\n    def set_model(self, model: Any, processor: Any = None, **kwargs):\n        processor = processor or self.processor\n        if processor is None:\n            raise ValueError(f\"'processor' is required to be set, got {processor}.\")\n        elif \"ObjectDetection\" not in model.__class__.__name__ or \"ImageProcessor\" not in processor.__class__.__name__:\n            raise ValueError(\n                \"Given 'model' is not an ObjectDetectionModel or 'processor' is not a valid ImageProcessor.\"\n            )\n        self.model = model\n        self.model.to(self.device)\n        self._processor = processor\n        self.category_mapping = self.model.config.id2label\n\n    def perform_inference(self, image: list | np.ndarray):\n        \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        \"\"\"\n        import torch\n\n        # Confirm model is loaded\n        if self.model is None or self.processor is None:\n            raise RuntimeError(\"Model is not loaded, load it by calling .load_model()\")\n\n        with torch.no_grad():\n            inputs = self.processor(images=image, return_tensors=\"pt\")\n            inputs[\"pixel_values\"] = inputs.pixel_values.to(self.device)\n            if hasattr(inputs, \"pixel_mask\"):\n                inputs[\"pixel_mask\"] = inputs.pixel_mask.to(self.device)\n            outputs = self.model(**inputs)\n\n        if isinstance(image, list):\n            self._image_shapes = [img.shape for img in image]\n        else:\n            self._image_shapes = [image.shape]\n        self._original_predictions = outputs\n\n    def get_valid_predictions(self, logits, pred_boxes) -&gt; tuple:\n        \"\"\"\n        Args:\n            logits: torch.Tensor\n            pred_boxes: torch.Tensor\n        Returns:\n            scores: torch.Tensor\n            cat_ids: torch.Tensor\n            boxes: torch.Tensor\n        \"\"\"\n        import torch\n\n        probs = logits.softmax(-1)\n        scores = probs.max(-1).values\n        cat_ids = probs.argmax(-1)\n        valid_detections = torch.where(cat_ids &lt; self.num_categories, 1, 0)\n        valid_confidences = torch.where(scores &gt;= self.confidence_threshold, 1, 0)\n        valid_mask = valid_detections.logical_and(valid_confidences)\n        scores = scores[valid_mask]\n        cat_ids = cat_ids[valid_mask]\n        boxes = pred_boxes[valid_mask]\n        return scores, cat_ids, boxes\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n        self._object_prediction_list_per_image.\n\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        original_predictions = self._original_predictions\n\n        # compatibility for sahi v0.8.15\n        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n        full_shape_list = fix_full_shape_list(full_shape_list)\n\n        n_image = original_predictions.logits.shape[0]\n        object_prediction_list_per_image = []\n        for image_ind in range(n_image):\n            image_height, image_width, _ = self.image_shapes[image_ind]\n            scores, cat_ids, boxes = self.get_valid_predictions(\n                logits=original_predictions.logits[image_ind], pred_boxes=original_predictions.pred_boxes[image_ind]\n            )\n\n            # create object_prediction_list\n            object_prediction_list = []\n\n            shift_amount = shift_amount_list[image_ind]\n            full_shape = None if full_shape_list is None else full_shape_list[image_ind]\n\n            for ind in range(len(boxes)):\n                category_id = cat_ids[ind].item()\n                yolo_bbox = boxes[ind].tolist()\n                bbox = list(\n                    pbf.convert_bbox(\n                        yolo_bbox,\n                        from_type=\"yolo\",\n                        to_type=\"voc\",\n                        image_size=(image_width, image_height),\n                        return_values=True,\n                        strict=False,\n                    )\n                )\n\n                # fix negative box coords\n                bbox[0] = max(0, bbox[0])\n                bbox[1] = max(0, bbox[1])\n                bbox[2] = min(bbox[2], image_width)\n                bbox[3] = min(bbox[3], image_height)\n\n                object_prediction = ObjectPrediction(\n                    bbox=bbox,\n                    segmentation=None,\n                    category_id=category_id,\n                    category_name=self.category_mapping[category_id],\n                    shift_amount=shift_amount,\n                    score=scores[ind].item(),\n                    full_shape=full_shape,\n                )\n                object_prediction_list.append(object_prediction)\n            object_prediction_list_per_image.append(object_prediction_list)\n\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre>"},{"location":"models/huggingface/#sahi.models.huggingface.HuggingfaceDetectionModel-attributes","title":"Attributes","text":""},{"location":"models/huggingface/#sahi.models.huggingface.HuggingfaceDetectionModel.num_categories","title":"<code>num_categories</code>  <code>property</code>","text":"<p>Returns number of categories.</p>"},{"location":"models/huggingface/#sahi.models.huggingface.HuggingfaceDetectionModel-functions","title":"Functions","text":""},{"location":"models/huggingface/#sahi.models.huggingface.HuggingfaceDetectionModel.get_valid_predictions","title":"<code>get_valid_predictions(logits, pred_boxes)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>logits</code> \u00b6 <p>torch.Tensor</p> required <code>pred_boxes</code> \u00b6 <p>torch.Tensor</p> required <p>Returns:     scores: torch.Tensor     cat_ids: torch.Tensor     boxes: torch.Tensor</p> Source code in <code>sahi/models/huggingface.py</code> <pre><code>def get_valid_predictions(self, logits, pred_boxes) -&gt; tuple:\n    \"\"\"\n    Args:\n        logits: torch.Tensor\n        pred_boxes: torch.Tensor\n    Returns:\n        scores: torch.Tensor\n        cat_ids: torch.Tensor\n        boxes: torch.Tensor\n    \"\"\"\n    import torch\n\n    probs = logits.softmax(-1)\n    scores = probs.max(-1).values\n    cat_ids = probs.argmax(-1)\n    valid_detections = torch.where(cat_ids &lt; self.num_categories, 1, 0)\n    valid_confidences = torch.where(scores &gt;= self.confidence_threshold, 1, 0)\n    valid_mask = valid_detections.logical_and(valid_confidences)\n    scores = scores[valid_mask]\n    cat_ids = cat_ids[valid_mask]\n    boxes = pred_boxes[valid_mask]\n    return scores, cat_ids, boxes\n</code></pre>"},{"location":"models/huggingface/#sahi.models.huggingface.HuggingfaceDetectionModel.perform_inference","title":"<code>perform_inference(image)</code>","text":"<p>Prediction is performed using self.model and the prediction result is set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>list | ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.</p> required Source code in <code>sahi/models/huggingface.py</code> <pre><code>def perform_inference(self, image: list | np.ndarray):\n    \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n    \"\"\"\n    import torch\n\n    # Confirm model is loaded\n    if self.model is None or self.processor is None:\n        raise RuntimeError(\"Model is not loaded, load it by calling .load_model()\")\n\n    with torch.no_grad():\n        inputs = self.processor(images=image, return_tensors=\"pt\")\n        inputs[\"pixel_values\"] = inputs.pixel_values.to(self.device)\n        if hasattr(inputs, \"pixel_mask\"):\n            inputs[\"pixel_mask\"] = inputs.pixel_mask.to(self.device)\n        outputs = self.model(**inputs)\n\n    if isinstance(image, list):\n        self._image_shapes = [img.shape for img in image]\n    else:\n        self._image_shapes = [image.shape]\n    self._original_predictions = outputs\n</code></pre>"},{"location":"models/huggingface/#sahi.models.huggingface-functions","title":"Functions","text":""},{"location":"models/mmdet/","title":"MmdetModel","text":""},{"location":"models/mmdet/#sahi.models.mmdet","title":"<code>sahi.models.mmdet</code>","text":""},{"location":"models/mmdet/#sahi.models.mmdet-classes","title":"Classes","text":""},{"location":"models/mmdet/#sahi.models.mmdet.DetInferencerWrapper","title":"<code>DetInferencerWrapper</code>","text":"<p>               Bases: <code>DetInferencer</code></p> Source code in <code>sahi/models/mmdet.py</code> <pre><code>class DetInferencerWrapper(DetInferencer):\n    def __init__(\n        self,\n        model: ModelType | str | None = None,\n        weights: str | None = None,\n        device: str | None = None,\n        scope: str | None = \"mmdet\",\n        palette: str = \"none\",\n        image_size: int | None = None,\n    ) -&gt; None:\n        self.image_size = image_size\n        super().__init__(model, weights, device, scope, palette)\n\n    def __call__(self, images: list[np.ndarray], batch_size: int = 1) -&gt; dict:\n        \"\"\"\n        Emulate DetInferencer(images) without progressbar\n        Args:\n            images: list of np.ndarray\n                A list of numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n            batch_size: int\n                Inference batch size. Defaults to 1.\n        \"\"\"\n        inputs = self.preprocess(images, batch_size=batch_size)\n        results_dict = {\"predictions\": [], \"visualization\": []}\n        for _, data in inputs:\n            preds = self.forward(data)\n            results = self.postprocess(\n                preds,\n                visualization=None,\n                return_datasample=False,\n                print_result=False,\n                no_save_pred=True,\n                pred_out_dir=None,\n            )\n            results_dict[\"predictions\"].extend(results[\"predictions\"])\n        return results_dict\n\n    def _init_pipeline(self, cfg: ConfigType) -&gt; Compose:\n        \"\"\"Initialize the test pipeline.\"\"\"\n        pipeline_cfg = cfg.test_dataloader.dataset.pipeline\n\n        # For inference, the key of ``img_id`` is not used.\n        if \"meta_keys\" in pipeline_cfg[-1]:\n            pipeline_cfg[-1][\"meta_keys\"] = tuple(\n                meta_key for meta_key in pipeline_cfg[-1][\"meta_keys\"] if meta_key != \"img_id\"\n            )\n\n        load_img_idx = self._get_transform_idx(pipeline_cfg, \"LoadImageFromFile\")\n        if load_img_idx == -1:\n            raise ValueError(\"LoadImageFromFile is not found in the test pipeline\")\n        pipeline_cfg[load_img_idx][\"type\"] = \"mmdet.InferencerLoader\"\n\n        resize_idx = self._get_transform_idx(pipeline_cfg, \"Resize\")\n        if resize_idx == -1:\n            raise ValueError(\"Resize is not found in the test pipeline\")\n        if self.image_size is not None:\n            pipeline_cfg[resize_idx][\"scale\"] = (self.image_size, self.image_size)\n        return Compose(pipeline_cfg)\n</code></pre>"},{"location":"models/mmdet/#sahi.models.mmdet.DetInferencerWrapper-functions","title":"Functions","text":""},{"location":"models/mmdet/#sahi.models.mmdet.DetInferencerWrapper.__call__","title":"<code>__call__(images, batch_size=1)</code>","text":"<p>Emulate DetInferencer(images) without progressbar Args:     images: list of np.ndarray         A list of numpy array that contains the image to be predicted. 3 channel image should be in RGB order.     batch_size: int         Inference batch size. Defaults to 1.</p> Source code in <code>sahi/models/mmdet.py</code> <pre><code>def __call__(self, images: list[np.ndarray], batch_size: int = 1) -&gt; dict:\n    \"\"\"\n    Emulate DetInferencer(images) without progressbar\n    Args:\n        images: list of np.ndarray\n            A list of numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        batch_size: int\n            Inference batch size. Defaults to 1.\n    \"\"\"\n    inputs = self.preprocess(images, batch_size=batch_size)\n    results_dict = {\"predictions\": [], \"visualization\": []}\n    for _, data in inputs:\n        preds = self.forward(data)\n        results = self.postprocess(\n            preds,\n            visualization=None,\n            return_datasample=False,\n            print_result=False,\n            no_save_pred=True,\n            pred_out_dir=None,\n        )\n        results_dict[\"predictions\"].extend(results[\"predictions\"])\n    return results_dict\n</code></pre>"},{"location":"models/mmdet/#sahi.models.mmdet.MmdetDetectionModel","title":"<code>MmdetDetectionModel</code>","text":"<p>               Bases: <code>DetectionModel</code></p> Source code in <code>sahi/models/mmdet.py</code> <pre><code>class MmdetDetectionModel(DetectionModel):\n    def __init__(\n        self,\n        model_path: str | None = None,\n        model: Any | None = None,\n        config_path: str | None = None,\n        device: str | None = None,\n        mask_threshold: float = 0.5,\n        confidence_threshold: float = 0.3,\n        category_mapping: dict | None = None,\n        category_remapping: dict | None = None,\n        load_at_init: bool = True,\n        image_size: int | None = None,\n        scope: str = \"mmdet\",\n    ):\n        self.scope = scope\n        self.image_size = image_size\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"mmdet\", \"mmcv\", \"torch\"]\n        super().__init__(\n            model_path,\n            model,\n            config_path,\n            device,\n            mask_threshold,\n            confidence_threshold,\n            category_mapping,\n            category_remapping,\n            load_at_init,\n            image_size,\n        )\n\n    def load_model(self):\n        \"\"\"Detection model is initialized and set to self.model.\"\"\"\n\n        # create model\n        model = DetInferencerWrapper(\n            self.config_path, self.model_path, device=self.device, scope=self.scope, image_size=self.image_size\n        )\n\n        self.set_model(model)\n\n    def set_model(self, model: Any):\n        \"\"\"Sets the underlying MMDetection model.\n\n        Args:\n            model: Any\n                A MMDetection model\n        \"\"\"\n\n        # set self.model\n        self.model = model\n\n        # set category_mapping\n        if not self.category_mapping:\n            category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n            self.category_mapping = category_mapping\n\n    def perform_inference(self, image: np.ndarray):\n        \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        \"\"\"\n\n        # Confirm model is loaded\n        if self.model is None:\n            raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n\n        # Supports only batch of 1\n\n        # perform inference\n        if isinstance(image, np.ndarray):\n            # https://github.com/obss/sahi/issues/265\n            image = image[:, :, ::-1]\n        # compatibility with sahi v0.8.15\n        if not isinstance(image, list):\n            image_list = [image]\n        prediction_result = self.model(image_list)\n\n        self._original_predictions = prediction_result[\"predictions\"]\n\n    @property\n    def num_categories(self):\n        \"\"\"Returns number of categories.\"\"\"\n        return len(self.category_names)\n\n    @property\n    def has_mask(self):\n        \"\"\"Returns if model output contains segmentation mask.\n\n        Considers both single dataset and ConcatDataset scenarios.\n        \"\"\"\n\n        def check_pipeline_for_mask(pipeline):\n            return any(\n                isinstance(item, dict) and any(\"mask\" in key and value is True for key, value in item.items())\n                for item in pipeline\n            )\n\n        # Access the dataset from the configuration\n        dataset_config = self.model.cfg[\"train_dataloader\"][\"dataset\"]\n\n        if dataset_config[\"type\"] == \"ConcatDataset\":\n            # If using ConcatDataset, check each dataset individually\n            datasets = dataset_config[\"datasets\"]\n            for dataset in datasets:\n                if check_pipeline_for_mask(dataset[\"pipeline\"]):\n                    return True\n        else:\n            # Otherwise, assume a single dataset with its own pipeline\n            if check_pipeline_for_mask(dataset_config[\"pipeline\"]):\n                return True\n\n        return False\n\n    @property\n    def category_names(self):\n        classes = self.model.model.dataset_meta[\"classes\"]\n        if isinstance(classes, str):\n            # https://github.com/open-mmlab/mmdetection/pull/4973\n            return (classes,)\n        else:\n            return classes\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n        self._object_prediction_list_per_image.\n\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        try:\n            from pycocotools import mask as mask_utils\n\n            can_decode_rle = True\n        except ImportError:\n            can_decode_rle = False\n        original_predictions = self._original_predictions\n        category_mapping = self.category_mapping\n\n        # compatilibty for sahi v0.8.15\n        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n        full_shape_list = fix_full_shape_list(full_shape_list)\n\n        # parse boxes and masks from predictions\n        object_prediction_list_per_image = []\n        for image_ind, original_prediction in enumerate(original_predictions):\n            shift_amount = shift_amount_list[image_ind]\n            full_shape = None if full_shape_list is None else full_shape_list[image_ind]\n\n            boxes = original_prediction[\"bboxes\"]\n            scores = original_prediction[\"scores\"]\n            labels = original_prediction[\"labels\"]\n            if self.has_mask:\n                masks = original_prediction[\"masks\"]\n\n            object_prediction_list = []\n\n            n_detects = len(labels)\n            # process predictions\n            for i in range(n_detects):\n                if self.has_mask:\n                    mask = masks[i]\n\n                bbox = boxes[i]\n                score = scores[i]\n                category_id = labels[i]\n                category_name = category_mapping[str(category_id)]\n\n                # ignore low scored predictions\n                if score &lt; self.confidence_threshold:\n                    continue\n\n                # parse prediction mask\n                if self.has_mask:\n                    if \"counts\" in mask:\n                        if can_decode_rle:\n                            bool_mask = mask_utils.decode(mask)\n                        else:\n                            raise ValueError(\n                                \"Can not decode rle mask. Please install pycocotools. ex: 'pip install pycocotools'\"\n                            )\n                    else:\n                        bool_mask = mask\n                    # check if mask is valid\n                    # https://github.com/obss/sahi/discussions/696\n                    if get_bbox_from_bool_mask(bool_mask) is None:\n                        continue\n                    segmentation = get_coco_segmentation_from_bool_mask(bool_mask)\n                else:\n                    segmentation = None\n\n                # fix negative box coords\n                bbox[0] = max(0, bbox[0])\n                bbox[1] = max(0, bbox[1])\n                bbox[2] = max(0, bbox[2])\n                bbox[3] = max(0, bbox[3])\n\n                # fix out of image box coords\n                if full_shape is not None:\n                    bbox[0] = min(full_shape[1], bbox[0])\n                    bbox[1] = min(full_shape[0], bbox[1])\n                    bbox[2] = min(full_shape[1], bbox[2])\n                    bbox[3] = min(full_shape[0], bbox[3])\n\n                # ignore invalid predictions\n                if not (bbox[0] &lt; bbox[2]) or not (bbox[1] &lt; bbox[3]):\n                    logger.warning(f\"ignoring invalid prediction with bbox: {bbox}\")\n                    continue\n\n                object_prediction = ObjectPrediction(\n                    bbox=bbox,\n                    category_id=category_id,\n                    score=score,\n                    segmentation=segmentation,\n                    category_name=category_name,\n                    shift_amount=shift_amount,\n                    full_shape=full_shape,\n                )\n                object_prediction_list.append(object_prediction)\n            object_prediction_list_per_image.append(object_prediction_list)\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre>"},{"location":"models/mmdet/#sahi.models.mmdet.MmdetDetectionModel-attributes","title":"Attributes","text":""},{"location":"models/mmdet/#sahi.models.mmdet.MmdetDetectionModel.has_mask","title":"<code>has_mask</code>  <code>property</code>","text":"<p>Returns if model output contains segmentation mask.</p> <p>Considers both single dataset and ConcatDataset scenarios.</p>"},{"location":"models/mmdet/#sahi.models.mmdet.MmdetDetectionModel.num_categories","title":"<code>num_categories</code>  <code>property</code>","text":"<p>Returns number of categories.</p>"},{"location":"models/mmdet/#sahi.models.mmdet.MmdetDetectionModel-functions","title":"Functions","text":""},{"location":"models/mmdet/#sahi.models.mmdet.MmdetDetectionModel.load_model","title":"<code>load_model()</code>","text":"<p>Detection model is initialized and set to self.model.</p> Source code in <code>sahi/models/mmdet.py</code> <pre><code>def load_model(self):\n    \"\"\"Detection model is initialized and set to self.model.\"\"\"\n\n    # create model\n    model = DetInferencerWrapper(\n        self.config_path, self.model_path, device=self.device, scope=self.scope, image_size=self.image_size\n    )\n\n    self.set_model(model)\n</code></pre>"},{"location":"models/mmdet/#sahi.models.mmdet.MmdetDetectionModel.perform_inference","title":"<code>perform_inference(image)</code>","text":"<p>Prediction is performed using self.model and the prediction result is set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.</p> required Source code in <code>sahi/models/mmdet.py</code> <pre><code>def perform_inference(self, image: np.ndarray):\n    \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n    \"\"\"\n\n    # Confirm model is loaded\n    if self.model is None:\n        raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n\n    # Supports only batch of 1\n\n    # perform inference\n    if isinstance(image, np.ndarray):\n        # https://github.com/obss/sahi/issues/265\n        image = image[:, :, ::-1]\n    # compatibility with sahi v0.8.15\n    if not isinstance(image, list):\n        image_list = [image]\n    prediction_result = self.model(image_list)\n\n    self._original_predictions = prediction_result[\"predictions\"]\n</code></pre>"},{"location":"models/mmdet/#sahi.models.mmdet.MmdetDetectionModel.set_model","title":"<code>set_model(model)</code>","text":"<p>Sets the underlying MMDetection model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> \u00b6 <code>Any</code> <p>Any A MMDetection model</p> required Source code in <code>sahi/models/mmdet.py</code> <pre><code>def set_model(self, model: Any):\n    \"\"\"Sets the underlying MMDetection model.\n\n    Args:\n        model: Any\n            A MMDetection model\n    \"\"\"\n\n    # set self.model\n    self.model = model\n\n    # set category_mapping\n    if not self.category_mapping:\n        category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n        self.category_mapping = category_mapping\n</code></pre>"},{"location":"models/mmdet/#sahi.models.mmdet-functions","title":"Functions","text":""},{"location":"models/roboflow/","title":"RoboflowModel","text":""},{"location":"models/roboflow/#sahi.models.roboflow","title":"<code>sahi.models.roboflow</code>","text":""},{"location":"models/roboflow/#sahi.models.roboflow-classes","title":"Classes","text":""},{"location":"models/roboflow/#sahi.models.roboflow.RoboflowDetectionModel","title":"<code>RoboflowDetectionModel</code>","text":"<p>               Bases: <code>DetectionModel</code></p> Source code in <code>sahi/models/roboflow.py</code> <pre><code>class RoboflowDetectionModel(DetectionModel):\n    def __init__(\n        self,\n        model: Any | None = None,\n        model_path: str | None = None,\n        config_path: str | None = None,\n        device: str | None = None,\n        mask_threshold: float = 0.5,\n        confidence_threshold: float = 0.3,\n        category_mapping: dict | None = None,\n        category_remapping: dict | None = None,\n        load_at_init: bool = True,\n        image_size: int | None = None,\n        api_key: str | None = None,\n    ):\n        \"\"\"Initialize the RoboflowDetectionModel with the given parameters.\n\n        Args:\n            model_path: str\n                Path for the instance segmentation model weight\n            config_path: str\n                Path for the mmdetection instance segmentation model config file\n            device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n            mask_threshold: float\n                Value to threshold mask pixels, should be between 0 and 1\n            confidence_threshold: float\n                All predictions with score &lt; confidence_threshold will be discarded\n            category_mapping: dict: str to str\n                Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n            category_remapping: dict: str to int\n                Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n            load_at_init: bool\n                If True, automatically loads the model at initialization\n            image_size: int\n                Inference input size.\n        \"\"\"\n        self._use_universe = model and isinstance(model, str)\n        self._model = model\n        self._device = device\n        self._api_key = api_key\n\n        if self._use_universe:\n            existing_packages = getattr(self, \"required_packages\", None) or []\n            self.required_packages = [*list(existing_packages), \"inference\"]\n        else:\n            existing_packages = getattr(self, \"required_packages\", None) or []\n            self.required_packages = [*list(existing_packages), \"rfdetr\"]\n\n        super().__init__(\n            model=model,\n            model_path=model_path,\n            config_path=config_path,\n            device=device,\n            mask_threshold=mask_threshold,\n            confidence_threshold=confidence_threshold,\n            category_mapping=category_mapping,\n            category_remapping=category_remapping,\n            load_at_init=False,\n            image_size=image_size,\n        )\n\n        if load_at_init:\n            self.load_model()\n\n    def set_model(self, model: Any, **kwargs):\n        \"\"\"\n        This function should be implemented to instantiate a DetectionModel out of an already loaded model\n        Args:\n            model: Any\n                Loaded model\n        \"\"\"\n        self.model = model\n\n    def load_model(self):\n        \"\"\"This function should be implemented in a way that detection model should be initialized and set to\n        self.model.\n\n        (self.model_path, self.config_path, and self.device should be utilized)\n        \"\"\"\n        if self._use_universe:\n            from inference import get_model\n            from inference.core.env import API_KEY\n            from inference.core.exceptions import RoboflowAPINotAuthorizedError\n\n            api_key = self._api_key or API_KEY\n\n            try:\n                model = get_model(self._model, api_key=api_key)\n            except RoboflowAPINotAuthorizedError as e:\n                raise ValueError(\n                    \"Authorization failed. Please pass a valid API key with \"\n                    \"the `api_key` parameter or set the `ROBOFLOW_API_KEY` environment variable.\"\n                ) from e\n\n            assert model.task_type == \"object-detection\", \"Roboflow model must be an object detection model.\"\n\n        else:\n            from rfdetr.detr import RFDETRBase, RFDETRLarge, RFDETRMedium, RFDETRNano, RFDETRSmall\n\n            model, model_path = self._model, self.model_path\n            model_names = (\"RFDETRBase\", \"RFDETRNano\", \"RFDETRSmall\", \"RFDETRMedium\", \"RFDETRLarge\")\n            if hasattr(model, \"__name__\") and model.__name__ in model_names:\n                model_params = dict(\n                    resolution=int(self.image_size) if self.image_size else 560,\n                    device=self._device,\n                    num_classes=len(self.category_mapping.keys()) if self.category_mapping else None,\n                )\n                if model_path:\n                    model_params[\"pretrain_weights\"] = model_path\n\n                model = model(**model_params)\n            elif isinstance(model, (RFDETRBase, RFDETRNano, RFDETRSmall, RFDETRMedium, RFDETRLarge)):\n                model = model\n            else:\n                raise ValueError(\n                    f\"Model must be a Roboflow model string or one of {model_names} models, got {self.model}.\"\n                )\n\n        self.set_model(model)\n\n    def perform_inference(\n        self,\n        image: np.ndarray,\n    ):\n        \"\"\"This function should be implemented in a way that prediction should be performed using self.model and the\n        prediction result should be set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted.\n        \"\"\"\n        if self._use_universe:\n            self._original_predictions = self.model.infer(image, confidence=self.confidence_threshold)\n        else:\n            self._original_predictions = [self.model.predict(image, threshold=self.confidence_threshold)]\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"This function should be implemented in a way that self._original_predictions should be converted to a list of\n        prediction.ObjectPrediction and set to self._object_prediction_list.\n\n        self.mask_threshold can also be utilized.\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        # compatibility for sahi v0.8.15\n        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n        full_shape_list = fix_full_shape_list(full_shape_list)\n\n        object_prediction_list: list[ObjectPrediction] = []\n\n        if self._use_universe:\n            from inference.core.entities.responses.inference import (\n                ObjectDetectionInferenceResponse as InferenceObjectDetectionInferenceResponse,\n            )\n            from inference.core.entities.responses.inference import (\n                ObjectDetectionPrediction as InferenceObjectDetectionPrediction,\n            )\n\n            original_reponses: list[InferenceObjectDetectionInferenceResponse] = self._original_predictions\n\n            assert len(original_reponses) == len(shift_amount_list) == len(full_shape_list), (\n                \"Length mismatch between original responses, shift amounts, and full shapes.\"\n            )\n\n            for original_reponse, shift_amount, full_shape in zip(\n                original_reponses,\n                shift_amount_list,\n                full_shape_list,\n            ):\n                for prediction in original_reponse.predictions:\n                    prediction: InferenceObjectDetectionPrediction\n                    bbox = [\n                        prediction.x - prediction.width / 2,\n                        prediction.y - prediction.height / 2,\n                        prediction.x + prediction.width / 2,\n                        prediction.y + prediction.height / 2,\n                    ]\n                    object_prediction = ObjectPrediction(\n                        bbox=bbox,\n                        category_id=prediction.class_id,\n                        category_name=prediction.class_name,\n                        score=prediction.confidence,\n                        shift_amount=shift_amount,\n                        full_shape=full_shape,\n                    )\n                    object_prediction_list.append(object_prediction)\n\n        else:\n            from supervision.detection.core import Detections\n\n            original_detections: list[Detections] = self._original_predictions\n\n            assert len(original_detections) == len(shift_amount_list) == len(full_shape_list), (\n                \"Length mismatch between original responses, shift amounts, and full shapes.\"\n            )\n\n            for original_detection, shift_amount, full_shape in zip(\n                original_detections,\n                shift_amount_list,\n                full_shape_list,\n            ):\n                for xyxy, confidence, class_id in zip(\n                    original_detection.xyxy,\n                    original_detection.confidence,\n                    original_detection.class_id,\n                ):\n                    object_prediction = ObjectPrediction(\n                        bbox=xyxy,\n                        category_id=int(class_id),\n                        category_name=self.category_mapping.get(int(class_id), None),\n                        score=float(confidence),\n                        shift_amount=shift_amount,\n                        full_shape=full_shape,\n                    )\n                    object_prediction_list.append(object_prediction)\n\n        object_prediction_list_per_image = [object_prediction_list]\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre>"},{"location":"models/roboflow/#sahi.models.roboflow.RoboflowDetectionModel-functions","title":"Functions","text":""},{"location":"models/roboflow/#sahi.models.roboflow.RoboflowDetectionModel.__init__","title":"<code>__init__(model=None, model_path=None, config_path=None, device=None, mask_threshold=0.5, confidence_threshold=0.3, category_mapping=None, category_remapping=None, load_at_init=True, image_size=None, api_key=None)</code>","text":"<p>Initialize the RoboflowDetectionModel with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> \u00b6 <code>str | None</code> <p>str Path for the instance segmentation model weight</p> <code>None</code> <code>config_path</code> \u00b6 <code>str | None</code> <p>str Path for the mmdetection instance segmentation model config file</p> <code>None</code> <code>device</code> \u00b6 <code>str | None</code> <p>Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.</p> <code>None</code> <code>mask_threshold</code> \u00b6 <code>float</code> <p>float Value to threshold mask pixels, should be between 0 and 1</p> <code>0.5</code> <code>confidence_threshold</code> \u00b6 <code>float</code> <p>float All predictions with score &lt; confidence_threshold will be discarded</p> <code>0.3</code> <code>category_mapping</code> \u00b6 <code>dict | None</code> <p>dict: str to str Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}</p> <code>None</code> <code>category_remapping</code> \u00b6 <code>dict | None</code> <p>dict: str to int Remap category ids based on category names, after performing inference e.g. {\"car\": 3}</p> <code>None</code> <code>load_at_init</code> \u00b6 <code>bool</code> <p>bool If True, automatically loads the model at initialization</p> <code>True</code> <code>image_size</code> \u00b6 <code>int | None</code> <p>int Inference input size.</p> <code>None</code> Source code in <code>sahi/models/roboflow.py</code> <pre><code>def __init__(\n    self,\n    model: Any | None = None,\n    model_path: str | None = None,\n    config_path: str | None = None,\n    device: str | None = None,\n    mask_threshold: float = 0.5,\n    confidence_threshold: float = 0.3,\n    category_mapping: dict | None = None,\n    category_remapping: dict | None = None,\n    load_at_init: bool = True,\n    image_size: int | None = None,\n    api_key: str | None = None,\n):\n    \"\"\"Initialize the RoboflowDetectionModel with the given parameters.\n\n    Args:\n        model_path: str\n            Path for the instance segmentation model weight\n        config_path: str\n            Path for the mmdetection instance segmentation model config file\n        device: Torch device, \"cpu\", \"mps\", \"cuda\", \"cuda:0\", \"cuda:1\", etc.\n        mask_threshold: float\n            Value to threshold mask pixels, should be between 0 and 1\n        confidence_threshold: float\n            All predictions with score &lt; confidence_threshold will be discarded\n        category_mapping: dict: str to str\n            Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n        category_remapping: dict: str to int\n            Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n        load_at_init: bool\n            If True, automatically loads the model at initialization\n        image_size: int\n            Inference input size.\n    \"\"\"\n    self._use_universe = model and isinstance(model, str)\n    self._model = model\n    self._device = device\n    self._api_key = api_key\n\n    if self._use_universe:\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"inference\"]\n    else:\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"rfdetr\"]\n\n    super().__init__(\n        model=model,\n        model_path=model_path,\n        config_path=config_path,\n        device=device,\n        mask_threshold=mask_threshold,\n        confidence_threshold=confidence_threshold,\n        category_mapping=category_mapping,\n        category_remapping=category_remapping,\n        load_at_init=False,\n        image_size=image_size,\n    )\n\n    if load_at_init:\n        self.load_model()\n</code></pre>"},{"location":"models/roboflow/#sahi.models.roboflow.RoboflowDetectionModel.load_model","title":"<code>load_model()</code>","text":"<p>This function should be implemented in a way that detection model should be initialized and set to self.model.</p> <p>(self.model_path, self.config_path, and self.device should be utilized)</p> Source code in <code>sahi/models/roboflow.py</code> <pre><code>def load_model(self):\n    \"\"\"This function should be implemented in a way that detection model should be initialized and set to\n    self.model.\n\n    (self.model_path, self.config_path, and self.device should be utilized)\n    \"\"\"\n    if self._use_universe:\n        from inference import get_model\n        from inference.core.env import API_KEY\n        from inference.core.exceptions import RoboflowAPINotAuthorizedError\n\n        api_key = self._api_key or API_KEY\n\n        try:\n            model = get_model(self._model, api_key=api_key)\n        except RoboflowAPINotAuthorizedError as e:\n            raise ValueError(\n                \"Authorization failed. Please pass a valid API key with \"\n                \"the `api_key` parameter or set the `ROBOFLOW_API_KEY` environment variable.\"\n            ) from e\n\n        assert model.task_type == \"object-detection\", \"Roboflow model must be an object detection model.\"\n\n    else:\n        from rfdetr.detr import RFDETRBase, RFDETRLarge, RFDETRMedium, RFDETRNano, RFDETRSmall\n\n        model, model_path = self._model, self.model_path\n        model_names = (\"RFDETRBase\", \"RFDETRNano\", \"RFDETRSmall\", \"RFDETRMedium\", \"RFDETRLarge\")\n        if hasattr(model, \"__name__\") and model.__name__ in model_names:\n            model_params = dict(\n                resolution=int(self.image_size) if self.image_size else 560,\n                device=self._device,\n                num_classes=len(self.category_mapping.keys()) if self.category_mapping else None,\n            )\n            if model_path:\n                model_params[\"pretrain_weights\"] = model_path\n\n            model = model(**model_params)\n        elif isinstance(model, (RFDETRBase, RFDETRNano, RFDETRSmall, RFDETRMedium, RFDETRLarge)):\n            model = model\n        else:\n            raise ValueError(\n                f\"Model must be a Roboflow model string or one of {model_names} models, got {self.model}.\"\n            )\n\n    self.set_model(model)\n</code></pre>"},{"location":"models/roboflow/#sahi.models.roboflow.RoboflowDetectionModel.perform_inference","title":"<code>perform_inference(image)</code>","text":"<p>This function should be implemented in a way that prediction should be performed using self.model and the prediction result should be set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted.</p> required Source code in <code>sahi/models/roboflow.py</code> <pre><code>def perform_inference(\n    self,\n    image: np.ndarray,\n):\n    \"\"\"This function should be implemented in a way that prediction should be performed using self.model and the\n    prediction result should be set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted.\n    \"\"\"\n    if self._use_universe:\n        self._original_predictions = self.model.infer(image, confidence=self.confidence_threshold)\n    else:\n        self._original_predictions = [self.model.predict(image, threshold=self.confidence_threshold)]\n</code></pre>"},{"location":"models/roboflow/#sahi.models.roboflow.RoboflowDetectionModel.set_model","title":"<code>set_model(model, **kwargs)</code>","text":"<p>This function should be implemented to instantiate a DetectionModel out of an already loaded model Args:     model: Any         Loaded model</p> Source code in <code>sahi/models/roboflow.py</code> <pre><code>def set_model(self, model: Any, **kwargs):\n    \"\"\"\n    This function should be implemented to instantiate a DetectionModel out of an already loaded model\n    Args:\n        model: Any\n            Loaded model\n    \"\"\"\n    self.model = model\n</code></pre>"},{"location":"models/rtdetr/","title":"RTDETRModel","text":""},{"location":"models/rtdetr/#sahi.models.rtdetr","title":"<code>sahi.models.rtdetr</code>","text":""},{"location":"models/rtdetr/#sahi.models.rtdetr-classes","title":"Classes","text":""},{"location":"models/rtdetr/#sahi.models.rtdetr.RTDetrDetectionModel","title":"<code>RTDetrDetectionModel</code>","text":"<p>               Bases: <code>UltralyticsDetectionModel</code></p> Source code in <code>sahi/models/rtdetr.py</code> <pre><code>class RTDetrDetectionModel(UltralyticsDetectionModel):\n    def __init__(self, **kwargs):\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"ultralytics\"]\n        super().__init__(**kwargs)\n\n    def load_model(self):\n        \"\"\"Detection model is initialized and set to self.model.\"\"\"\n        from ultralytics import RTDETR\n\n        try:\n            model_source = self.model_path or \"rtdetr-l.pt\"\n            model = RTDETR(model_source)\n            model.to(self.device)\n            self.set_model(model)\n        except Exception as e:\n            raise TypeError(\"model_path is not a valid rtdet model path: \", e)\n</code></pre>"},{"location":"models/rtdetr/#sahi.models.rtdetr.RTDetrDetectionModel-functions","title":"Functions","text":""},{"location":"models/rtdetr/#sahi.models.rtdetr.RTDetrDetectionModel.load_model","title":"<code>load_model()</code>","text":"<p>Detection model is initialized and set to self.model.</p> Source code in <code>sahi/models/rtdetr.py</code> <pre><code>def load_model(self):\n    \"\"\"Detection model is initialized and set to self.model.\"\"\"\n    from ultralytics import RTDETR\n\n    try:\n        model_source = self.model_path or \"rtdetr-l.pt\"\n        model = RTDETR(model_source)\n        model.to(self.device)\n        self.set_model(model)\n    except Exception as e:\n        raise TypeError(\"model_path is not a valid rtdet model path: \", e)\n</code></pre>"},{"location":"models/torchvision/","title":"TorchvisionModel","text":""},{"location":"models/torchvision/#sahi.models.torchvision","title":"<code>sahi.models.torchvision</code>","text":""},{"location":"models/torchvision/#sahi.models.torchvision-classes","title":"Classes","text":""},{"location":"models/torchvision/#sahi.models.torchvision.TorchVisionDetectionModel","title":"<code>TorchVisionDetectionModel</code>","text":"<p>               Bases: <code>DetectionModel</code></p> Source code in <code>sahi/models/torchvision.py</code> <pre><code>class TorchVisionDetectionModel(DetectionModel):\n    def __init__(self, *args, **kwargs):\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"torch\", \"torchvision\"]\n        super().__init__(*args, **kwargs)\n\n    def load_model(self):\n        import torch\n\n        # read config params\n        model_name = None\n        num_classes = None\n        if self.config_path is not None:\n            with open(self.config_path) as stream:\n                try:\n                    config = yaml.safe_load(stream)\n                except yaml.YAMLError as exc:\n                    raise RuntimeError(exc)\n\n            model_name = config.get(\"model_name\", None)\n            num_classes = config.get(\"num_classes\", None)\n\n        # complete params if not provided in config\n        if not model_name:\n            model_name = \"fasterrcnn_resnet50_fpn\"\n            logger.warning(f\"model_name not provided in config, using default model_type: {model_name}'\")\n        if num_classes is None:\n            logger.warning(\"num_classes not provided in config, using default num_classes: 91\")\n            num_classes = 91\n        if self.model_path is None:\n            logger.warning(\"model_path not provided in config, using pretrained weights and default num_classes: 91.\")\n            weights = \"DEFAULT\"\n            num_classes = 91\n        else:\n            weights = None\n\n        # load model\n        # Note: torchvision &gt;= 0.13 is required for the 'weights' parameter\n        model = MODEL_NAME_TO_CONSTRUCTOR[model_name](num_classes=num_classes, weights=weights)\n        if self.model_path:\n            try:\n                model.load_state_dict(torch.load(self.model_path))\n            except Exception as e:\n                logger.error(f\"Invalid {self.model_path=}\")\n                raise TypeError(\"model_path is not a valid torchvision model path: \", e)\n\n        self.set_model(model)\n\n    def set_model(self, model: Any):\n        \"\"\"Sets the underlying TorchVision model.\n\n        Args:\n            model: Any\n                A TorchVision model\n        \"\"\"\n\n        model.eval()\n        self.model = model.to(self.device)\n\n        # set category_mapping\n\n        if self.category_mapping is None:\n            category_names = {str(i): COCO_CLASSES[i] for i in range(len(COCO_CLASSES))}\n            self.category_mapping = category_names\n\n    def perform_inference(self, image: np.ndarray, image_size: int | None = None):\n        \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n            image_size: int\n                Inference input size.\n        \"\"\"\n        from sahi.utils.torch_utils import to_float_tensor\n\n        # arrange model input size\n        if self.image_size is not None:\n            # get min and max of image height and width\n            min_shape, max_shape = min(image.shape[:2]), max(image.shape[:2])\n            # torchvision resize transform scales the shorter dimension to the target size\n            # we want to scale the longer dimension to the target size\n            image_size = self.image_size * min_shape / max_shape\n            self.model.transform.min_size = (image_size,)  # default is (800,)\n            self.model.transform.max_size = image_size  # default is 1333\n\n        image = to_float_tensor(image)\n        image = image.to(self.device)\n        prediction_result = self.model([image])\n\n        self._original_predictions = prediction_result\n\n    @property\n    def num_categories(self):\n        \"\"\"Returns number of categories.\"\"\"\n        return len(self.category_mapping)\n\n    @property\n    def has_mask(self):\n        \"\"\"Returns if model output contains segmentation mask.\"\"\"\n        return hasattr(self.model, \"roi_heads\") and hasattr(self.model.roi_heads, \"mask_predictor\")\n\n    @property\n    def category_names(self):\n        return list(self.category_mapping.values())\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n        self._object_prediction_list_per_image.\n\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        original_predictions = self._original_predictions\n\n        # compatilibty for sahi v0.8.20\n        if isinstance(shift_amount_list[0], int):\n            shift_amount_list = [shift_amount_list]\n        if full_shape_list is not None and isinstance(full_shape_list[0], int):\n            full_shape_list = [full_shape_list]\n\n        for image_predictions in original_predictions:\n            object_prediction_list_per_image = []\n\n            # get indices of boxes with score &gt; confidence_threshold\n            scores = image_predictions[\"scores\"].cpu().detach().numpy()\n            selected_indices = np.where(scores &gt; self.confidence_threshold)[0]\n\n            # parse boxes, masks, scores, category_ids from predictions\n            category_ids = list(image_predictions[\"labels\"][selected_indices].cpu().detach().numpy())\n            boxes = list(image_predictions[\"boxes\"][selected_indices].cpu().detach().numpy())\n            scores = scores[selected_indices]\n\n            # check if predictions contain mask\n            masks = image_predictions.get(\"masks\", None)\n            if masks is not None:\n                masks = list(\n                    (image_predictions[\"masks\"][selected_indices] &gt; self.mask_threshold).cpu().detach().numpy()\n                )\n            else:\n                masks = None\n\n            # create object_prediction_list\n            object_prediction_list = []\n\n            shift_amount = shift_amount_list[0]\n            full_shape = None if full_shape_list is None else full_shape_list[0]\n\n            for ind in range(len(boxes)):\n                if masks is not None:\n                    segmentation = get_coco_segmentation_from_bool_mask(np.array(masks[ind]))\n                else:\n                    segmentation = None\n\n                object_prediction = ObjectPrediction(\n                    bbox=boxes[ind],\n                    segmentation=segmentation,\n                    category_id=int(category_ids[ind]),\n                    category_name=self.category_mapping[str(int(category_ids[ind]))],\n                    shift_amount=shift_amount,\n                    score=scores[ind],\n                    full_shape=full_shape,\n                )\n                object_prediction_list.append(object_prediction)\n            object_prediction_list_per_image.append(object_prediction_list)\n\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre>"},{"location":"models/torchvision/#sahi.models.torchvision.TorchVisionDetectionModel-attributes","title":"Attributes","text":""},{"location":"models/torchvision/#sahi.models.torchvision.TorchVisionDetectionModel.has_mask","title":"<code>has_mask</code>  <code>property</code>","text":"<p>Returns if model output contains segmentation mask.</p>"},{"location":"models/torchvision/#sahi.models.torchvision.TorchVisionDetectionModel.num_categories","title":"<code>num_categories</code>  <code>property</code>","text":"<p>Returns number of categories.</p>"},{"location":"models/torchvision/#sahi.models.torchvision.TorchVisionDetectionModel-functions","title":"Functions","text":""},{"location":"models/torchvision/#sahi.models.torchvision.TorchVisionDetectionModel.perform_inference","title":"<code>perform_inference(image, image_size=None)</code>","text":"<p>Prediction is performed using self.model and the prediction result is set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.</p> required <code>image_size</code> \u00b6 <code>int | None</code> <p>int Inference input size.</p> <code>None</code> Source code in <code>sahi/models/torchvision.py</code> <pre><code>def perform_inference(self, image: np.ndarray, image_size: int | None = None):\n    \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        image_size: int\n            Inference input size.\n    \"\"\"\n    from sahi.utils.torch_utils import to_float_tensor\n\n    # arrange model input size\n    if self.image_size is not None:\n        # get min and max of image height and width\n        min_shape, max_shape = min(image.shape[:2]), max(image.shape[:2])\n        # torchvision resize transform scales the shorter dimension to the target size\n        # we want to scale the longer dimension to the target size\n        image_size = self.image_size * min_shape / max_shape\n        self.model.transform.min_size = (image_size,)  # default is (800,)\n        self.model.transform.max_size = image_size  # default is 1333\n\n    image = to_float_tensor(image)\n    image = image.to(self.device)\n    prediction_result = self.model([image])\n\n    self._original_predictions = prediction_result\n</code></pre>"},{"location":"models/torchvision/#sahi.models.torchvision.TorchVisionDetectionModel.set_model","title":"<code>set_model(model)</code>","text":"<p>Sets the underlying TorchVision model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> \u00b6 <code>Any</code> <p>Any A TorchVision model</p> required Source code in <code>sahi/models/torchvision.py</code> <pre><code>def set_model(self, model: Any):\n    \"\"\"Sets the underlying TorchVision model.\n\n    Args:\n        model: Any\n            A TorchVision model\n    \"\"\"\n\n    model.eval()\n    self.model = model.to(self.device)\n\n    # set category_mapping\n\n    if self.category_mapping is None:\n        category_names = {str(i): COCO_CLASSES[i] for i in range(len(COCO_CLASSES))}\n        self.category_mapping = category_names\n</code></pre>"},{"location":"models/torchvision/#sahi.models.torchvision-functions","title":"Functions","text":""},{"location":"models/ultralytics/","title":"UltralyticsModel","text":""},{"location":"models/ultralytics/#sahi.models.ultralytics","title":"<code>sahi.models.ultralytics</code>","text":""},{"location":"models/ultralytics/#sahi.models.ultralytics-classes","title":"Classes","text":""},{"location":"models/ultralytics/#sahi.models.ultralytics.UltralyticsDetectionModel","title":"<code>UltralyticsDetectionModel</code>","text":"<p>               Bases: <code>DetectionModel</code></p> <p>Detection model for Ultralytics YOLO models.</p> <p>Supports both PyTorch (.pt) and ONNX (.onnx) models.</p> Source code in <code>sahi/models/ultralytics.py</code> <pre><code>class UltralyticsDetectionModel(DetectionModel):\n    \"\"\"Detection model for Ultralytics YOLO models.\n\n    Supports both PyTorch (.pt) and ONNX (.onnx) models.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.fuse: bool = kwargs.pop(\"fuse\", False)\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"ultralytics\"]\n        super().__init__(*args, **kwargs)\n\n    def load_model(self):\n        \"\"\"Detection model is initialized and set to self.model.\n\n        Supports both PyTorch (.pt) and ONNX (.onnx) models.\n        \"\"\"\n\n        from ultralytics import YOLO\n\n        if self.model_path and \".onnx\" in self.model_path:\n            check_requirements([\"onnx\", \"onnxruntime\"])\n\n        try:\n            model = YOLO(self.model_path)\n            # Only call .to(device) for PyTorch models, not ONNX\n            if self.model_path and not self.model_path.endswith(\".onnx\"):\n                model.to(self.device)\n            self.set_model(model)\n            if self.fuse and hasattr(model, \"fuse\"):\n                model.fuse()\n\n        except Exception as e:\n            raise TypeError(\"model_path is not a valid Ultralytics model path: \", e)\n\n    def set_model(self, model: Any, **kwargs):\n        \"\"\"Sets the underlying Ultralytics model.\n\n        Args:\n            model: Any\n                A Ultralytics model\n        \"\"\"\n\n        self.model = model\n        # set category_mapping\n        if not self.category_mapping:\n            category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n            self.category_mapping = category_mapping\n\n    def perform_inference(self, image: np.ndarray):\n        \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        \"\"\"\n\n        # Confirm model is loaded\n\n        import torch\n\n        if self.model is None:\n            raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n\n        kwargs = {\"cfg\": self.config_path, \"verbose\": False, \"conf\": self.confidence_threshold, \"device\": self.device}\n\n        if self.image_size is not None:\n            kwargs = {\"imgsz\": self.image_size, **kwargs}\n\n        prediction_result = self.model(image[:, :, ::-1], **kwargs)  # YOLO expects numpy arrays to have BGR\n\n        # Handle different result types for PyTorch vs ONNX models\n        # ONNX models might return results in a different format\n        if self.has_mask:\n            from ultralytics.engine.results import Masks\n\n            if not prediction_result[0].masks:\n                # Create empty masks if none exist\n                if hasattr(self.model, \"device\"):\n                    device = self.model.device\n                else:\n                    device = \"cpu\"  # Default for ONNX models\n                prediction_result[0].masks = Masks(\n                    torch.tensor([], device=device), prediction_result[0].boxes.orig_shape\n                )\n\n            # We do not filter results again as confidence threshold is already applied above\n            prediction_result = [\n                (\n                    result.boxes.data,\n                    result.masks.data,\n                )\n                for result in prediction_result\n            ]\n        elif self.is_obb:\n            # For OBB task, get OBB points in xyxyxyxy format\n            device = getattr(self.model, \"device\", \"cpu\")\n            prediction_result = [\n                (\n                    # Get OBB data: xyxy, conf, cls\n                    torch.cat(\n                        [\n                            result.obb.xyxy,  # box coordinates\n                            result.obb.conf.unsqueeze(-1),  # confidence scores\n                            result.obb.cls.unsqueeze(-1),  # class ids\n                        ],\n                        dim=1,\n                    )\n                    if result.obb is not None\n                    else torch.empty((0, 6), device=device),\n                    # Get OBB points in (N, 4, 2) format\n                    result.obb.xyxyxyxy if result.obb is not None else torch.empty((0, 4, 2), device=device),\n                )\n                for result in prediction_result\n            ]\n        else:  # If model doesn't do segmentation or OBB then no need to check masks\n            # We do not filter results again as confidence threshold is already applied above\n            prediction_result = [result.boxes.data for result in prediction_result]\n\n        self._original_predictions = prediction_result\n        self._original_shape = image.shape\n\n    @property\n    def category_names(self):\n        # For ONNX models, names might not be available, use category_mapping\n        if hasattr(self.model, \"names\") and self.model.names:\n            return self.model.names.values()\n        elif self.category_mapping:\n            return list(self.category_mapping.values())\n        else:\n            raise ValueError(\"Category names not available. Please provide category_mapping for ONNX models.\")\n\n    @property\n    def num_categories(self):\n        \"\"\"Returns number of categories.\"\"\"\n        if hasattr(self.model, \"names\") and self.model.names:\n            return len(self.model.names)\n        elif self.category_mapping:\n            return len(self.category_mapping)\n        else:\n            raise ValueError(\"Cannot determine number of categories. Please provide category_mapping for ONNX models.\")\n\n    @property\n    def has_mask(self):\n        \"\"\"Returns if model output contains segmentation mask.\"\"\"\n        # Check if model has 'task' attribute (for both .pt and .onnx models)\n        if hasattr(self.model, \"overrides\") and \"task\" in self.model.overrides:\n            return self.model.overrides[\"task\"] == \"segment\"\n        # For ONNX models, task might be stored differently\n        elif hasattr(self.model, \"task\"):\n            return self.model.task == \"segment\"\n        # For ONNX models without task info, check model path\n        elif self.model_path and isinstance(self.model_path, str):\n            return \"seg\" in self.model_path.lower()\n        return False\n\n    @property\n    def is_obb(self):\n        \"\"\"Returns if model output contains oriented bounding boxes.\"\"\"\n        # Check if model has 'task' attribute (for both .pt and .onnx models)\n        if hasattr(self.model, \"overrides\") and \"task\" in self.model.overrides:\n            return self.model.overrides[\"task\"] == \"obb\"\n        # For ONNX models, task might be stored differently\n        elif hasattr(self.model, \"task\"):\n            return self.model.task == \"obb\"\n        # For ONNX models without task info, check model path\n        elif self.model_path and isinstance(self.model_path, str):\n            return \"obb\" in self.model_path.lower()\n        return False\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n        self._object_prediction_list_per_image.\n\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        original_predictions = self._original_predictions\n\n        # compatibility for sahi v0.8.15\n        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n        full_shape_list = fix_full_shape_list(full_shape_list)\n\n        # handle all predictions\n        object_prediction_list_per_image = []\n\n        for image_ind, image_predictions in enumerate(original_predictions):\n            shift_amount = shift_amount_list[image_ind]\n            full_shape = None if full_shape_list is None else full_shape_list[image_ind]\n            object_prediction_list = []\n\n            # Extract boxes and optional masks/obb\n            if self.has_mask or self.is_obb:\n                boxes = image_predictions[0].cpu().detach().numpy()\n                masks_or_points = image_predictions[1].cpu().detach().numpy()\n            else:\n                boxes = image_predictions.data.cpu().detach().numpy()\n                masks_or_points = None\n\n            # Process each prediction\n            for pred_ind, prediction in enumerate(boxes):\n                # Get bbox coordinates\n                bbox = prediction[:4].tolist()\n                score = prediction[4]\n                category_id = int(prediction[5])\n                category_name = self.category_mapping[str(category_id)]\n\n                # Fix box coordinates\n                bbox = [max(0, coord) for coord in bbox]\n                if full_shape is not None:\n                    bbox[0] = min(full_shape[1], bbox[0])\n                    bbox[1] = min(full_shape[0], bbox[1])\n                    bbox[2] = min(full_shape[1], bbox[2])\n                    bbox[3] = min(full_shape[0], bbox[3])\n\n                # Ignore invalid predictions\n                if not (bbox[0] &lt; bbox[2]) or not (bbox[1] &lt; bbox[3]):\n                    logger.warning(f\"ignoring invalid prediction with bbox: {bbox}\")\n                    continue\n\n                # Get segmentation or OBB points\n                segmentation = None\n                if masks_or_points is not None:\n                    if self.has_mask:\n                        bool_mask = masks_or_points[pred_ind]\n                        # Resize mask to original image size\n                        bool_mask = cv2.resize(\n                            bool_mask.astype(np.uint8), (self._original_shape[1], self._original_shape[0])\n                        )\n                        segmentation = get_coco_segmentation_from_bool_mask(bool_mask)\n                    else:  # is_obb\n                        obb_points = masks_or_points[pred_ind]  # Get OBB points for this prediction\n                        segmentation = [obb_points.reshape(-1).tolist()]\n\n                    if len(segmentation) == 0:\n                        continue\n\n                # Create and append object prediction\n                object_prediction = ObjectPrediction(\n                    bbox=bbox,\n                    category_id=category_id,\n                    score=score,\n                    segmentation=segmentation,\n                    category_name=category_name,\n                    shift_amount=shift_amount,\n                    full_shape=self._original_shape[:2] if full_shape is None else full_shape,  # (height, width)\n                )\n                object_prediction_list.append(object_prediction)\n\n            object_prediction_list_per_image.append(object_prediction_list)\n\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre>"},{"location":"models/ultralytics/#sahi.models.ultralytics.UltralyticsDetectionModel-attributes","title":"Attributes","text":""},{"location":"models/ultralytics/#sahi.models.ultralytics.UltralyticsDetectionModel.has_mask","title":"<code>has_mask</code>  <code>property</code>","text":"<p>Returns if model output contains segmentation mask.</p>"},{"location":"models/ultralytics/#sahi.models.ultralytics.UltralyticsDetectionModel.is_obb","title":"<code>is_obb</code>  <code>property</code>","text":"<p>Returns if model output contains oriented bounding boxes.</p>"},{"location":"models/ultralytics/#sahi.models.ultralytics.UltralyticsDetectionModel.num_categories","title":"<code>num_categories</code>  <code>property</code>","text":"<p>Returns number of categories.</p>"},{"location":"models/ultralytics/#sahi.models.ultralytics.UltralyticsDetectionModel-functions","title":"Functions","text":""},{"location":"models/ultralytics/#sahi.models.ultralytics.UltralyticsDetectionModel.load_model","title":"<code>load_model()</code>","text":"<p>Detection model is initialized and set to self.model.</p> <p>Supports both PyTorch (.pt) and ONNX (.onnx) models.</p> Source code in <code>sahi/models/ultralytics.py</code> <pre><code>def load_model(self):\n    \"\"\"Detection model is initialized and set to self.model.\n\n    Supports both PyTorch (.pt) and ONNX (.onnx) models.\n    \"\"\"\n\n    from ultralytics import YOLO\n\n    if self.model_path and \".onnx\" in self.model_path:\n        check_requirements([\"onnx\", \"onnxruntime\"])\n\n    try:\n        model = YOLO(self.model_path)\n        # Only call .to(device) for PyTorch models, not ONNX\n        if self.model_path and not self.model_path.endswith(\".onnx\"):\n            model.to(self.device)\n        self.set_model(model)\n        if self.fuse and hasattr(model, \"fuse\"):\n            model.fuse()\n\n    except Exception as e:\n        raise TypeError(\"model_path is not a valid Ultralytics model path: \", e)\n</code></pre>"},{"location":"models/ultralytics/#sahi.models.ultralytics.UltralyticsDetectionModel.perform_inference","title":"<code>perform_inference(image)</code>","text":"<p>Prediction is performed using self.model and the prediction result is set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.</p> required Source code in <code>sahi/models/ultralytics.py</code> <pre><code>def perform_inference(self, image: np.ndarray):\n    \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n    \"\"\"\n\n    # Confirm model is loaded\n\n    import torch\n\n    if self.model is None:\n        raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n\n    kwargs = {\"cfg\": self.config_path, \"verbose\": False, \"conf\": self.confidence_threshold, \"device\": self.device}\n\n    if self.image_size is not None:\n        kwargs = {\"imgsz\": self.image_size, **kwargs}\n\n    prediction_result = self.model(image[:, :, ::-1], **kwargs)  # YOLO expects numpy arrays to have BGR\n\n    # Handle different result types for PyTorch vs ONNX models\n    # ONNX models might return results in a different format\n    if self.has_mask:\n        from ultralytics.engine.results import Masks\n\n        if not prediction_result[0].masks:\n            # Create empty masks if none exist\n            if hasattr(self.model, \"device\"):\n                device = self.model.device\n            else:\n                device = \"cpu\"  # Default for ONNX models\n            prediction_result[0].masks = Masks(\n                torch.tensor([], device=device), prediction_result[0].boxes.orig_shape\n            )\n\n        # We do not filter results again as confidence threshold is already applied above\n        prediction_result = [\n            (\n                result.boxes.data,\n                result.masks.data,\n            )\n            for result in prediction_result\n        ]\n    elif self.is_obb:\n        # For OBB task, get OBB points in xyxyxyxy format\n        device = getattr(self.model, \"device\", \"cpu\")\n        prediction_result = [\n            (\n                # Get OBB data: xyxy, conf, cls\n                torch.cat(\n                    [\n                        result.obb.xyxy,  # box coordinates\n                        result.obb.conf.unsqueeze(-1),  # confidence scores\n                        result.obb.cls.unsqueeze(-1),  # class ids\n                    ],\n                    dim=1,\n                )\n                if result.obb is not None\n                else torch.empty((0, 6), device=device),\n                # Get OBB points in (N, 4, 2) format\n                result.obb.xyxyxyxy if result.obb is not None else torch.empty((0, 4, 2), device=device),\n            )\n            for result in prediction_result\n        ]\n    else:  # If model doesn't do segmentation or OBB then no need to check masks\n        # We do not filter results again as confidence threshold is already applied above\n        prediction_result = [result.boxes.data for result in prediction_result]\n\n    self._original_predictions = prediction_result\n    self._original_shape = image.shape\n</code></pre>"},{"location":"models/ultralytics/#sahi.models.ultralytics.UltralyticsDetectionModel.set_model","title":"<code>set_model(model, **kwargs)</code>","text":"<p>Sets the underlying Ultralytics model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> \u00b6 <code>Any</code> <p>Any A Ultralytics model</p> required Source code in <code>sahi/models/ultralytics.py</code> <pre><code>def set_model(self, model: Any, **kwargs):\n    \"\"\"Sets the underlying Ultralytics model.\n\n    Args:\n        model: Any\n            A Ultralytics model\n    \"\"\"\n\n    self.model = model\n    # set category_mapping\n    if not self.category_mapping:\n        category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n        self.category_mapping = category_mapping\n</code></pre>"},{"location":"models/ultralytics/#sahi.models.ultralytics-functions","title":"Functions","text":""},{"location":"models/yolov5/","title":"Yolov5Model","text":""},{"location":"models/yolov5/#sahi.models.yolov5","title":"<code>sahi.models.yolov5</code>","text":""},{"location":"models/yolov5/#sahi.models.yolov5-classes","title":"Classes","text":""},{"location":"models/yolov5/#sahi.models.yolov5.Yolov5DetectionModel","title":"<code>Yolov5DetectionModel</code>","text":"<p>               Bases: <code>DetectionModel</code></p> Source code in <code>sahi/models/yolov5.py</code> <pre><code>class Yolov5DetectionModel(DetectionModel):\n    def __init__(self, *args, **kwargs):\n        existing_packages = getattr(self, \"required_packages\", None) or []\n        self.required_packages = [*list(existing_packages), \"yolov5\", \"torch\"]\n        super().__init__(*args, **kwargs)\n\n    def load_model(self):\n        \"\"\"Detection model is initialized and set to self.model.\"\"\"\n        import yolov5\n\n        try:\n            model = yolov5.load(self.model_path, device=self.device)\n            self.set_model(model)\n        except Exception as e:\n            raise TypeError(\"model_path is not a valid yolov5 model path: \", e)\n\n    def set_model(self, model: Any):\n        \"\"\"Sets the underlying YOLOv5 model.\n\n        Args:\n            model: Any\n                A YOLOv5 model\n        \"\"\"\n\n        if model.__class__.__module__ not in [\"yolov5.models.common\", \"models.common\"]:\n            raise Exception(f\"Not a yolov5 model: {type(model)}\")\n\n        model.conf = self.confidence_threshold\n        self.model = model\n\n        # set category_mapping\n        if not self.category_mapping:\n            category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n            self.category_mapping = category_mapping\n\n    def perform_inference(self, image: np.ndarray):\n        \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n        Args:\n            image: np.ndarray\n                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n        \"\"\"\n\n        # Confirm model is loaded\n        if self.model is None:\n            raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n        if self.image_size is not None:\n            prediction_result = self.model(image, size=self.image_size)\n        else:\n            prediction_result = self.model(image)\n\n        self._original_predictions = prediction_result\n\n    @property\n    def num_categories(self):\n        \"\"\"Returns number of categories.\"\"\"\n        return len(self.model.names)\n\n    @property\n    def has_mask(self):\n        \"\"\"Returns if model output contains segmentation mask.\"\"\"\n\n        return False  # fix when yolov5 supports segmentation models\n\n    @property\n    def category_names(self):\n        if check_package_minimum_version(\"yolov5\", \"6.2.0\"):\n            return list(self.model.names.values())\n        else:\n            return self.model.names\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: list[list[int]] | None = [[0, 0]],\n        full_shape_list: list[list[int]] | None = None,\n    ):\n        \"\"\"self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n        self._object_prediction_list_per_image.\n\n        Args:\n            shift_amount_list: list of list\n                To shift the box and mask predictions from sliced image to full sized image, should\n                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n            full_shape_list: list of list\n                Size of the full image after shifting, should be in the form of\n                List[[height, width],[height, width],...]\n        \"\"\"\n        original_predictions = self._original_predictions\n\n        # compatilibty for sahi v0.8.15\n        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n        full_shape_list = fix_full_shape_list(full_shape_list)\n\n        # handle all predictions\n        object_prediction_list_per_image = []\n        for image_ind, image_predictions_in_xyxy_format in enumerate(original_predictions.xyxy):\n            shift_amount = shift_amount_list[image_ind]\n            full_shape = None if full_shape_list is None else full_shape_list[image_ind]\n            object_prediction_list = []\n\n            # process predictions\n            for prediction in image_predictions_in_xyxy_format.cpu().detach().numpy():\n                x1 = prediction[0]\n                y1 = prediction[1]\n                x2 = prediction[2]\n                y2 = prediction[3]\n                bbox = [x1, y1, x2, y2]\n                score = prediction[4]\n                category_id = int(prediction[5])\n                category_name = self.category_mapping[str(category_id)]\n\n                # fix negative box coords\n                bbox[0] = max(0, bbox[0])\n                bbox[1] = max(0, bbox[1])\n                bbox[2] = max(0, bbox[2])\n                bbox[3] = max(0, bbox[3])\n\n                # fix out of image box coords\n                if full_shape is not None:\n                    bbox[0] = min(full_shape[1], bbox[0])\n                    bbox[1] = min(full_shape[0], bbox[1])\n                    bbox[2] = min(full_shape[1], bbox[2])\n                    bbox[3] = min(full_shape[0], bbox[3])\n\n                # ignore invalid predictions\n                if not (bbox[0] &lt; bbox[2]) or not (bbox[1] &lt; bbox[3]):\n                    logger.warning(f\"ignoring invalid prediction with bbox: {bbox}\")\n                    continue\n\n                object_prediction = ObjectPrediction(\n                    bbox=bbox,\n                    category_id=category_id,\n                    score=score,\n                    segmentation=None,\n                    category_name=category_name,\n                    shift_amount=shift_amount,\n                    full_shape=full_shape,\n                )\n                object_prediction_list.append(object_prediction)\n            object_prediction_list_per_image.append(object_prediction_list)\n\n        self._object_prediction_list_per_image = object_prediction_list_per_image\n</code></pre>"},{"location":"models/yolov5/#sahi.models.yolov5.Yolov5DetectionModel-attributes","title":"Attributes","text":""},{"location":"models/yolov5/#sahi.models.yolov5.Yolov5DetectionModel.has_mask","title":"<code>has_mask</code>  <code>property</code>","text":"<p>Returns if model output contains segmentation mask.</p>"},{"location":"models/yolov5/#sahi.models.yolov5.Yolov5DetectionModel.num_categories","title":"<code>num_categories</code>  <code>property</code>","text":"<p>Returns number of categories.</p>"},{"location":"models/yolov5/#sahi.models.yolov5.Yolov5DetectionModel-functions","title":"Functions","text":""},{"location":"models/yolov5/#sahi.models.yolov5.Yolov5DetectionModel.load_model","title":"<code>load_model()</code>","text":"<p>Detection model is initialized and set to self.model.</p> Source code in <code>sahi/models/yolov5.py</code> <pre><code>def load_model(self):\n    \"\"\"Detection model is initialized and set to self.model.\"\"\"\n    import yolov5\n\n    try:\n        model = yolov5.load(self.model_path, device=self.device)\n        self.set_model(model)\n    except Exception as e:\n        raise TypeError(\"model_path is not a valid yolov5 model path: \", e)\n</code></pre>"},{"location":"models/yolov5/#sahi.models.yolov5.Yolov5DetectionModel.perform_inference","title":"<code>perform_inference(image)</code>","text":"<p>Prediction is performed using self.model and the prediction result is set to self._original_predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <code>ndarray</code> <p>np.ndarray A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.</p> required Source code in <code>sahi/models/yolov5.py</code> <pre><code>def perform_inference(self, image: np.ndarray):\n    \"\"\"Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n\n    Args:\n        image: np.ndarray\n            A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n    \"\"\"\n\n    # Confirm model is loaded\n    if self.model is None:\n        raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n    if self.image_size is not None:\n        prediction_result = self.model(image, size=self.image_size)\n    else:\n        prediction_result = self.model(image)\n\n    self._original_predictions = prediction_result\n</code></pre>"},{"location":"models/yolov5/#sahi.models.yolov5.Yolov5DetectionModel.set_model","title":"<code>set_model(model)</code>","text":"<p>Sets the underlying YOLOv5 model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> \u00b6 <code>Any</code> <p>Any A YOLOv5 model</p> required Source code in <code>sahi/models/yolov5.py</code> <pre><code>def set_model(self, model: Any):\n    \"\"\"Sets the underlying YOLOv5 model.\n\n    Args:\n        model: Any\n            A YOLOv5 model\n    \"\"\"\n\n    if model.__class__.__module__ not in [\"yolov5.models.common\", \"models.common\"]:\n        raise Exception(f\"Not a yolov5 model: {type(model)}\")\n\n    model.conf = self.confidence_threshold\n    self.model = model\n\n    # set category_mapping\n    if not self.category_mapping:\n        category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n        self.category_mapping = category_mapping\n</code></pre>"},{"location":"models/yolov5/#sahi.models.yolov5-functions","title":"Functions","text":""},{"location":"notebooks/inference_for_detectron2/","title":"SAHI with Detectron2 for Sliced Inference","text":"<ul> <li>Install latest version of SAHI and Detectron2:</li> </ul> In\u00a0[\u00a0]: Copied! <pre># flake8: noqa: E501\n!pip install -U sahi\n!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.10/index.html # for Detectron2-cpu\n#!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html # for Detectron2-cuda11.1\n</pre> # flake8: noqa: E501 !pip install -U sahi !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.10/index.html # for Detectron2-cpu #!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html # for Detectron2-cuda11.1 In\u00a0[1]: Copied! <pre>import os\n\nos.getcwd()\n</pre> import os  os.getcwd() Out[1]: <pre>'/home/fatihakyon/dev/obss/sahi/demo'</pre> <ul> <li>Import required modules:</li> </ul> In\u00a0[2]: Copied! <pre># will be used for detectron2 fasterrcnn model zoo name\nfrom IPython.display import Image\n\n# import required functions, classes\nfrom sahi import AutoDetectionModel\nfrom sahi.predict import get_prediction, get_sliced_prediction, predict\nfrom sahi.utils.cv import read_image\nfrom sahi.utils.detectron2 import Detectron2TestConstants\nfrom sahi.utils.file import download_from_url\n</pre> # will be used for detectron2 fasterrcnn model zoo name from IPython.display import Image  # import required functions, classes from sahi import AutoDetectionModel from sahi.predict import get_prediction, get_sliced_prediction, predict from sahi.utils.cv import read_image from sahi.utils.detectron2 import Detectron2TestConstants from sahi.utils.file import download_from_url In\u00a0[3]: Copied! <pre># set detectron2 fasterrcnn model zoo name\nmodel_path = Detectron2TestConstants.FASTERCNN_MODEL_ZOO_NAME\n\n# download test images into demo_data folder\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",\n    \"demo_data/small-vehicles1.jpeg\",\n)\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\"\n)\n</pre> # set detectron2 fasterrcnn model zoo name model_path = Detectron2TestConstants.FASTERCNN_MODEL_ZOO_NAME  # download test images into demo_data folder download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",     \"demo_data/small-vehicles1.jpeg\", ) download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\" ) <ul> <li>Instantiate a detection model by defining model weight path, config path and other parameters:</li> </ul> In\u00a0[4]: Copied! <pre>detection_model = AutoDetectionModel.from_pretrained(\n    model_type=\"detectron2\",\n    model_path=model_path,\n    config_path=model_path,\n    confidence_threshold=0.5,\n    image_size=640,\n    device=\"cpu\",  # or 'cuda:0'\n)\n</pre> detection_model = AutoDetectionModel.from_pretrained(     model_type=\"detectron2\",     model_path=model_path,     config_path=model_path,     confidence_threshold=0.5,     image_size=640,     device=\"cpu\",  # or 'cuda:0' ) <pre>09/27/2022 17:44:09 - INFO - fvcore.common.checkpoint -   [Checkpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n09/27/2022 17:44:09 - INFO - iopath.common.file_io -   URL https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl cached in /home/fatihakyon/.torch/iopath_cache/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\n09/27/2022 17:44:09 - INFO - fvcore.common.checkpoint -   Reading a file from 'Detectron2 Model Zoo'\n</pre> <ul> <li>Perform prediction by feeding the <code>get_prediction</code> function with an image path and a DetectionModel instance:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model)\n</pre> result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model) <ul> <li>Or perform prediction by feeding the <code>get_prediction</code> function with a numpy image and a DetectionModel instance:</li> </ul> In\u00a0[6]: Copied! <pre>result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model)\n</pre> result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model) <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[7]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[7]: <ul> <li>To perform sliced prediction we need to specify slice parameters. In this example we will perform prediction over slices of 256x256 with an overlap ratio of 0.2:</li> </ul> In\u00a0[8]: Copied! <pre>result = get_sliced_prediction(\n    \"demo_data/small-vehicles1.jpeg\",\n    detection_model,\n    slice_height=256,\n    slice_width=256,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n</pre> result = get_sliced_prediction(     \"demo_data/small-vehicles1.jpeg\",     detection_model,     slice_height=256,     slice_width=256,     overlap_height_ratio=0.2,     overlap_width_ratio=0.2, ) <pre>Performing prediction on 15 number of slices.\n</pre> <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[9]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[9]: <ul> <li>Predictions are returned as sahi.prediction.PredictionResult, you can access the object prediction list as:</li> </ul> In\u00a0[10]: Copied! <pre>object_prediction_list = result.object_prediction_list\n</pre> object_prediction_list = result.object_prediction_list In\u00a0[11]: Copied! <pre>object_prediction_list[0]\n</pre> object_prediction_list[0] Out[11]: <pre>ObjectPrediction&lt;\n    bbox: BoundingBox: &lt;(656, 197, 671, 215), w: 15, h: 18&gt;,\n    mask: None,\n    score: PredictionScore: &lt;value: 0.9950496554374695&gt;,\n    category: Category: &lt;id: 2, name: car&gt;&gt;</pre> In\u00a0[9]: Copied! <pre>result.to_coco_annotations()[:3]\n</pre> result.to_coco_annotations()[:3] Out[9]: <pre>[{'image_id': None,\n  'bbox': [656, 197, 15, 18],\n  'score': 0.9950494170188904,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 270},\n {'image_id': None,\n  'bbox': [446, 308, 49, 34],\n  'score': 0.9942395687103271,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1666},\n {'image_id': None,\n  'bbox': [759, 231, 22, 18],\n  'score': 0.9921348094940186,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 396}]</pre> <ul> <li>ObjectPrediction's can be converted to COCO prediction format:</li> </ul> In\u00a0[12]: Copied! <pre>result.to_coco_predictions(image_id=1)[:3]\n</pre> result.to_coco_predictions(image_id=1)[:3] Out[12]: <pre>[{'image_id': 1,\n  'bbox': [656, 197, 15, 18],\n  'score': 0.9950496554374695,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 270},\n {'image_id': 1,\n  'bbox': [446, 308, 49, 34],\n  'score': 0.9942396879196167,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1666},\n {'image_id': 1,\n  'bbox': [759, 231, 22, 18],\n  'score': 0.9921349287033081,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 396}]</pre> <ul> <li>ObjectPrediction's can be converted to imantics annotation format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!pip install -U imantics\n</pre> !pip install -U imantics In\u00a0[13]: Copied! <pre>result.to_imantics_annotations()[:3]\n</pre> result.to_imantics_annotations()[:3] Out[13]: <pre>[&lt;imantics.annotation.Annotation at 0x7fce3bab95e0&gt;,\n &lt;imantics.annotation.Annotation at 0x7fce30371880&gt;,\n &lt;imantics.annotation.Annotation at 0x7fce30371f40&gt;]</pre> <ul> <li>Set model and directory parameters:</li> </ul> In\u00a0[14]: Copied! <pre>model_type = \"detectron2\"\nmodel_path = model_path\nmodel_config_path = model_path\nmodel_device = \"cpu\"  # or 'cuda:0'\nmodel_confidence_threshold = 0.5\n\nslice_height = 480\nslice_width = 480\noverlap_height_ratio = 0.2\noverlap_width_ratio = 0.2\n\nsource_image_dir = \"demo_data/\"\n</pre> model_type = \"detectron2\" model_path = model_path model_config_path = model_path model_device = \"cpu\"  # or 'cuda:0' model_confidence_threshold = 0.5  slice_height = 480 slice_width = 480 overlap_height_ratio = 0.2 overlap_width_ratio = 0.2  source_image_dir = \"demo_data/\" <ul> <li>Perform sliced inference on given folder:</li> </ul> In\u00a0[15]: Copied! <pre>predict(\n    model_type=model_type,\n    model_path=model_path,\n    model_config_path=model_path,\n    model_device=model_device,\n    model_confidence_threshold=model_confidence_threshold,\n    source=source_image_dir,\n    slice_height=slice_height,\n    slice_width=slice_width,\n    overlap_height_ratio=overlap_height_ratio,\n    overlap_width_ratio=overlap_width_ratio,\n)\n</pre> predict(     model_type=model_type,     model_path=model_path,     model_config_path=model_path,     model_device=model_device,     model_confidence_threshold=model_confidence_threshold,     source=source_image_dir,     slice_height=slice_height,     slice_width=slice_width,     overlap_height_ratio=overlap_height_ratio,     overlap_width_ratio=overlap_width_ratio, ) <pre>There are 3 listed files in folder: demo_data/\n</pre> <pre>09/27/2022 17:45:01 - INFO - fvcore.common.checkpoint -   [Checkpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n09/27/2022 17:45:01 - INFO - fvcore.common.checkpoint -   Reading a file from 'Detectron2 Model Zoo'\nPerforming inference on images:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Performing prediction on 6 number of slices.\n</pre> <pre>Performing inference on images:  33%|\u2588\u2588\u2588\u258e      | 1/3 [00:01&lt;00:03,  1.66s/it]</pre> <pre>Prediction time is: 1628.41 ms\nPerforming prediction on 6 number of slices.\n</pre> <pre>Performing inference on images:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:02&lt;00:01,  1.14s/it]</pre> <pre>Prediction time is: 728.87 ms\nPerforming prediction on 6 number of slices.\n</pre> <pre>Performing inference on images: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:03&lt;00:00,  1.09s/it]</pre> <pre>Prediction time is: 762.43 ms\nPrediction results are successfully exported to runs/predict/exp18\n</pre> <pre>\n</pre>"},{"location":"notebooks/inference_for_detectron2/#sahi-with-detectron2-for-sliced-inference","title":"SAHI with Detectron2 for Sliced Inference\u00b6","text":""},{"location":"notebooks/inference_for_detectron2/#0-preparation","title":"0. Preparation\u00b6","text":""},{"location":"notebooks/inference_for_detectron2/#1-standard-inference-with-a-detectron2-model","title":"1. Standard Inference with a Detectron2 Model\u00b6","text":""},{"location":"notebooks/inference_for_detectron2/#2-sliced-inference-with-a-detectron2-model","title":"2. Sliced Inference with a Detectron2 Model\u00b6","text":""},{"location":"notebooks/inference_for_detectron2/#3-prediction-result","title":"3. Prediction Result\u00b6","text":""},{"location":"notebooks/inference_for_detectron2/#4-batch-prediction","title":"4. Batch Prediction\u00b6","text":""},{"location":"notebooks/inference_for_huggingface/","title":"SAHI with HuggingFace Transformer for Sliced Inference","text":"<ul> <li>Install latest version of SAHI and HuggingFace transformers:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!pip install -U sahi\n!pip install transformers timm\n</pre> !pip install -U sahi !pip install transformers timm In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.getcwd()\n</pre> import os  os.getcwd() <ul> <li>Import required modules:</li> </ul> In\u00a0[1]: Copied! <pre># import required functions, classes\nimport os\n\nfrom IPython.display import Image\n\nfrom sahi import AutoDetectionModel\nfrom sahi.predict import get_prediction, get_sliced_prediction, predict\nfrom sahi.utils.cv import read_image\nfrom sahi.utils.file import download_from_url\n</pre> # import required functions, classes import os  from IPython.display import Image  from sahi import AutoDetectionModel from sahi.predict import get_prediction, get_sliced_prediction, predict from sahi.utils.cv import read_image from sahi.utils.file import download_from_url <p>You can see object detection models available at HF model hub. We use smallest variant of RT-DETR v2 model for this demo.</p> In\u00a0[5]: Copied! <pre># Select a model to use, we use a RD-DETRv2 model.\nmodel_path = \"PekingU/rtdetr_v2_r18vd\"  # larger models: PekingU/rtdetr_v2_r50vd, PekingU/rtdetr_v2_r101vd\nos.environ[\"HF_TOKEN\"] = \"hf_...\"  # HF requires a token to download public models (no idea why)\n\n# download test images into demo_data folder\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",\n    \"demo_data/small-vehicles1.jpeg\",\n)\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\"\n)\n</pre> # Select a model to use, we use a RD-DETRv2 model. model_path = \"PekingU/rtdetr_v2_r18vd\"  # larger models: PekingU/rtdetr_v2_r50vd, PekingU/rtdetr_v2_r101vd os.environ[\"HF_TOKEN\"] = \"hf_...\"  # HF requires a token to download public models (no idea why)  # download test images into demo_data folder download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",     \"demo_data/small-vehicles1.jpeg\", ) download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\" ) <ul> <li>Instantiate a detection model by defining model weight path, config path and other parameters:</li> </ul> In\u00a0[9]: Copied! <pre>detection_model = AutoDetectionModel.from_pretrained(\n    model_type=\"huggingface\",\n    model_path=model_path,\n    confidence_threshold=0.5,\n    image_size=640,\n    device=\"cpu\",  # or 'cuda'\n)\n</pre> detection_model = AutoDetectionModel.from_pretrained(     model_type=\"huggingface\",     model_path=model_path,     confidence_threshold=0.5,     image_size=640,     device=\"cpu\",  # or 'cuda' ) <ul> <li>Perform prediction by feeding the <code>get_prediction</code> function with an image path and a DetectionModel instance:</li> </ul> In\u00a0[10]: Copied! <pre>result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model)\n</pre> result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model) <ul> <li>Or perform prediction by feeding the <code>get_prediction</code> function with a numpy image and a DetectionModel instance:</li> </ul> In\u00a0[5]: Copied! <pre>result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model)\n</pre> result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model) <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[11]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[11]: <ul> <li>To perform sliced prediction we need to specify slice parameters. In this example we will perform prediction over slices of 512x512 with an overlap ratio of 0.2:</li> </ul> In\u00a0[12]: Copied! <pre>result = get_sliced_prediction(\n    \"demo_data/small-vehicles1.jpeg\",\n    detection_model,\n    slice_height=512,\n    slice_width=512,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n</pre> result = get_sliced_prediction(     \"demo_data/small-vehicles1.jpeg\",     detection_model,     slice_height=512,     slice_width=512,     overlap_height_ratio=0.2,     overlap_width_ratio=0.2, ) <pre>Performing prediction on 6 slices.\n</pre> <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[13]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[13]: <ul> <li>Predictions are returned as sahi.prediction.PredictionResult, you can access the object prediction list as:</li> </ul> In\u00a0[15]: Copied! <pre>object_prediction_list = result.object_prediction_list\n</pre> object_prediction_list = result.object_prediction_list In\u00a0[16]: Copied! <pre>object_prediction_list[0]\n</pre> object_prediction_list[0] Out[16]: <pre>ObjectPrediction&lt;\n    bbox: BoundingBox: &lt;(321, 322, 383, 364), w: 62, h: 42&gt;,\n    mask: None,\n    score: PredictionScore: &lt;value: 0.9695767164230347&gt;,\n    category: Category: &lt;id: 2, name: car&gt;&gt;</pre> In\u00a0[17]: Copied! <pre>result.to_coco_annotations()[:3]\n</pre> result.to_coco_annotations()[:3] Out[17]: <pre>[{'image_id': None,\n  'bbox': [321.0, 322.0, 62.0, 42.0],\n  'score': 0.9695767164230347,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 2604},\n {'image_id': None,\n  'bbox': [448.0, 310.0, 48.0, 31.0],\n  'score': 0.9618504047393799,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1488},\n {'image_id': None,\n  'bbox': [833.0, 308.0, 41.0, 36.0],\n  'score': 0.9468276500701904,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1476}]</pre> <ul> <li>ObjectPrediction's can be converted to COCO prediction format:</li> </ul> In\u00a0[18]: Copied! <pre>result.to_coco_predictions(image_id=1)[:3]\n</pre> result.to_coco_predictions(image_id=1)[:3] Out[18]: <pre>[{'image_id': 1,\n  'bbox': [321.0, 322.0, 62.0, 42.0],\n  'score': 0.9695767164230347,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 2604},\n {'image_id': 1,\n  'bbox': [448.0, 310.0, 48.0, 31.0],\n  'score': 0.9618504047393799,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1488},\n {'image_id': 1,\n  'bbox': [833.0, 308.0, 41.0, 36.0],\n  'score': 0.9468276500701904,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1476}]</pre> <ul> <li>ObjectPrediction's can be converted to fiftyone detection format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\nresult.to_fiftyone_detections()[:3]\n</pre> !pip install fiftyone result.to_fiftyone_detections()[:3] <ul> <li>ObjectPrediction's can be converted to imantics annotation format:</li> </ul> In\u00a0[12]: Copied! <pre>!pip install -U imantics\nresult.to_imantics_annotations()[:3]\n</pre> !pip install -U imantics result.to_imantics_annotations()[:3] Out[12]: <pre>[&lt;imantics.annotation.Annotation at 0x7fdf60465100&gt;,\n &lt;imantics.annotation.Annotation at 0x7fdf5b2c06a0&gt;,\n &lt;imantics.annotation.Annotation at 0x7fdf5b2c0c10&gt;]</pre> <ul> <li>Set model and directory parameters:</li> </ul> In\u00a0[19]: Copied! <pre>model_type = \"huggingface\"\nmodel_path = model_path\nmodel_device = \"cpu\"  # or 'cuda:0'\nmodel_confidence_threshold = 0.5\n\nslice_height = 512\nslice_width = 512\noverlap_height_ratio = 0.2\noverlap_width_ratio = 0.2\n\nsource_image_dir = \"demo_data/\"\n</pre> model_type = \"huggingface\" model_path = model_path model_device = \"cpu\"  # or 'cuda:0' model_confidence_threshold = 0.5  slice_height = 512 slice_width = 512 overlap_height_ratio = 0.2 overlap_width_ratio = 0.2  source_image_dir = \"demo_data/\" <ul> <li>Perform sliced inference on given folder:</li> </ul> In\u00a0[21]: Copied! <pre>predict(\n    model_type=model_type,\n    model_path=model_path,\n    model_device=model_device,\n    model_confidence_threshold=model_confidence_threshold,\n    source=source_image_dir,\n    slice_height=slice_height,\n    slice_width=slice_width,\n    overlap_height_ratio=overlap_height_ratio,\n    overlap_width_ratio=overlap_width_ratio,\n)\n</pre> predict(     model_type=model_type,     model_path=model_path,     model_device=model_device,     model_confidence_threshold=model_confidence_threshold,     source=source_image_dir,     slice_height=slice_height,     slice_width=slice_width,     overlap_height_ratio=overlap_height_ratio,     overlap_width_ratio=overlap_width_ratio, ) <pre>There are 4 listed files in folder: demo_data/\n</pre> <pre>Performing inference on images:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Performing prediction on 15 slices.\n</pre> <pre>Performing inference on images:  25%|\u2588\u2588\u258c       | 1/4 [00:01&lt;00:03,  1.08s/it]</pre> <pre>Prediction time is: 990.94 ms\nPerforming prediction on 6 slices.\n</pre> <pre>Performing inference on images:  50%|\u2588\u2588\u2588\u2588\u2588     | 2/4 [00:01&lt;00:01,  1.37it/s]</pre> <pre>Prediction time is: 436.86 ms\nPerforming prediction on 6 slices.\n</pre> <pre>Performing inference on images:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:02&lt;00:00,  1.60it/s]</pre> <pre>Prediction time is: 460.08 ms\nPerforming prediction on 6 slices.\n</pre> <pre>Performing inference on images: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:02&lt;00:00,  1.56it/s]</pre> <pre>Prediction time is: 456.77 ms\nPrediction results are successfully exported to runs\\predict\\exp6\n</pre> <pre>\n</pre>"},{"location":"notebooks/inference_for_huggingface/#sahi-with-huggingface-transformer-for-sliced-inference","title":"SAHI with HuggingFace Transformer for Sliced Inference\u00b6","text":""},{"location":"notebooks/inference_for_huggingface/#0-preparation","title":"0. Preparation\u00b6","text":""},{"location":"notebooks/inference_for_huggingface/#1-standard-inference-with-huggingface-rt-detr-v2-model","title":"1. Standard Inference with HuggingFace RT-DETR v2 Model\u00b6","text":""},{"location":"notebooks/inference_for_huggingface/#2-sliced-inference-with-huggingface-rt-detr-v2-model","title":"2. Sliced Inference with HuggingFace RT-DETR v2 Model\u00b6","text":""},{"location":"notebooks/inference_for_huggingface/#3-prediction-result","title":"3. Prediction Result\u00b6","text":""},{"location":"notebooks/inference_for_huggingface/#4-batch-prediction","title":"4. Batch Prediction\u00b6","text":""},{"location":"notebooks/inference_for_mmdetection/","title":"SAHI with MMDetection for Sliced Inference","text":"<ul> <li>Install latest version of SAHI and MMDetection:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!pip install -U torch sahi mmdet mmengine 'mmcv&gt;=2.0.0'\n</pre> !pip install -U torch sahi mmdet mmengine 'mmcv&gt;=2.0.0' In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.getcwd()\n</pre> import os  os.getcwd() <ul> <li>Import required modules:</li> </ul> In\u00a0[\u00a0]: Copied! <pre># arrange an instance segmentation model for test\nfrom IPython.display import Image\n\n# import required functions, classes\nfrom sahi import AutoDetectionModel\nfrom sahi.predict import get_prediction, get_sliced_prediction, predict\nfrom sahi.utils.cv import read_image\nfrom sahi.utils.file import download_from_url\nfrom sahi.utils.mmdet import (\n    download_mmdet_cascade_mask_rcnn_model,\n    download_mmdet_config,\n)\n</pre> # arrange an instance segmentation model for test from IPython.display import Image  # import required functions, classes from sahi import AutoDetectionModel from sahi.predict import get_prediction, get_sliced_prediction, predict from sahi.utils.cv import read_image from sahi.utils.file import download_from_url from sahi.utils.mmdet import (     download_mmdet_cascade_mask_rcnn_model,     download_mmdet_config, ) <ul> <li>Download a cascade mask rcnn model and two test images:</li> </ul> In\u00a0[\u00a0]: Copied! <pre># download cascade mask rcnn model&amp;config\nmodel_path = \"models/cascade_mask_rcnn.pth\"\ndownload_mmdet_cascade_mask_rcnn_model(model_path)\nconfig_path = download_mmdet_config(\n    model_name=\"cascade_rcnn\",\n    config_file_name=\"cascade-mask-rcnn_r50_fpn_1x_coco.py\",\n)\n\n# download test images into demo_data folder\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",\n    \"demo_data/small-vehicles1.jpeg\",\n)\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\"\n)\n</pre> # download cascade mask rcnn model&amp;config model_path = \"models/cascade_mask_rcnn.pth\" download_mmdet_cascade_mask_rcnn_model(model_path) config_path = download_mmdet_config(     model_name=\"cascade_rcnn\",     config_file_name=\"cascade-mask-rcnn_r50_fpn_1x_coco.py\", )  # download test images into demo_data folder download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",     \"demo_data/small-vehicles1.jpeg\", ) download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\" ) <ul> <li>Instantiate a detection model by defining model weight path, config path and other parameters:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>detection_model = AutoDetectionModel.from_pretrained(\n    model_type=\"mmdet\",\n    model_path=model_path,\n    config_path=config_path,\n    confidence_threshold=0.4,\n    image_size=640,\n    device=\"cpu\",  # or 'cuda:0'\n)\n</pre> detection_model = AutoDetectionModel.from_pretrained(     model_type=\"mmdet\",     model_path=model_path,     config_path=config_path,     confidence_threshold=0.4,     image_size=640,     device=\"cpu\",  # or 'cuda:0' ) <pre>load checkpoint from local path: models/cascade_mask_rcnn.pth\n</pre> <ul> <li>Perform prediction by feeding the <code>get_prediction</code> function with an image path and a DetectionModel instance:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model)\n</pre> result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model) <ul> <li>Or perform prediction by feeding the <code>get_prediction</code> function with a numpy image and a DetectionModel instance:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model)\n</pre> result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model) <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[\u00a0]: <ul> <li>To perform sliced prediction we need to specify slice parameters. In this example we will perform prediction over slices of 256x256 with an overlap ratio of 0.2:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result = get_sliced_prediction(\n    \"demo_data/small-vehicles1.jpeg\",\n    detection_model,\n    slice_height=320,\n    slice_width=320,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n</pre> result = get_sliced_prediction(     \"demo_data/small-vehicles1.jpeg\",     detection_model,     slice_height=320,     slice_width=320,     overlap_height_ratio=0.2,     overlap_width_ratio=0.2, ) <pre>Performing prediction on 12 number of slices.\n</pre> <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[\u00a0]: <ul> <li>Predictions are returned as sahi.prediction.PredictionResult, you can access the object prediction list as:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>object_prediction_list = result.object_prediction_list\n</pre> object_prediction_list = result.object_prediction_list In\u00a0[\u00a0]: Copied! <pre>object_prediction_list[0]\n</pre> object_prediction_list[0] Out[\u00a0]: <pre>ObjectPrediction&lt;\n    bbox: BoundingBox: &lt;(448, 310, 494, 340), w: 46, h: 30&gt;,\n    mask: &lt;sahi.annotation.Mask object&gt;,\n    score: PredictionScore: &lt;value: 0.9975390434265137&gt;,\n    category: Category: &lt;id: 2, name: car&gt;&gt;</pre> <ul> <li>ObjectPrediction's can be converted to COCO annotation format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result.to_coco_annotations()[0]\n</pre> result.to_coco_annotations()[0] Out[\u00a0]: <pre>{'image_id': None,\n 'bbox': [448, 310, 46, 30],\n 'score': 0.9975390434265137,\n 'category_id': 2,\n 'category_name': 'car',\n 'segmentation': [[460,\n   310,\n   455,\n   315,\n   455,\n   316,\n   454,\n   317,\n   454,\n   318,\n   452,\n   320,\n   452,\n   321,\n   451,\n   322,\n   451,\n   323,\n   449,\n   325,\n   449,\n   327,\n   448,\n   328,\n   448,\n   335,\n   449,\n   336,\n   449,\n   338,\n   451,\n   340,\n   486,\n   340,\n   487,\n   339,\n   488,\n   339,\n   490,\n   337,\n   491,\n   337,\n   493,\n   335,\n   493,\n   334,\n   494,\n   333,\n   494,\n   325,\n   493,\n   324,\n   493,\n   319,\n   492,\n   318,\n   492,\n   317,\n   490,\n   315,\n   489,\n   315,\n   487,\n   313,\n   486,\n   313,\n   484,\n   311,\n   482,\n   311,\n   481,\n   310]],\n 'iscrowd': 0,\n 'area': 1199}</pre> <ul> <li>ObjectPrediction's can be converted to COCO prediction format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result.to_coco_predictions(image_id=1)[0]\n</pre> result.to_coco_predictions(image_id=1)[0] Out[\u00a0]: <pre>{'image_id': 1,\n 'bbox': [448, 310, 46, 30],\n 'score': 0.9974352717399597,\n 'category_id': 2,\n 'category_name': 'car',\n 'segmentation': [[465,\n   310,\n   464,\n   311,\n   460,\n   311,\n   459,\n   312,\n   458,\n   312,\n   457,\n   313,\n   457,\n   314,\n   455,\n   316,\n   455,\n   317,\n   452,\n   320,\n   452,\n   321,\n   450,\n   323,\n   450,\n   324,\n   449,\n   325,\n   449,\n   329,\n   448,\n   330,\n   448,\n   334,\n   449,\n   335,\n   449,\n   338,\n   450,\n   339,\n   451,\n   339,\n   452,\n   340,\n   453,\n   340,\n   454,\n   339,\n   458,\n   339,\n   459,\n   338,\n   466,\n   338,\n   467,\n   337,\n   471,\n   337,\n   472,\n   338,\n   481,\n   338,\n   482,\n   339,\n   483,\n   339,\n   484,\n   340,\n   487,\n   340,\n   492,\n   335,\n   493,\n   335,\n   493,\n   334,\n   494,\n   333,\n   494,\n   321,\n   493,\n   320,\n   493,\n   319,\n   490,\n   316,\n   489,\n   316,\n   486,\n   313,\n   485,\n   313,\n   484,\n   312,\n   483,\n   312,\n   482,\n   311,\n   477,\n   311,\n   476,\n   310]],\n 'iscrowd': 0,\n 'area': 1118}</pre> <ul> <li>ObjectPrediction's can be converted to imantics annotation format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result.to_imantics_annotations()[0]\n</pre> result.to_imantics_annotations()[0] Out[\u00a0]: <pre>&lt;imantics.annotation.Annotation&gt;</pre> <ul> <li>ObjectPrediction's can be converted to fiftyone detection format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result.to_fiftyone_detections()[0]\n</pre> result.to_fiftyone_detections()[0] Out[\u00a0]: <pre>&lt;Detection: {\n    'id': '633309e4f0d5345e36349674',\n    'attributes': BaseDict({}),\n    'tags': BaseList([]),\n    'label': 'car',\n    'bounding_box': BaseList([\n        0.41947565543071164,\n        0.5344827586206896,\n        0.04307116104868914,\n        0.05172413793103448,\n    ]),\n    'mask': None,\n    'confidence': 0.9975390434265137,\n    'index': None,\n}&gt;</pre> <ul> <li>Set model and directory parameters:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>model_type = \"mmdet\"\nmodel_path = model_path\nmodel_config_path = config_path\nmodel_device = \"cpu\"  # or 'cuda:0'\nmodel_confidence_threshold = 0.4\n\nslice_height = 640\nslice_width = 640\noverlap_height_ratio = 0.2\noverlap_width_ratio = 0.2\n\nsource_image_dir = \"demo_data/\"\n</pre> model_type = \"mmdet\" model_path = model_path model_config_path = config_path model_device = \"cpu\"  # or 'cuda:0' model_confidence_threshold = 0.4  slice_height = 640 slice_width = 640 overlap_height_ratio = 0.2 overlap_width_ratio = 0.2  source_image_dir = \"demo_data/\" <ul> <li>Perform sliced inference on given folder:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>predict(\n    model_type=model_type,\n    model_path=model_path,\n    model_config_path=config_path,\n    model_device=model_device,\n    model_confidence_threshold=model_confidence_threshold,\n    source=source_image_dir,\n    slice_height=slice_height,\n    slice_width=slice_width,\n    overlap_height_ratio=overlap_height_ratio,\n    overlap_width_ratio=overlap_width_ratio,\n)\n</pre> predict(     model_type=model_type,     model_path=model_path,     model_config_path=config_path,     model_device=model_device,     model_confidence_threshold=model_confidence_threshold,     source=source_image_dir,     slice_height=slice_height,     slice_width=slice_width,     overlap_height_ratio=overlap_height_ratio,     overlap_width_ratio=overlap_width_ratio, ) <pre>There are 3 listed files in folder: demo_data/\nload checkpoint from local path: models/cascade_mask_rcnn.pth\n</pre> <pre>Performing inference on images:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Performing prediction on 4 number of slices.\n</pre> <pre>Performing inference on images:  33%|\u2588\u2588\u2588\u258e      | 1/3 [00:15&lt;00:31, 15.53s/it]</pre> <pre>Prediction time is: 15474.02 ms\nPerforming prediction on 2 number of slices.\n</pre> <pre>Performing inference on images:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:27&lt;00:13, 13.36s/it]</pre> <pre>Prediction time is: 11749.64 ms\nPerforming prediction on 2 number of slices.\n</pre> <pre>Performing inference on images: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:37&lt;00:00, 12.40s/it]</pre> <pre>Prediction time is: 9733.20 ms\nPrediction results are successfully exported to runs/predict/exp16\n</pre> <pre>\n</pre>"},{"location":"notebooks/inference_for_mmdetection/#sahi-with-mmdetection-for-sliced-inference","title":"SAHI with MMDetection for Sliced Inference\u00b6","text":""},{"location":"notebooks/inference_for_mmdetection/#0-preparation","title":"0. Preparation\u00b6","text":""},{"location":"notebooks/inference_for_mmdetection/#1-standard-inference-with-a-mmdetection-model","title":"1. Standard Inference with a MMDetection Model\u00b6","text":""},{"location":"notebooks/inference_for_mmdetection/#2-sliced-inference-with-a-mmdetection-model","title":"2. Sliced Inference with a MMDetection Model\u00b6","text":""},{"location":"notebooks/inference_for_mmdetection/#3-prediction-result","title":"3. Prediction Result\u00b6","text":""},{"location":"notebooks/inference_for_mmdetection/#4-batch-prediction","title":"4. Batch Prediction\u00b6","text":""},{"location":"notebooks/inference_for_roboflow/","title":"SAHI with Roboflow for Sliced Inference","text":"In\u00a0[1]: Copied! <pre>#!pip install -U sahi\n#!pip install -U inference rfdetr\n</pre> #!pip install -U sahi #!pip install -U inference rfdetr In\u00a0[3]: Copied! <pre>from sahi import AutoDetectionModel\n\ndetection_model = AutoDetectionModel.from_pretrained(\n    model_type=\"roboflow\",\n    model=\"rfdetr-base\",  # RoboFlow model ID as string\n    confidence_threshold=0.5,\n    device=\"cpu\",\n)\n</pre> from sahi import AutoDetectionModel  detection_model = AutoDetectionModel.from_pretrained(     model_type=\"roboflow\",     model=\"rfdetr-base\",  # RoboFlow model ID as string     confidence_threshold=0.5,     device=\"cpu\", ) In\u00a0[\u00a0]: Copied! <pre>from rfdetr.detr import RFDETRBase\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nfrom sahi import AutoDetectionModel\n\ndetection_model = AutoDetectionModel.from_pretrained(\n    model_type=\"roboflow\",\n    model=RFDETRBase,  # pass the class itself\n    confidence_threshold=0.5,\n    category_mapping=COCO_CLASSES,\n    device=\"cpu\",\n)\n</pre> from rfdetr.detr import RFDETRBase from rfdetr.util.coco_classes import COCO_CLASSES  from sahi import AutoDetectionModel  detection_model = AutoDetectionModel.from_pretrained(     model_type=\"roboflow\",     model=RFDETRBase,  # pass the class itself     confidence_threshold=0.5,     category_mapping=COCO_CLASSES,     device=\"cpu\", ) <pre>06/06/2025 21:59:52 - WARNING - rfdetr.main -   num_classes mismatch: pretrain weights has 90 classes, but your model has 80 classes\nreinitializing detection head with 90 classes\n</pre> <pre>Loading pretrain weights\n</pre> In\u00a0[\u00a0]: Copied! <pre>from rfdetr.detr import RFDETRBase\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nfrom sahi import AutoDetectionModel\n\ndetection_model = AutoDetectionModel.from_pretrained(\n    model_type=\"roboflow\",\n    model=RFDETRBase(),  # pass an instance of a trained model\n    confidence_threshold=0.5,\n    category_mapping=COCO_CLASSES,\n    device=\"cpu\",\n)\n</pre> from rfdetr.detr import RFDETRBase from rfdetr.util.coco_classes import COCO_CLASSES  from sahi import AutoDetectionModel  detection_model = AutoDetectionModel.from_pretrained(     model_type=\"roboflow\",     model=RFDETRBase(),  # pass an instance of a trained model     confidence_threshold=0.5,     category_mapping=COCO_CLASSES,     device=\"cpu\", ) <pre>Loading pretrain weights\n</pre> In\u00a0[6]: Copied! <pre>from sahi.utils.file import download_from_url\n\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",\n    \"demo_data/small-vehicles1.jpeg\",\n)\n</pre> from sahi.utils.file import download_from_url  download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",     \"demo_data/small-vehicles1.jpeg\", ) In\u00a0[7]: Copied! <pre>from IPython.display import Image\n\nfrom sahi.predict import get_prediction, get_sliced_prediction\n\nresult = get_prediction(\n    \"demo_data/small-vehicles1.jpeg\",\n    detection_model,\n)\nresult.export_visuals(export_dir=\"demo_data/\")\nImage(\"demo_data/prediction_visual.png\")\n</pre> from IPython.display import Image  from sahi.predict import get_prediction, get_sliced_prediction  result = get_prediction(     \"demo_data/small-vehicles1.jpeg\",     detection_model, ) result.export_visuals(export_dir=\"demo_data/\") Image(\"demo_data/prediction_visual.png\") <pre>UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\nUserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n</pre> Out[7]: In\u00a0[8]: Copied! <pre>result_sliced = get_sliced_prediction(\n    \"demo_data/small-vehicles1.jpeg\",\n    detection_model,\n    slice_height=224,\n    slice_width=224,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\nresult_sliced.export_visuals(export_dir=\"demo_data/\")\nImage(\"demo_data/prediction_visual.png\")\n</pre> result_sliced = get_sliced_prediction(     \"demo_data/small-vehicles1.jpeg\",     detection_model,     slice_height=224,     slice_width=224,     overlap_height_ratio=0.2,     overlap_width_ratio=0.2, ) result_sliced.export_visuals(export_dir=\"demo_data/\") Image(\"demo_data/prediction_visual.png\") <pre>Performing prediction on 18 slices.\n</pre> Out[8]: In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/inference_for_roboflow/#sahi-with-roboflow-for-sliced-inference","title":"SAHI with Roboflow for Sliced Inference\u00b6","text":""},{"location":"notebooks/inference_for_roboflow/#sahi-roboflow","title":"SAHI + Roboflow\u00b6","text":"<p>This notebook demonstrates how to run object detection using Roboflow's RF-DETR models in combination with SAHI (Slicing Aided Hyper Inference) for improved detection performance\u2014particularly on small objects.</p> <p>You can set the <code>model</code> parameter in one of three ways, depending on whether you're using a model from Roboflow Universe or a custom-trained RF-DETR model.</p>"},{"location":"notebooks/inference_for_roboflow/#model-input-options","title":"Model Input Options\u00b6","text":""},{"location":"notebooks/inference_for_roboflow/#option-1-roboflow-universe-model-id-as-a-string","title":"\u2705 Option 1: Roboflow Universe Model ID (as a string)\u00b6","text":"<p>Use this when loading a model hosted on Roboflow Universe.</p>"},{"location":"notebooks/inference_for_roboflow/#option-2-use-a-model-class-eg-rfdetrbase","title":"\u2705 Option 2: Use a Model Class (e.g. <code>RFDETRBase</code>)\u00b6","text":"<p>Use this when you want to explicitly define the model class without instantiating it.</p>"},{"location":"notebooks/inference_for_roboflow/#option-3-use-a-model-instance","title":"\u2705 Option 3: Use a Model Instance\u00b6","text":"<p>Use this if you have a pre-loaded or trained model instance.</p>"},{"location":"notebooks/inference_for_roboflow/#running-inference","title":"Running Inference\u00b6","text":"<p>Once your <code>detection_model</code> is initialized using any of the above methods, you can perform inference using SAHI\u2019s slicing and prediction API:</p>"},{"location":"notebooks/inference_for_rtdetr/","title":"SAHI with RT-DETR for Sliced Inference","text":"<ul> <li>Install latest version of SAHI and ultralytics:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!pip install -U torch sahi ultralytics\n</pre> !pip install -U torch sahi ultralytics In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.getcwd()\n</pre> import os  os.getcwd() <ul> <li>Import required modules:</li> </ul> In\u00a0[\u00a0]: Copied! <pre># arrange an instance segmentation model for test\nfrom IPython.display import Image\n\nfrom sahi import AutoDetectionModel\nfrom sahi.predict import get_prediction, get_sliced_prediction, predict\nfrom sahi.utils.cv import read_image\nfrom sahi.utils.file import download_from_url\nfrom sahi.utils.rtdetr import download_rtdetrl_model\n</pre> # arrange an instance segmentation model for test from IPython.display import Image  from sahi import AutoDetectionModel from sahi.predict import get_prediction, get_sliced_prediction, predict from sahi.utils.cv import read_image from sahi.utils.file import download_from_url from sahi.utils.rtdetr import download_rtdetrl_model <ul> <li>Download a rtdetr model and two test images:</li> </ul> In\u00a0[\u00a0]: Copied! <pre># download rtdetr-l model to 'models/rtdetr-l.pt'\nrtdetr_model_path = \"models/rtdetr-l.pt\"\ndownload_rtdetrl_model(rtdetr_model_path)\n\n# download test images into demo_data folder\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",\n    \"demo_data/small-vehicles1.jpeg\",\n)\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\"\n)\n</pre> # download rtdetr-l model to 'models/rtdetr-l.pt' rtdetr_model_path = \"models/rtdetr-l.pt\" download_rtdetrl_model(rtdetr_model_path)  # download test images into demo_data folder download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",     \"demo_data/small-vehicles1.jpeg\", ) download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\" ) <ul> <li>Instantiate a detection model by defining model weight path and other parameters:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>detection_model = AutoDetectionModel.from_pretrained(\n    model_type=\"rtdetr\",\n    model_path=rtdetr_model_path,\n    confidence_threshold=0.3,\n    device=\"cpu\",  # or 'cuda:0'\n)\n</pre> detection_model = AutoDetectionModel.from_pretrained(     model_type=\"rtdetr\",     model_path=rtdetr_model_path,     confidence_threshold=0.3,     device=\"cpu\",  # or 'cuda:0' ) <ul> <li>Perform prediction by feeding the <code>get_prediction</code> function with an image path and a DetectionModel instance:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model)\n</pre> result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model) <ul> <li>Or perform prediction by feeding the <code>get_prediction</code> function with a numpy image and a DetectionModel instance:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model)\n</pre> result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model) <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") <ul> <li>To perform sliced prediction we need to specify slice parameters. In this example we will perform prediction over slices of 256x256 with an overlap ratio of 0.2:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result = get_sliced_prediction(\n    \"demo_data/small-vehicles1.jpeg\",\n    detection_model,\n    slice_height=256,\n    slice_width=256,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n</pre> result = get_sliced_prediction(     \"demo_data/small-vehicles1.jpeg\",     detection_model,     slice_height=256,     slice_width=256,     overlap_height_ratio=0.2,     overlap_width_ratio=0.2, ) <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") <ul> <li>Predictions are returned as sahi.prediction.PredictionResult, you can access the object prediction list as:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>object_prediction_list = result.object_prediction_list\n</pre> object_prediction_list = result.object_prediction_list In\u00a0[\u00a0]: Copied! <pre>object_prediction_list[0]\n</pre> object_prediction_list[0] <ul> <li>ObjectPrediction's can be converted to COCO annotation format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result.to_coco_annotations()[:3]\n</pre> result.to_coco_annotations()[:3] <ul> <li>ObjectPrediction's can be converted to COCO prediction format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result.to_coco_predictions(image_id=1)[:3]\n</pre> result.to_coco_predictions(image_id=1)[:3] <ul> <li>ObjectPrediction's can be converted to imantics annotation format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result.to_imantics_annotations()[:3]\n</pre> result.to_imantics_annotations()[:3] <ul> <li>ObjectPrediction's can be converted to fiftyone detection format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result.to_fiftyone_detections()[:3]\n</pre> result.to_fiftyone_detections()[:3] <ul> <li>Set model and directory parameters:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>model_type = \"rtdetr\"\nmodel_path = rtdetr_model_path\nmodel_device = \"cpu\"  # or 'cuda:0'\nmodel_confidence_threshold = 0.4\n\nslice_height = 256\nslice_width = 256\noverlap_height_ratio = 0.2\noverlap_width_ratio = 0.2\n\nsource_image_dir = \"demo_data/\"\n</pre> model_type = \"rtdetr\" model_path = rtdetr_model_path model_device = \"cpu\"  # or 'cuda:0' model_confidence_threshold = 0.4  slice_height = 256 slice_width = 256 overlap_height_ratio = 0.2 overlap_width_ratio = 0.2  source_image_dir = \"demo_data/\" <ul> <li>Perform sliced inference on given folder:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>predict(\n    model_type=model_type,\n    model_path=model_path,\n    model_device=model_device,\n    model_confidence_threshold=model_confidence_threshold,\n    source=source_image_dir,\n    slice_height=slice_height,\n    slice_width=slice_width,\n    overlap_height_ratio=overlap_height_ratio,\n    overlap_width_ratio=overlap_width_ratio,\n)\n</pre> predict(     model_type=model_type,     model_path=model_path,     model_device=model_device,     model_confidence_threshold=model_confidence_threshold,     source=source_image_dir,     slice_height=slice_height,     slice_width=slice_width,     overlap_height_ratio=overlap_height_ratio,     overlap_width_ratio=overlap_width_ratio, )"},{"location":"notebooks/inference_for_rtdetr/#sahi-with-rt-detr-for-sliced-inference","title":"SAHI with RT-DETR for Sliced Inference\u00b6","text":""},{"location":"notebooks/inference_for_rtdetr/#0-preparation","title":"0. Preparation\u00b6","text":""},{"location":"notebooks/inference_for_rtdetr/#1-standard-inference-with-a-rtdetr-model","title":"1. Standard Inference with a RTDETR Model\u00b6","text":""},{"location":"notebooks/inference_for_rtdetr/#2-sliced-inference-with-a-rtdetr-model","title":"2. Sliced Inference with a RTDETR Model\u00b6","text":""},{"location":"notebooks/inference_for_rtdetr/#3-prediction-result","title":"3. Prediction Result\u00b6","text":""},{"location":"notebooks/inference_for_rtdetr/#4-batch-prediction","title":"4. Batch Prediction\u00b6","text":""},{"location":"notebooks/inference_for_torchvision/","title":"SAHI with TorchVision for Sliced Inference","text":"<ul> <li>Install latest version of SAHI and Torchvision:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!pip install -U git+https://github.com/obss/sahi\n!pip install torch torchvision\n</pre> !pip install -U git+https://github.com/obss/sahi !pip install torch torchvision In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.getcwd()\n</pre> import os  os.getcwd() <ul> <li>Import required modules:</li> </ul> In\u00a0[7]: Copied! <pre># import required functions, classes\nfrom IPython.display import Image\n\nfrom sahi import AutoDetectionModel\nfrom sahi.predict import get_prediction, get_sliced_prediction, predict\nfrom sahi.utils.cv import read_image\nfrom sahi.utils.file import download_from_url\n</pre> # import required functions, classes from IPython.display import Image  from sahi import AutoDetectionModel from sahi.predict import get_prediction, get_sliced_prediction, predict from sahi.utils.cv import read_image from sahi.utils.file import download_from_url In\u00a0[8]: Copied! <pre># set torchvision FasterRCNN model\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n\n# download test images into demo_data folder\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",\n    \"demo_data/small-vehicles1.jpeg\",\n)\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\"\n)\n</pre> # set torchvision FasterRCNN model import torchvision from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)  # download test images into demo_data folder download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",     \"demo_data/small-vehicles1.jpeg\", ) download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\" ) <ul> <li>Instantiate a torchvision model by defining model weight path, config path and other parameters:</li> </ul> In\u00a0[9]: Copied! <pre>detection_model = AutoDetectionModel.from_pretrained(\n    model_type=\"torchvision\",\n    model=model,\n    confidence_threshold=0.5,\n    image_size=640,\n    device=\"cpu\",  # or \"cuda:0\"\n    load_at_init=True,\n)\n</pre> detection_model = AutoDetectionModel.from_pretrained(     model_type=\"torchvision\",     model=model,     confidence_threshold=0.5,     image_size=640,     device=\"cpu\",  # or \"cuda:0\"     load_at_init=True, ) <ul> <li>Perform prediction by feeding the <code>get_prediction</code> function with an image path and a DetectionModel instance:</li> </ul> In\u00a0[10]: Copied! <pre>result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model)\n</pre> result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model) <ul> <li>Or perform prediction by feeding the <code>get_prediction</code> function with a numpy image and a DetectionModel instance:</li> </ul> In\u00a0[5]: Copied! <pre>result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model)\n</pre> result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model) <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[11]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[11]: <ul> <li>To perform sliced prediction we need to specify slice parameters. In this example we will perform prediction over slices of 320x320 with an overlap ratio of 0.2:</li> </ul> In\u00a0[7]: Copied! <pre>result = get_sliced_prediction(\n    \"demo_data/small-vehicles1.jpeg\",\n    detection_model,\n    slice_height=320,\n    slice_width=320,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n</pre> result = get_sliced_prediction(     \"demo_data/small-vehicles1.jpeg\",     detection_model,     slice_height=320,     slice_width=320,     overlap_height_ratio=0.2,     overlap_width_ratio=0.2, ) <pre>Performing prediction on 12 slices.\n</pre> <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[8]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[8]: <ul> <li>Predictions are returned as sahi.prediction.PredictionResult, you can access the object prediction list as:</li> </ul> In\u00a0[9]: Copied! <pre>object_prediction_list = result.object_prediction_list\n</pre> object_prediction_list = result.object_prediction_list In\u00a0[10]: Copied! <pre>object_prediction_list[0]\n</pre> object_prediction_list[0] Out[10]: <pre>ObjectPrediction&lt;\n    bbox: BoundingBox: &lt;(np.float64(319.9983215332031), np.float64(317.1016845703125), np.float64(383.74927520751953), np.float64(365.41888427734375)), w: 63.750953674316406, h: 48.31719970703125&gt;,\n    mask: None,\n    score: PredictionScore: &lt;value: 0.9990587830543518&gt;,\n    category: Category: &lt;id: 3, name: car&gt;&gt;</pre> <ul> <li>ObjectPrediction's can be converted to COCO annotation format:</li> </ul> In\u00a0[11]: Copied! <pre>result.to_coco_annotations()[:3]\n</pre> result.to_coco_annotations()[:3] Out[11]: <pre>[{'image_id': None,\n  'bbox': [319.9983215332031,\n   317.1016845703125,\n   63.750953674316406,\n   48.31719970703125],\n  'score': 0.9990587830543518,\n  'category_id': 3,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 3080},\n {'image_id': None,\n  'bbox': [448.3526611328125,\n   305.8587646484375,\n   47.124786376953125,\n   38.234619140625],\n  'score': 0.9988723397254944,\n  'category_id': 3,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1801},\n {'image_id': None,\n  'bbox': [762.3434448242188,\n   252.02978515625,\n   31.857330322265625,\n   32.469417572021484],\n  'score': 0.996906578540802,\n  'category_id': 3,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1034}]</pre> <ul> <li>ObjectPrediction's can be converted to COCO prediction format:</li> </ul> In\u00a0[12]: Copied! <pre>result.to_coco_predictions(image_id=1)[:3]\n</pre> result.to_coco_predictions(image_id=1)[:3] Out[12]: <pre>[{'image_id': 1,\n  'bbox': [319.9983215332031,\n   317.1016845703125,\n   63.750953674316406,\n   48.31719970703125],\n  'score': 0.9990587830543518,\n  'category_id': 3,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 3080},\n {'image_id': 1,\n  'bbox': [448.3526611328125,\n   305.8587646484375,\n   47.124786376953125,\n   38.234619140625],\n  'score': 0.9988723397254944,\n  'category_id': 3,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1801},\n {'image_id': 1,\n  'bbox': [762.3434448242188,\n   252.02978515625,\n   31.857330322265625,\n   32.469417572021484],\n  'score': 0.996906578540802,\n  'category_id': 3,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1034}]</pre> <ul> <li>ObjectPrediction's can be converted to imantics annotation format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!pip install -U imantics\n</pre> !pip install -U imantics In\u00a0[13]: Copied! <pre>result.to_imantics_annotations()[:3]\n</pre> result.to_imantics_annotations()[:3] Out[13]: <pre>[&lt;imantics.annotation.Annotation at 0x7f81f7545e50&gt;,\n &lt;imantics.annotation.Annotation at 0x7f81ef156b50&gt;,\n &lt;imantics.annotation.Annotation at 0x7f81ef1614c0&gt;]</pre> <ul> <li>Set model and directory parameters:</li> </ul> In\u00a0[15]: Copied! <pre>model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\ndetection_model = AutoDetectionModel.from_pretrained(\n    model_type=\"torchvision\",\n    model=model,\n    confidence_threshold=0.4,\n    image_size=640,\n    device=\"cpu\",  # or \"cuda:0\"\n    load_at_init=True,\n)\n\nslice_height = 256\nslice_width = 256\noverlap_height_ratio = 0.2\noverlap_width_ratio = 0.2\n\nsource_image_dir = \"demo_data/\"\n</pre> model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) detection_model = AutoDetectionModel.from_pretrained(     model_type=\"torchvision\",     model=model,     confidence_threshold=0.4,     image_size=640,     device=\"cpu\",  # or \"cuda:0\"     load_at_init=True, )  slice_height = 256 slice_width = 256 overlap_height_ratio = 0.2 overlap_width_ratio = 0.2  source_image_dir = \"demo_data/\" <ul> <li>Perform sliced inference on given folder:</li> </ul> In\u00a0[16]: Copied! <pre>predict(\n    detection_model=detection_model,\n    source=source_image_dir,\n    slice_height=slice_height,\n    slice_width=slice_width,\n    overlap_height_ratio=overlap_height_ratio,\n    overlap_width_ratio=overlap_width_ratio,\n)\n</pre> predict(     detection_model=detection_model,     source=source_image_dir,     slice_height=slice_height,     slice_width=slice_width,     overlap_height_ratio=overlap_height_ratio,     overlap_width_ratio=overlap_width_ratio, ) <pre>There are 3 listed files in folder: demo_data/\n</pre> <pre>Performing inference on images:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Performing prediction on 15 slices.\n</pre> <pre>Performing inference on images:  33%|\u2588\u2588\u2588\u258e      | 1/3 [00:04&lt;00:08,  4.30s/it]</pre> <pre>Prediction time is: 4271.57 ms\nPerforming prediction on 15 slices.\n</pre> <pre>Performing inference on images:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:08&lt;00:04,  4.38s/it]</pre> <pre>Prediction time is: 4413.79 ms\nPerforming prediction on 20 slices.\n</pre> <pre>Performing inference on images: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:14&lt;00:00,  4.89s/it]</pre> <pre>Prediction time is: 5902.43 ms\nPrediction results are successfully exported to runs/predict/exp6\n</pre> <pre>\n</pre> <p>SAHI also supports the torchvision maskrcnn and maskrcnn_v2 instance segmentation models:</p> In\u00a0[19]: Copied! <pre>maskrcnn_model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n    weights=torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n)\ndetection_model_seg = AutoDetectionModel.from_pretrained(\n    model_type=\"torchvision\",\n    model=maskrcnn_model,\n    confidence_threshold=0.5,\n    mask_threshold=0.8,\n    image_size=1333,\n    device=\"cpu\",  # or \"cuda:0\"\n    load_at_init=True,\n)\nim = read_image(\"demo_data/small-vehicles1.jpeg\")\n</pre> maskrcnn_model = torchvision.models.detection.maskrcnn_resnet50_fpn(     weights=torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights.DEFAULT ) detection_model_seg = AutoDetectionModel.from_pretrained(     model_type=\"torchvision\",     model=maskrcnn_model,     confidence_threshold=0.5,     mask_threshold=0.8,     image_size=1333,     device=\"cpu\",  # or \"cuda:0\"     load_at_init=True, ) im = read_image(\"demo_data/small-vehicles1.jpeg\") <ul> <li>Perform standard segmentation:</li> </ul> In\u00a0[20]: Copied! <pre>result = get_prediction(im, detection_model_seg)\n</pre> result = get_prediction(im, detection_model_seg) In\u00a0[21]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[21]: <ul> <li>Repeat for sliced segmentation:</li> </ul> In\u00a0[22]: Copied! <pre>result = get_sliced_prediction(\n    im,\n    detection_model_seg,\n    slice_height=320,\n    slice_width=320,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n</pre> result = get_sliced_prediction(     im,     detection_model_seg,     slice_height=320,     slice_width=320,     overlap_height_ratio=0.2,     overlap_width_ratio=0.2, ) <pre>Performing prediction on 12 slices.\n</pre> In\u00a0[23]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[23]: <p>Sliced predictions are much better!</p> <ul> <li>Observe the prediction format:</li> </ul> In\u00a0[24]: Copied! <pre>object_prediction_list = result.object_prediction_list\nobject_prediction_list[0]\n</pre> object_prediction_list = result.object_prediction_list object_prediction_list[0] Out[24]: <pre>ObjectPrediction&lt;\n    bbox: BoundingBox: &lt;(524, 225, 544, 240), w: 20, h: 15&gt;,\n    mask: &lt;sahi.annotation.Mask object at 0x7ad2a3b7dbd0&gt;,\n    score: PredictionScore: &lt;value: 0.9975883960723877&gt;,\n    category: Category: &lt;id: 3, name: car&gt;&gt;</pre> In\u00a0[25]: Copied! <pre>object_prediction_list[0].mask.segmentation\n</pre> object_prediction_list[0].mask.segmentation Out[25]: <pre>[[526,\n  227,\n  526,\n  228,\n  525,\n  229,\n  524,\n  230,\n  524,\n  234,\n  524,\n  236,\n  524,\n  240,\n  525,\n  240,\n  526,\n  239,\n  528,\n  239,\n  537,\n  239,\n  538,\n  239,\n  539,\n  239,\n  540,\n  240,\n  541,\n  240,\n  542,\n  239,\n  543,\n  239,\n  544,\n  238,\n  544,\n  237,\n  544,\n  233,\n  543,\n  232,\n  543,\n  231,\n  541,\n  229,\n  541,\n  228,\n  540,\n  227,\n  539,\n  226,\n  538,\n  226,\n  537,\n  225,\n  536,\n  225,\n  535,\n  226,\n  534,\n  226,\n  533,\n  225,\n  532,\n  225,\n  531,\n  226,\n  529,\n  226,\n  528,\n  226,\n  527,\n  227]]</pre>"},{"location":"notebooks/inference_for_torchvision/#sahi-with-torchvision-for-sliced-inference","title":"SAHI with TorchVision for Sliced Inference\u00b6","text":""},{"location":"notebooks/inference_for_torchvision/#0-preparation","title":"0. Preparation\u00b6","text":""},{"location":"notebooks/inference_for_torchvision/#1-standard-inference-with-a-torchvision-model","title":"1. Standard Inference with a Torchvision Model\u00b6","text":""},{"location":"notebooks/inference_for_torchvision/#2-sliced-inference-with-a-torchvision-model","title":"2. Sliced Inference with a TorchVision Model\u00b6","text":""},{"location":"notebooks/inference_for_torchvision/#3-prediction-result","title":"3. Prediction Result\u00b6","text":""},{"location":"notebooks/inference_for_torchvision/#4-batch-prediction","title":"4. Batch Prediction\u00b6","text":""},{"location":"notebooks/inference_for_torchvision/#5-sliced-segmentation","title":"5. Sliced Segmentation\u00b6","text":""},{"location":"notebooks/inference_for_ultralytics/","title":"SAHI with Ultralytics for Sliced Inference","text":"<ul> <li>Install latest version of SAHI and ultralytics:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!pip install -U torch sahi ultralytics\n!pip install ipywidgets\n</pre> !pip install -U torch sahi ultralytics !pip install ipywidgets In\u00a0[\u00a0]: Copied! <pre># import os\n# os.getcwd()\n</pre> # import os # os.getcwd() <ul> <li>Import required modules:</li> </ul> In\u00a0[1]: Copied! <pre>from IPython.display import Image\n\nfrom sahi import AutoDetectionModel\nfrom sahi.predict import get_prediction, get_sliced_prediction, predict\nfrom sahi.utils.cv import read_image\nfrom sahi.utils.file import download_from_url\n</pre> from IPython.display import Image  from sahi import AutoDetectionModel from sahi.predict import get_prediction, get_sliced_prediction, predict from sahi.utils.cv import read_image from sahi.utils.file import download_from_url <ul> <li>Download two test images:</li> </ul> In\u00a0[2]: Copied! <pre>download_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",\n    \"demo_data/small-vehicles1.jpeg\",\n)\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\"\n)\ndownload_from_url(\"https://ultralytics.com/images/boats.jpg\", \"demo_data/obb_test_image.png\")\n</pre> download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",     \"demo_data/small-vehicles1.jpeg\", ) download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\" ) download_from_url(\"https://ultralytics.com/images/boats.jpg\", \"demo_data/obb_test_image.png\") <ul> <li>Instantiate a detection model by defining model weight path and other parameters:</li> </ul> In\u00a0[2]: Copied! <pre>detection_model = AutoDetectionModel.from_pretrained(\n    model_type=\"ultralytics\",\n    model_path=\"yolo11n.pt\",  # any yolov8/yolov9/yolo11/yolo12/rt-detr det model is supported\n    confidence_threshold=0.35,\n    device=\"cpu\",  # or 'cuda:0' if GPU is available\n)\n</pre> detection_model = AutoDetectionModel.from_pretrained(     model_type=\"ultralytics\",     model_path=\"yolo11n.pt\",  # any yolov8/yolov9/yolo11/yolo12/rt-detr det model is supported     confidence_threshold=0.35,     device=\"cpu\",  # or 'cuda:0' if GPU is available ) <ul> <li>Perform prediction by feeding the <code>get_prediction</code> function with an image path and a DetectionModel instance:</li> </ul> In\u00a0[3]: Copied! <pre>result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model)\n</pre> result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model) <ul> <li>Or perform prediction by feeding the <code>get_prediction</code> function with a numpy image and a DetectionModel instance:</li> </ul> In\u00a0[10]: Copied! <pre>result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model)\n</pre> result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model) <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[4]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\", hide_conf=True)\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\", hide_conf=True)  Image(\"demo_data/prediction_visual.png\") Out[4]: <ul> <li>To perform sliced prediction we need to specify slice parameters. In this example we will perform prediction over slices of 256x256 with an overlap ratio of 0.2:</li> </ul> In\u00a0[5]: Copied! <pre>result = get_sliced_prediction(\n    \"demo_data/small-vehicles1.jpeg\",\n    detection_model,\n    slice_height=256,\n    slice_width=256,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n</pre> result = get_sliced_prediction(     \"demo_data/small-vehicles1.jpeg\",     detection_model,     slice_height=256,     slice_width=256,     overlap_height_ratio=0.2,     overlap_width_ratio=0.2, ) <pre>Performing prediction on 15 slices.\n</pre> <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[6]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\", hide_conf=True)\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\", hide_conf=True)  Image(\"demo_data/prediction_visual.png\") Out[6]: <ul> <li>Predictions are returned as sahi.prediction.PredictionResult, you can access the object prediction list as:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>object_prediction_list = result.object_prediction_list\n</pre> object_prediction_list = result.object_prediction_list In\u00a0[\u00a0]: Copied! <pre>object_prediction_list[0]\n</pre> object_prediction_list[0] Out[\u00a0]: <pre>ObjectPrediction&lt;\n    bbox: BoundingBox: &lt;(447.674072265625, 309.57244873046875, 495.7220458984375, 342.06915283203125), w: 48.0479736328125, h: 32.4967041015625&gt;,\n    mask: None,\n    score: PredictionScore: &lt;value: 0.8857606053352356&gt;,\n    category: Category: &lt;id: 2, name: car&gt;&gt;</pre> <ul> <li>ObjectPrediction's can be converted to COCO annotation format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>result.to_coco_annotations()[:3]\n</pre> result.to_coco_annotations()[:3] Out[\u00a0]: <pre>[{'image_id': None,\n  'bbox': [447.674072265625,\n   309.57244873046875,\n   48.0479736328125,\n   32.4967041015625],\n  'score': 0.8857606053352356,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1561},\n {'image_id': None,\n  'bbox': [321.2298278808594,\n   322.0831069946289,\n   61.74859619140625,\n   41.10980987548828],\n  'score': 0.872576892375946,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 2538},\n {'image_id': None,\n  'bbox': [832.658935546875,\n   308.4786148071289,\n   41.4306640625,\n   36.40044403076172],\n  'score': 0.8661476373672485,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1508}]</pre> <ul> <li>ObjectPrediction's can be converted to COCO prediction format:</li> </ul> In\u00a0[8]: Copied! <pre>result.to_coco_predictions(image_id=1)[:3]\n</pre> result.to_coco_predictions(image_id=1)[:3] Out[8]: <pre>[{'image_id': 1,\n  'bbox': [447.7891845703125,\n   309.07598876953125,\n   47.58802032470703,\n   33.136810302734375],\n  'score': 0.9221271276473999,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1576},\n {'image_id': 1,\n  'bbox': [832.6661224365234,\n   308.70198822021484,\n   41.41694641113281,\n   36.017311096191406],\n  'score': 0.8925901055335999,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1491},\n {'image_id': 1,\n  'bbox': [766.1792449951172,\n   260.6599998474121,\n   27.710586547851562,\n   23.558719635009766],\n  'score': 0.8334103226661682,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 652}]</pre> <ul> <li>ObjectPrediction's can be converted to imantics annotation format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!pip install imantics\nresult.to_imantics_annotations()[:3]\n</pre> !pip install imantics result.to_imantics_annotations()[:3] <ul> <li>ObjectPrediction's can be converted to fiftyone detection format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!pip install fiftyone\nresult.to_fiftyone_detections()[:3]\n</pre> !pip install fiftyone result.to_fiftyone_detections()[:3] <ul> <li>Set model and directory parameters:</li> </ul> In\u00a0[7]: Copied! <pre>model_type = \"ultralytics\"\nmodel_path = \"yolo11n.pt\"\nmodel_device = \"cpu\"  # or 'cuda:0' if GPU is available\nmodel_confidence_threshold = 0.4\n\nslice_height = 256\nslice_width = 256\noverlap_height_ratio = 0.2\noverlap_width_ratio = 0.2\n\nsource_image_dir = \"demo_data/\"\n</pre> model_type = \"ultralytics\" model_path = \"yolo11n.pt\" model_device = \"cpu\"  # or 'cuda:0' if GPU is available model_confidence_threshold = 0.4  slice_height = 256 slice_width = 256 overlap_height_ratio = 0.2 overlap_width_ratio = 0.2  source_image_dir = \"demo_data/\" <ul> <li>Perform sliced inference on given folder:</li> </ul> In\u00a0[8]: Copied! <pre>predict(\n    model_type=model_type,\n    model_path=model_path,\n    model_device=model_device,\n    model_confidence_threshold=model_confidence_threshold,\n    source=source_image_dir,\n    slice_height=slice_height,\n    slice_width=slice_width,\n    overlap_height_ratio=overlap_height_ratio,\n    overlap_width_ratio=overlap_width_ratio,\n)\n</pre> predict(     model_type=model_type,     model_path=model_path,     model_device=model_device,     model_confidence_threshold=model_confidence_threshold,     source=source_image_dir,     slice_height=slice_height,     slice_width=slice_width,     overlap_height_ratio=overlap_height_ratio,     overlap_width_ratio=overlap_width_ratio, ) <pre>There are 4 listed files in folder: demo_data/\n</pre> <pre>Performing inference on images:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Performing prediction on 60 slices.\n</pre> <pre>Performing inference on images:  25%|\u2588\u2588\u258c       | 1/4 [00:01&lt;00:05,  1.82s/it]</pre> <pre>Prediction time is: 1737.92 ms\nPerforming prediction on 15 slices.\n</pre> <pre>Performing inference on images:  50%|\u2588\u2588\u2588\u2588\u2588     | 2/4 [00:02&lt;00:02,  1.01s/it]</pre> <pre>Prediction time is: 390.09 ms\nPerforming prediction on 15 slices.\n</pre> <pre>Performing inference on images:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:02&lt;00:00,  1.32it/s]</pre> <pre>Prediction time is: 420.70 ms\nPerforming prediction on 20 slices.\n</pre> <pre>Performing inference on images: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:03&lt;00:00,  1.22it/s]</pre> <pre>Prediction time is: 524.14 ms\nPrediction results are successfully exported to runs\\predict\\exp5\n</pre> <pre>\n</pre> <p>Run the same steps for YOLO11 segmentation model:</p> In\u00a0[3]: Copied! <pre>detection_model_seg = AutoDetectionModel.from_pretrained(\n    model_type=\"ultralytics\",\n    model_path=\"yolo11n-seg.pt\",  # any yolov8/yolov9/yolo11/yolo12 seg model is supported\n    confidence_threshold=0.3,\n    device=\"cpu\",  # or 'cuda:0' if GPU is available\n)\n\nim = read_image(\"demo_data/small-vehicles1.jpeg\")\n</pre> detection_model_seg = AutoDetectionModel.from_pretrained(     model_type=\"ultralytics\",     model_path=\"yolo11n-seg.pt\",  # any yolov8/yolov9/yolo11/yolo12 seg model is supported     confidence_threshold=0.3,     device=\"cpu\",  # or 'cuda:0' if GPU is available )  im = read_image(\"demo_data/small-vehicles1.jpeg\") <ul> <li>Perform standard segmentation:</li> </ul> In\u00a0[4]: Copied! <pre>result = get_prediction(im, detection_model_seg)\n</pre> result = get_prediction(im, detection_model_seg) In\u00a0[11]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[11]: <ul> <li>Repeat for sliced segmentation:</li> </ul> In\u00a0[5]: Copied! <pre>result = get_sliced_prediction(\n    im, detection_model_seg, slice_height=256, slice_width=256, overlap_height_ratio=0.2, overlap_width_ratio=0.2\n)\n</pre> result = get_sliced_prediction(     im, detection_model_seg, slice_height=256, slice_width=256, overlap_height_ratio=0.2, overlap_width_ratio=0.2 ) <pre>Performing prediction on 15 slices.\n</pre> In\u00a0[6]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[6]: <p>Sliced predictions are much better!</p> <ul> <li>Observe the prediction format:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>object_prediction_list = result.object_prediction_list\nobject_prediction_list[0]\n</pre> object_prediction_list = result.object_prediction_list object_prediction_list[0] Out[\u00a0]: <pre>ObjectPrediction&lt;\n    bbox: BoundingBox: &lt;(833, 309, 873, 343), w: 40, h: 34&gt;,\n    mask: &lt;sahi.annotation.Mask object&gt;,\n    score: PredictionScore: &lt;value: 0.9211080074310303&gt;,\n    category: Category: &lt;id: 2, name: car&gt;&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>object_prediction_list[0].mask.segmentation\n</pre> object_prediction_list[0].mask.segmentation Out[\u00a0]: <pre>[[837,\n  313,\n  836,\n  313,\n  836,\n  315,\n  835,\n  316,\n  835,\n  317,\n  835,\n  318,\n  834,\n  319,\n  833,\n  320,\n  833,\n  336,\n  834,\n  337,\n  834,\n  338,\n  834,\n  339,\n  835,\n  340,\n  835,\n  341,\n  836,\n  342,\n  837,\n  342,\n  841,\n  342,\n  846,\n  342,\n  863,\n  342,\n  864,\n  343,\n  869,\n  343,\n  870,\n  343,\n  871,\n  342,\n  871,\n  340,\n  872,\n  340,\n  872,\n  339,\n  873,\n  338,\n  873,\n  323,\n  873,\n  321,\n  873,\n  320,\n  872,\n  319,\n  872,\n  318,\n  871,\n  318,\n  866,\n  313,\n  867,\n  312,\n  865,\n  310,\n  863,\n  310,\n  862,\n  309,\n  841,\n  309,\n  840,\n  309,\n  839,\n  310,\n  839,\n  311,\n  838,\n  311,\n  838,\n  312]]</pre> In\u00a0[\u00a0]: Copied! <pre>predict(\n    model_type=model_type,\n    model_path=model_path,\n    model_device=model_device,\n    model_confidence_threshold=model_confidence_threshold,\n    source=source_image_dir,\n    slice_height=slice_height,\n    slice_width=slice_width,\n    overlap_height_ratio=overlap_height_ratio,\n    overlap_width_ratio=overlap_width_ratio,\n)\n</pre> predict(     model_type=model_type,     model_path=model_path,     model_device=model_device,     model_confidence_threshold=model_confidence_threshold,     source=source_image_dir,     slice_height=slice_height,     slice_width=slice_width,     overlap_height_ratio=overlap_height_ratio,     overlap_width_ratio=overlap_width_ratio, ) <pre>There are 3 listed files in folder: demo_data/\n</pre> <pre>Performing inference on images:   0%|                                                | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Performing prediction on 15 slices.\n</pre> <pre>Performing inference on images:  33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                          | 1/3 [00:01&lt;00:03,  1.74s/it]</pre> <pre>Prediction time is: 1615.49 ms\nPerforming prediction on 15 slices.\n</pre> <pre>Performing inference on images:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b             | 2/3 [00:03&lt;00:01,  1.70s/it]</pre> <pre>Prediction time is: 1593.49 ms\nPerforming prediction on 20 slices.\n</pre> <pre>Performing inference on images: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:05&lt;00:00,  1.86s/it]</pre> <pre>Prediction time is: 2070.87 ms\nPrediction results are successfully exported to runs/predict/exp9\n</pre> <pre>\n</pre> In\u00a0[7]: Copied! <pre>detection_model_obb = AutoDetectionModel.from_pretrained(\n    model_type=\"ultralytics\",\n    model_path=\"yolo11n-obb.pt\",  # any yolov8/yolov9/yolo11/yolo12 obb model is supported\n    confidence_threshold=0.3,\n    device=\"cuda:0\",  # or 'cuda:0' if GPU is available\n)\n\nim = read_image(\"demo_data/obb_test_image.png\")\n</pre> detection_model_obb = AutoDetectionModel.from_pretrained(     model_type=\"ultralytics\",     model_path=\"yolo11n-obb.pt\",  # any yolov8/yolov9/yolo11/yolo12 obb model is supported     confidence_threshold=0.3,     device=\"cuda:0\",  # or 'cuda:0' if GPU is available )  im = read_image(\"demo_data/obb_test_image.png\") In\u00a0[8]: Copied! <pre>result = get_sliced_prediction(\n    im,\n    detection_model_obb,\n    slice_height=512,\n    slice_width=512,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n    # perform_standard_pred = False\n)\n</pre> result = get_sliced_prediction(     im,     detection_model_obb,     slice_height=512,     slice_width=512,     overlap_height_ratio=0.2,     overlap_width_ratio=0.2,     # perform_standard_pred = False ) <pre>Performing prediction on 15 slices.\n</pre> In\u00a0[9]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\", text_size=1, rect_th=3, hide_conf=True)\n\nImage(\"demo_data/prediction_visual.png\", width=800)\n</pre> result.export_visuals(export_dir=\"demo_data/\", text_size=1, rect_th=3, hide_conf=True)  Image(\"demo_data/prediction_visual.png\", width=800) Out[9]:"},{"location":"notebooks/inference_for_ultralytics/#sahi-with-ultralytics-for-sliced-inference","title":"SAHI with Ultralytics for Sliced Inference\u00b6","text":""},{"location":"notebooks/inference_for_ultralytics/#0-preparation","title":"0. Preparation\u00b6","text":""},{"location":"notebooks/inference_for_ultralytics/#1-standard-inference-with-an-ultralytics-model","title":"1. Standard Inference with an Ultralytics Model\u00b6","text":""},{"location":"notebooks/inference_for_ultralytics/#2-sliced-inference-with-a-yolov8yolo11-model","title":"2. Sliced Inference with a YOLOv8/YOLO11 Model\u00b6","text":""},{"location":"notebooks/inference_for_ultralytics/#3-prediction-result","title":"3. Prediction Result\u00b6","text":""},{"location":"notebooks/inference_for_ultralytics/#4-batch-prediction","title":"4. Batch Prediction\u00b6","text":""},{"location":"notebooks/inference_for_ultralytics/#5-sliced-segmentation","title":"5 Sliced Segmentation\u00b6","text":""},{"location":"notebooks/inference_for_ultralytics/#7-sliced-obb-prediction","title":"7 Sliced OBB Prediction\u00b6","text":""},{"location":"notebooks/inference_for_yolov5/","title":"SAHI with YOLOv5 for Sliced Inference","text":"<ul> <li>Install latest version of SAHI and YOLOv5:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!pip install -U torch sahi==0.11.21 yolov5==7.0.14\n</pre> !pip install -U torch sahi==0.11.21 yolov5==7.0.14 In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.getcwd()\n</pre> import os  os.getcwd() <ul> <li>Import required modules:</li> </ul> In\u00a0[\u00a0]: Copied! <pre># arrange an instance segmentation model for test\nfrom IPython.display import Image\n\n# import required functions, classes\nfrom sahi import AutoDetectionModel\nfrom sahi.predict import get_prediction, get_sliced_prediction, predict\nfrom sahi.utils.cv import read_image\nfrom sahi.utils.file import download_from_url\nfrom sahi.utils.yolov5 import download_yolov5s6_model\n</pre> # arrange an instance segmentation model for test from IPython.display import Image  # import required functions, classes from sahi import AutoDetectionModel from sahi.predict import get_prediction, get_sliced_prediction, predict from sahi.utils.cv import read_image from sahi.utils.file import download_from_url from sahi.utils.yolov5 import download_yolov5s6_model <ul> <li>Download a yolov5 model and two test images:</li> </ul> In\u00a0[2]: Copied! <pre># download YOLOV5S6 model to 'models/yolov5s6.pt'\nyolov5_model_path = \"models/yolov5s6.pt\"\ndownload_yolov5s6_model(destination_path=yolov5_model_path)\n\n# download test images into demo_data folder\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",\n    \"demo_data/small-vehicles1.jpeg\",\n)\ndownload_from_url(\n    \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\"\n)\n</pre> # download YOLOV5S6 model to 'models/yolov5s6.pt' yolov5_model_path = \"models/yolov5s6.pt\" download_yolov5s6_model(destination_path=yolov5_model_path)  # download test images into demo_data folder download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/small-vehicles1.jpeg\",     \"demo_data/small-vehicles1.jpeg\", ) download_from_url(     \"https://raw.githubusercontent.com/obss/sahi/main/demo/demo_data/terrain2.png\", \"demo_data/terrain2.png\" ) <ul> <li>Instantiate a detection model by defining model weight path and other parameters:</li> </ul> In\u00a0[18]: Copied! <pre>detection_model = AutoDetectionModel.from_pretrained(\n    model_type=\"yolov5\",\n    model_path=yolov5_model_path,\n    confidence_threshold=0.3,\n    device=\"cpu\",  # or 'cuda:0'\n)\n</pre> detection_model = AutoDetectionModel.from_pretrained(     model_type=\"yolov5\",     model_path=yolov5_model_path,     confidence_threshold=0.3,     device=\"cpu\",  # or 'cuda:0' ) <ul> <li>Perform prediction by feeding the <code>get_prediction</code> function with an image path and a DetectionModel instance:</li> </ul> In\u00a0[4]: Copied! <pre>result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model)\n</pre> result = get_prediction(\"demo_data/small-vehicles1.jpeg\", detection_model) <ul> <li>Or perform prediction by feeding the <code>get_prediction</code> function with a numpy image and a DetectionModel instance:</li> </ul> In\u00a0[5]: Copied! <pre>result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model)\n</pre> result = get_prediction(read_image(\"demo_data/small-vehicles1.jpeg\"), detection_model) <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[6]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[6]: <ul> <li>To perform sliced prediction we need to specify slice parameters. In this example we will perform prediction over slices of 256x256 with an overlap ratio of 0.2:</li> </ul> In\u00a0[7]: Copied! <pre>result = get_sliced_prediction(\n    \"demo_data/small-vehicles1.jpeg\",\n    detection_model,\n    slice_height=256,\n    slice_width=256,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n)\n</pre> result = get_sliced_prediction(     \"demo_data/small-vehicles1.jpeg\",     detection_model,     slice_height=256,     slice_width=256,     overlap_height_ratio=0.2,     overlap_width_ratio=0.2, ) <pre>Performing prediction on 15 number of slices.\n</pre> <ul> <li>Visualize predicted bounding boxes and masks over the original image:</li> </ul> In\u00a0[8]: Copied! <pre>result.export_visuals(export_dir=\"demo_data/\")\n\nImage(\"demo_data/prediction_visual.png\")\n</pre> result.export_visuals(export_dir=\"demo_data/\")  Image(\"demo_data/prediction_visual.png\") Out[8]: <ul> <li>Predictions are returned as sahi.prediction.PredictionResult, you can access the object prediction list as:</li> </ul> In\u00a0[9]: Copied! <pre>object_prediction_list = result.object_prediction_list\n</pre> object_prediction_list = result.object_prediction_list In\u00a0[10]: Copied! <pre>object_prediction_list[0]\n</pre> object_prediction_list[0] Out[10]: <pre>ObjectPrediction&lt;\n    bbox: BoundingBox: &lt;(447, 308, 496, 342), w: 49, h: 34&gt;,\n    mask: None,\n    score: PredictionScore: &lt;value: 0.9154329299926758&gt;,\n    category: Category: &lt;id: 2, name: car&gt;&gt;</pre> <ul> <li>ObjectPrediction's can be converted to COCO annotation format:</li> </ul> In\u00a0[11]: Copied! <pre>result.to_coco_annotations()[:3]\n</pre> result.to_coco_annotations()[:3] Out[11]: <pre>[{'image_id': None,\n  'bbox': [447, 308, 49, 34],\n  'score': 0.9154329299926758,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1666},\n {'image_id': None,\n  'bbox': [321, 321, 62, 41],\n  'score': 0.887977659702301,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 2542},\n {'image_id': None,\n  'bbox': [382, 278, 37, 26],\n  'score': 0.8796820640563965,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 962}]</pre> <ul> <li>ObjectPrediction's can be converted to COCO prediction format:</li> </ul> In\u00a0[12]: Copied! <pre>result.to_coco_predictions(image_id=1)[:3]\n</pre> result.to_coco_predictions(image_id=1)[:3] Out[12]: <pre>[{'image_id': 1,\n  'bbox': [447, 308, 49, 34],\n  'score': 0.9154329299926758,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 1666},\n {'image_id': 1,\n  'bbox': [321, 321, 62, 41],\n  'score': 0.887977659702301,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 2542},\n {'image_id': 1,\n  'bbox': [382, 278, 37, 26],\n  'score': 0.8796820640563965,\n  'category_id': 2,\n  'category_name': 'car',\n  'segmentation': [],\n  'iscrowd': 0,\n  'area': 962}]</pre> <ul> <li>ObjectPrediction's can be converted to imantics annotation format:</li> </ul> In\u00a0[13]: Copied! <pre>result.to_imantics_annotations()[:3]\n</pre> result.to_imantics_annotations()[:3] Out[13]: <pre>[&lt;imantics.annotation.Annotation at 0x7f9d84c8fa60&gt;,\n &lt;imantics.annotation.Annotation at 0x7f9d84d118e0&gt;,\n &lt;imantics.annotation.Annotation at 0x7f9d84d11e50&gt;]</pre> <ul> <li>ObjectPrediction's can be converted to fiftyone detection format:</li> </ul> In\u00a0[14]: Copied! <pre>result.to_fiftyone_detections()[:3]\n</pre> result.to_fiftyone_detections()[:3] Out[14]: <pre>[&lt;Detection: {\n     'id': '633305c1e757fa81eb2b8348',\n     'attributes': BaseDict({}),\n     'tags': BaseList([]),\n     'label': 'car',\n     'bounding_box': BaseList([\n         0.41853932584269665,\n         0.5310344827586206,\n         0.04588014981273408,\n         0.05862068965517241,\n     ]),\n     'mask': None,\n     'confidence': 0.9154329299926758,\n     'index': None,\n }&gt;,\n &lt;Detection: {\n     'id': '633305c1e757fa81eb2b8349',\n     'attributes': BaseDict({}),\n     'tags': BaseList([]),\n     'label': 'car',\n     'bounding_box': BaseList([\n         0.300561797752809,\n         0.553448275862069,\n         0.05805243445692884,\n         0.0706896551724138,\n     ]),\n     'mask': None,\n     'confidence': 0.887977659702301,\n     'index': None,\n }&gt;,\n &lt;Detection: {\n     'id': '633305c1e757fa81eb2b834a',\n     'attributes': BaseDict({}),\n     'tags': BaseList([]),\n     'label': 'car',\n     'bounding_box': BaseList([\n         0.35767790262172283,\n         0.4793103448275862,\n         0.03464419475655431,\n         0.04482758620689655,\n     ]),\n     'mask': None,\n     'confidence': 0.8796820640563965,\n     'index': None,\n }&gt;]</pre> <ul> <li>Set model and directory parameters:</li> </ul> In\u00a0[3]: Copied! <pre>model_type = \"yolov5\"\nmodel_path = yolov5_model_path\nmodel_device = \"cpu\"  # or 'cuda:0'\nmodel_confidence_threshold = 0.4\n\nslice_height = 256\nslice_width = 256\noverlap_height_ratio = 0.2\noverlap_width_ratio = 0.2\n\nsource_image_dir = \"demo_data/\"\n</pre> model_type = \"yolov5\" model_path = yolov5_model_path model_device = \"cpu\"  # or 'cuda:0' model_confidence_threshold = 0.4  slice_height = 256 slice_width = 256 overlap_height_ratio = 0.2 overlap_width_ratio = 0.2  source_image_dir = \"demo_data/\" <ul> <li>Perform sliced inference on given folder:</li> </ul> In\u00a0[4]: Copied! <pre>predict(\n    model_type=model_type,\n    model_path=model_path,\n    model_device=model_device,\n    model_confidence_threshold=model_confidence_threshold,\n    source=source_image_dir,\n    slice_height=slice_height,\n    slice_width=slice_width,\n    overlap_height_ratio=overlap_height_ratio,\n    overlap_width_ratio=overlap_width_ratio,\n)\n</pre> predict(     model_type=model_type,     model_path=model_path,     model_device=model_device,     model_confidence_threshold=model_confidence_threshold,     source=source_image_dir,     slice_height=slice_height,     slice_width=slice_width,     overlap_height_ratio=overlap_height_ratio,     overlap_width_ratio=overlap_width_ratio, ) <pre>There are 3 listed files in folder: demo_data/\n</pre> <pre>Performing inference on images:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Performing prediction on 20 number of slices.\n</pre> <pre>Performing inference on images:  33%|\u2588\u2588\u2588\u258e      | 1/3 [00:01&lt;00:02,  1.01s/it]</pre> <pre>Prediction time is: 971.29 ms\nPerforming prediction on 15 number of slices.\n</pre> <pre>Performing inference on images:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:01&lt;00:00,  1.59it/s]</pre> <pre>Prediction time is: 318.99 ms\nPerforming prediction on 15 number of slices.\n</pre> <pre>Performing inference on images: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01&lt;00:00,  1.77it/s]</pre> <pre>Prediction time is: 292.02 ms\nPrediction results are successfully exported to runs/predict/exp12\n</pre> <pre>\n</pre>"},{"location":"notebooks/inference_for_yolov5/#sahi-with-yolov5-for-sliced-inference","title":"SAHI with YOLOv5 for Sliced Inference\u00b6","text":""},{"location":"notebooks/inference_for_yolov5/#0-preparation","title":"0. Preparation\u00b6","text":""},{"location":"notebooks/inference_for_yolov5/#1-standard-inference-with-a-yolov5-model","title":"1. Standard Inference with a YOLOv5 Model\u00b6","text":""},{"location":"notebooks/inference_for_yolov5/#2-sliced-inference-with-a-yolov5-model","title":"2. Sliced Inference with a YOLOv5 Model\u00b6","text":""},{"location":"notebooks/inference_for_yolov5/#3-prediction-result","title":"3. Prediction Result\u00b6","text":""},{"location":"notebooks/inference_for_yolov5/#4-batch-prediction","title":"4. Batch Prediction\u00b6","text":""},{"location":"notebooks/slicing/","title":"Slicing with SAHI","text":"<ul> <li>Be sure current working directory is 'demo/' folder:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.getcwd()\n</pre> import os  os.getcwd() <ul> <li>Import required functions:</li> </ul> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\n\nfrom sahi.slicing import slice_coco\nfrom sahi.utils.file import load_json\n</pre> import matplotlib.pyplot as plt from PIL import Image, ImageDraw  from sahi.slicing import slice_coco from sahi.utils.file import load_json <ul> <li>Visualize original image and annotations:</li> </ul> In\u00a0[2]: Copied! <pre>coco_dict = load_json(\"demo_data/terrain2_coco.json\")\n\nf, axarr = plt.subplots(1, 1, figsize=(12, 12))\n# read image\nimg_ind = 0\nimg = Image.open(\"demo_data/\" + coco_dict[\"images\"][img_ind][\"file_name\"]).convert(\"RGBA\")\n# iterate over all annotations\nfor ann_ind in range(len(coco_dict[\"annotations\"])):\n    # convert coco bbox to pil bbox\n    xywh = coco_dict[\"annotations\"][ann_ind][\"bbox\"]\n    xyxy = [xywh[0], xywh[1], xywh[0] + xywh[2], xywh[1] + xywh[3]]\n    # visualize bbox over image\n    ImageDraw.Draw(img, \"RGBA\").rectangle(xyxy, width=5)\naxarr.imshow(img)\n</pre> coco_dict = load_json(\"demo_data/terrain2_coco.json\")  f, axarr = plt.subplots(1, 1, figsize=(12, 12)) # read image img_ind = 0 img = Image.open(\"demo_data/\" + coco_dict[\"images\"][img_ind][\"file_name\"]).convert(\"RGBA\") # iterate over all annotations for ann_ind in range(len(coco_dict[\"annotations\"])):     # convert coco bbox to pil bbox     xywh = coco_dict[\"annotations\"][ann_ind][\"bbox\"]     xyxy = [xywh[0], xywh[1], xywh[0] + xywh[2], xywh[1] + xywh[3]]     # visualize bbox over image     ImageDraw.Draw(img, \"RGBA\").rectangle(xyxy, width=5) axarr.imshow(img) Out[2]: <pre>&lt;matplotlib.image.AxesImage at 0x7fdb3fe44310&gt;</pre> <ul> <li>To slice a COCO dataset annoations an images, we need to specify slice parameters. In this example we will slice images into 256x256 grids overlap ratio of 0.2:</li> </ul> In\u00a0[3]: Copied! <pre>coco_dict, coco_path = slice_coco(\n    coco_annotation_file_path=\"demo_data/terrain2_coco.json\",\n    image_dir=\"demo_data/\",\n    output_coco_annotation_file_name=\"sliced_coco.json\",\n    ignore_negative_samples=False,\n    output_dir=\"demo_data/sliced/\",\n    slice_height=256,\n    slice_width=256,\n    overlap_height_ratio=0.2,\n    overlap_width_ratio=0.2,\n    min_area_ratio=0.1,\n    verbose=True,\n)\n</pre> coco_dict, coco_path = slice_coco(     coco_annotation_file_path=\"demo_data/terrain2_coco.json\",     image_dir=\"demo_data/\",     output_coco_annotation_file_name=\"sliced_coco.json\",     ignore_negative_samples=False,     output_dir=\"demo_data/sliced/\",     slice_height=256,     slice_width=256,     overlap_height_ratio=0.2,     overlap_width_ratio=0.2,     min_area_ratio=0.1,     verbose=True, ) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 121222.66it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 10.75it/s]</pre> <pre>indexing coco dataset annotations...\nimage.shape: (1024, 682)\nsliced image path: demo_data/sliced/terrain2_0_0_256_256.png\nsliced image path: demo_data/sliced/terrain2_205_0_461_256.png\nsliced image path: demo_data/sliced/terrain2_410_0_666_256.png\nsliced image path: demo_data/sliced/terrain2_615_0_871_256.png\nsliced image path: demo_data/sliced/terrain2_768_0_1024_256.png\nsliced image path: demo_data/sliced/terrain2_0_205_256_461.png\nsliced image path: demo_data/sliced/terrain2_205_205_461_461.png\nsliced image path: demo_data/sliced/terrain2_410_205_666_461.png\nsliced image path: demo_data/sliced/terrain2_615_205_871_461.png\nsliced image path: demo_data/sliced/terrain2_768_205_1024_461.png\nsliced image path: demo_data/sliced/terrain2_0_410_256_666.png\nsliced image path: demo_data/sliced/terrain2_205_410_461_666.png\nsliced image path: demo_data/sliced/terrain2_410_410_666_666.png\nsliced image path: demo_data/sliced/terrain2_615_410_871_666.png\nsliced image path: demo_data/sliced/terrain2_768_410_1024_666.png\nsliced image path: demo_data/sliced/terrain2_0_426_256_682.png\nsliced image path: demo_data/sliced/terrain2_205_426_461_682.png\nsliced image path: demo_data/sliced/terrain2_410_426_666_682.png\nsliced image path: demo_data/sliced/terrain2_615_426_871_682.png\nsliced image path: demo_data/sliced/terrain2_768_426_1024_682.png\nNum slices: 20 slice_height 256 slice_width 256\nTime to slice demo_data/terrain2.png 0.09270477294921875 seconds\n</pre> <pre>\n</pre> <ul> <li>Visualize sliced images:</li> </ul> In\u00a0[4]: Copied! <pre>f, axarr = plt.subplots(4, 5, figsize=(13, 13))\nimg_ind = 0\nfor ind1 in range(4):\n    for ind2 in range(5):\n        img = Image.open(\"demo_data/sliced/\" + coco_dict[\"images\"][img_ind][\"file_name\"])\n        axarr[ind1, ind2].imshow(img)\n        img_ind += 1\n</pre> f, axarr = plt.subplots(4, 5, figsize=(13, 13)) img_ind = 0 for ind1 in range(4):     for ind2 in range(5):         img = Image.open(\"demo_data/sliced/\" + coco_dict[\"images\"][img_ind][\"file_name\"])         axarr[ind1, ind2].imshow(img)         img_ind += 1 <ul> <li>Visualize sliced annotations on sliced images:</li> </ul> In\u00a0[5]: Copied! <pre>f, axarr = plt.subplots(4, 5, figsize=(13, 13))\nimg_ind = 0\nfor row_ind in range(4):\n    for column_ind in range(5):\n        # read image\n        img = Image.open(\"demo_data/sliced/\" + coco_dict[\"images\"][img_ind][\"file_name\"]).convert(\"RGBA\")\n        # iterate over all annotations\n        for ann_ind in range(len(coco_dict[\"annotations\"])):\n            # find annotations that belong the selected image\n            if coco_dict[\"annotations\"][ann_ind][\"image_id\"] == coco_dict[\"images\"][img_ind][\"id\"]:\n                # convert coco bbox to pil bbox\n                xywh = coco_dict[\"annotations\"][ann_ind][\"bbox\"]\n                xyxy = [xywh[0], xywh[1], xywh[0] + xywh[2], xywh[1] + xywh[3]]\n                # visualize bbox over image\n                ImageDraw.Draw(img, \"RGBA\").rectangle(xyxy, width=5)\n            axarr[row_ind, column_ind].imshow(img)\n        img_ind += 1\n</pre> f, axarr = plt.subplots(4, 5, figsize=(13, 13)) img_ind = 0 for row_ind in range(4):     for column_ind in range(5):         # read image         img = Image.open(\"demo_data/sliced/\" + coco_dict[\"images\"][img_ind][\"file_name\"]).convert(\"RGBA\")         # iterate over all annotations         for ann_ind in range(len(coco_dict[\"annotations\"])):             # find annotations that belong the selected image             if coco_dict[\"annotations\"][ann_ind][\"image_id\"] == coco_dict[\"images\"][img_ind][\"id\"]:                 # convert coco bbox to pil bbox                 xywh = coco_dict[\"annotations\"][ann_ind][\"bbox\"]                 xyxy = [xywh[0], xywh[1], xywh[0] + xywh[2], xywh[1] + xywh[3]]                 # visualize bbox over image                 ImageDraw.Draw(img, \"RGBA\").rectangle(xyxy, width=5)             axarr[row_ind, column_ind].imshow(img)         img_ind += 1"},{"location":"notebooks/slicing/#slicing-with-sahi","title":"Slicing with SAHI\u00b6","text":""},{"location":"notebooks/slicing/#0-preparation","title":"0. Preparation\u00b6","text":""},{"location":"notebooks/slicing/#1-slicing-coco-dataset-into-grids","title":"1. Slicing COCO Dataset into Grids\u00b6","text":""},{"location":"postprocess/combine/","title":"Combine","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine","title":"<code>sahi.postprocess.combine</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine-classes","title":"Classes","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.PostprocessPredictions","title":"<code>PostprocessPredictions</code>","text":"<p>Utilities for calculating IOU/IOS based match for given ObjectPredictions.</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>class PostprocessPredictions:\n    \"\"\"Utilities for calculating IOU/IOS based match for given ObjectPredictions.\"\"\"\n\n    def __init__(\n        self,\n        match_threshold: float = 0.5,\n        match_metric: str = \"IOU\",\n        class_agnostic: bool = True,\n    ):\n        self.match_threshold = match_threshold\n        self.class_agnostic = class_agnostic\n        self.match_metric = match_metric\n\n        check_requirements([\"torch\"])\n\n    def __call__(self, predictions: list[ObjectPrediction]):\n        raise NotImplementedError()\n</code></pre>"},{"location":"postprocess/combine/#sahi.postprocess.combine-functions","title":"Functions","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.batched_greedy_nmm","title":"<code>batched_greedy_nmm(object_predictions_as_tensor, match_metric='IOU', match_threshold=0.5)</code>","text":"<p>Apply greedy version of non-maximum merging per category to avoid detecting too many overlapping bounding boxes for a given object.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <p>(tensor) The location preds for the image along with the class predscores, Shape: [num_boxes,5].</p> required <code>str</code> <p>(str) IOU or IOS</p> <code>'IOU'</code> <code>float</code> <p>(float) The overlap thresh for match metric.</p> <code>0.5</code> <p>Returns:     keep_to_merge_list: (Dict[int:List[int]]) mapping from prediction indices     to keep to a list of prediction indices to be merged.</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>def batched_greedy_nmm(\n    object_predictions_as_tensor: torch.tensor,\n    match_metric: str = \"IOU\",\n    match_threshold: float = 0.5,\n):\n    \"\"\"Apply greedy version of non-maximum merging per category to avoid detecting too many overlapping bounding boxes\n    for a given object.\n\n    Args:\n        object_predictions_as_tensor: (tensor) The location preds for the image\n            along with the class predscores, Shape: [num_boxes,5].\n        match_metric: (str) IOU or IOS\n        match_threshold: (float) The overlap thresh for\n            match metric.\n    Returns:\n        keep_to_merge_list: (Dict[int:List[int]]) mapping from prediction indices\n        to keep to a list of prediction indices to be merged.\n    \"\"\"\n    category_ids = object_predictions_as_tensor[:, 5].squeeze()\n    keep_to_merge_list = {}\n    for category_id in torch.unique(category_ids):\n        curr_indices = torch.where(category_ids == category_id)[0]\n        curr_keep_to_merge_list = greedy_nmm(object_predictions_as_tensor[curr_indices], match_metric, match_threshold)\n        curr_indices_list = curr_indices.tolist()\n        for curr_keep, curr_merge_list in curr_keep_to_merge_list.items():\n            keep = curr_indices_list[curr_keep]\n            merge_list = [curr_indices_list[curr_merge_ind] for curr_merge_ind in curr_merge_list]\n            keep_to_merge_list[keep] = merge_list\n    return keep_to_merge_list\n</code></pre>"},{"location":"postprocess/combine/#sahi.postprocess.combine.batched_greedy_nmm(object_predictions_as_tensor)","title":"<code>object_predictions_as_tensor</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.batched_greedy_nmm(match_metric)","title":"<code>match_metric</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.batched_greedy_nmm(match_threshold)","title":"<code>match_threshold</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.batched_nmm","title":"<code>batched_nmm(object_predictions_as_tensor, match_metric='IOU', match_threshold=0.5)</code>","text":"<p>Apply non-maximum merging per category to avoid detecting too many overlapping bounding boxes for a given object.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>(tensor) The location preds for the image along with the class predscores, Shape: [num_boxes,5].</p> required <code>str</code> <p>(str) IOU or IOS</p> <code>'IOU'</code> <code>float</code> <p>(float) The overlap thresh for match metric.</p> <code>0.5</code> <p>Returns:     keep_to_merge_list: (Dict[int:List[int]]) mapping from prediction indices     to keep to a list of prediction indices to be merged.</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>def batched_nmm(\n    object_predictions_as_tensor: torch.Tensor,\n    match_metric: str = \"IOU\",\n    match_threshold: float = 0.5,\n):\n    \"\"\"Apply non-maximum merging per category to avoid detecting too many overlapping bounding boxes for a given object.\n\n    Args:\n        object_predictions_as_tensor: (tensor) The location preds for the image\n            along with the class predscores, Shape: [num_boxes,5].\n        match_metric: (str) IOU or IOS\n        match_threshold: (float) The overlap thresh for\n            match metric.\n    Returns:\n        keep_to_merge_list: (Dict[int:List[int]]) mapping from prediction indices\n        to keep to a list of prediction indices to be merged.\n    \"\"\"\n    category_ids = object_predictions_as_tensor[:, 5].squeeze()\n    keep_to_merge_list = {}\n    for category_id in torch.unique(category_ids):\n        curr_indices = torch.where(category_ids == category_id)[0]\n        curr_keep_to_merge_list = nmm(object_predictions_as_tensor[curr_indices], match_metric, match_threshold)\n        curr_indices_list = curr_indices.tolist()\n        for curr_keep, curr_merge_list in curr_keep_to_merge_list.items():\n            keep = curr_indices_list[curr_keep]\n            merge_list = [curr_indices_list[curr_merge_ind] for curr_merge_ind in curr_merge_list]\n            keep_to_merge_list[keep] = merge_list\n    return keep_to_merge_list\n</code></pre>"},{"location":"postprocess/combine/#sahi.postprocess.combine.batched_nmm(object_predictions_as_tensor)","title":"<code>object_predictions_as_tensor</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.batched_nmm(match_metric)","title":"<code>match_metric</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.batched_nmm(match_threshold)","title":"<code>match_threshold</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.batched_nms","title":"<code>batched_nms(predictions, match_metric='IOU', match_threshold=0.5)</code>","text":"<p>Apply non-maximum suppression to avoid detecting too many overlapping bounding boxes for a given object.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <p>(tensor) The location preds for the image along with the class predscores, Shape: [num_boxes,5].</p> required <code>str</code> <p>(str) IOU or IOS</p> <code>'IOU'</code> <code>float</code> <p>(float) The overlap thresh for match metric.</p> <code>0.5</code> <p>Returns:     A list of filtered indexes, Shape: [ ,]</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>def batched_nms(predictions: torch.tensor, match_metric: str = \"IOU\", match_threshold: float = 0.5):\n    \"\"\"Apply non-maximum suppression to avoid detecting too many overlapping bounding boxes for a given object.\n\n    Args:\n        predictions: (tensor) The location preds for the image\n            along with the class predscores, Shape: [num_boxes,5].\n        match_metric: (str) IOU or IOS\n        match_threshold: (float) The overlap thresh for\n            match metric.\n    Returns:\n        A list of filtered indexes, Shape: [ ,]\n    \"\"\"\n\n    scores = predictions[:, 4].squeeze()\n    category_ids = predictions[:, 5].squeeze()\n    keep_mask = torch.zeros_like(category_ids, dtype=torch.bool)\n    for category_id in torch.unique(category_ids):\n        curr_indices = torch.where(category_ids == category_id)[0]\n        curr_keep_indices = nms(predictions[curr_indices], match_metric, match_threshold)\n        keep_mask[curr_indices[curr_keep_indices]] = True\n    keep_indices = torch.where(keep_mask)[0]\n    # sort selected indices by their scores\n    keep_indices = keep_indices[scores[keep_indices].sort(descending=True)[1]].tolist()\n    return keep_indices\n</code></pre>"},{"location":"postprocess/combine/#sahi.postprocess.combine.batched_nms(predictions)","title":"<code>predictions</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.batched_nms(match_metric)","title":"<code>match_metric</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.batched_nms(match_threshold)","title":"<code>match_threshold</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.greedy_nmm","title":"<code>greedy_nmm(object_predictions_as_tensor, match_metric='IOU', match_threshold=0.5)</code>","text":"<p>Optimized greedy non-maximum merging for axis-aligned bounding boxes using STRTree.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>(tensor) The location preds for the image along with the class predscores, Shape: [num_boxes,5].</p> required <code>str</code> <p>(str) IOU or IOS</p> <code>'IOU'</code> <code>float</code> <p>(float) The overlap thresh for match metric.</p> <code>0.5</code> <p>Returns:     keep_to_merge_list: (dict[int, list[int]]) mapping from prediction indices     to keep to a list of prediction indices to be merged.</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>def greedy_nmm(\n    object_predictions_as_tensor: torch.Tensor,\n    match_metric: str = \"IOU\",\n    match_threshold: float = 0.5,\n):\n    \"\"\"\n    Optimized greedy non-maximum merging for axis-aligned bounding boxes using STRTree.\n\n    Args:\n        object_predictions_as_tensor: (tensor) The location preds for the image\n            along with the class predscores, Shape: [num_boxes,5].\n        match_metric: (str) IOU or IOS\n        match_threshold: (float) The overlap thresh for match metric.\n    Returns:\n        keep_to_merge_list: (dict[int, list[int]]) mapping from prediction indices\n        to keep to a list of prediction indices to be merged.\n    \"\"\"\n    # Extract coordinates and scores as tensors\n    x1 = object_predictions_as_tensor[:, 0]\n    y1 = object_predictions_as_tensor[:, 1]\n    x2 = object_predictions_as_tensor[:, 2]\n    y2 = object_predictions_as_tensor[:, 3]\n    scores = object_predictions_as_tensor[:, 4]\n\n    # Calculate areas as tensor (vectorized operation)\n    areas = (x2 - x1) * (y2 - y1)\n\n    # Create Shapely boxes only once\n    boxes = []\n    for i in range(len(object_predictions_as_tensor)):\n        boxes.append(\n            box(\n                x1[i].item(),  # Convert only individual values\n                y1[i].item(),\n                x2[i].item(),\n                y2[i].item(),\n            )\n        )\n\n    # Sort indices by score (descending) using torch\n    sorted_idxs = torch.argsort(scores, descending=True).tolist()\n\n    # Build STRtree\n    tree = STRtree(boxes)\n\n    keep_to_merge_list = {}\n    suppressed = set()\n\n    for current_idx in sorted_idxs:\n        if current_idx in suppressed:\n            continue\n\n        current_box = boxes[current_idx]\n        current_area = areas[current_idx].item()  # Convert only when needed\n\n        # Query potential intersections using STRtree\n        candidate_idxs = tree.query(current_box)\n\n        merge_list = []\n        for candidate_idx in candidate_idxs:\n            if candidate_idx == current_idx or candidate_idx in suppressed:\n                continue\n\n            # Only consider candidates with lower or equal score\n            if scores[candidate_idx] &gt; scores[current_idx]:\n                continue\n\n            # For equal scores, use deterministic tie-breaking based on box coordinates\n            if scores[candidate_idx] == scores[current_idx]:\n                # Use box coordinates for stable ordering\n                current_coords = (\n                    x1[current_idx].item(),\n                    y1[current_idx].item(),\n                    x2[current_idx].item(),\n                    y2[current_idx].item(),\n                )\n                candidate_coords = (\n                    x1[candidate_idx].item(),\n                    y1[candidate_idx].item(),\n                    x2[candidate_idx].item(),\n                    y2[candidate_idx].item(),\n                )\n\n                # Compare coordinates lexicographically\n                if candidate_coords &gt; current_coords:\n                    continue\n\n            # Calculate intersection area\n            candidate_box = boxes[candidate_idx]\n            intersection = current_box.intersection(candidate_box).area\n\n            # Calculate metric\n            if match_metric == \"IOU\":\n                union = current_area + areas[candidate_idx].item() - intersection\n                metric = intersection / union if union &gt; 0 else 0\n            elif match_metric == \"IOS\":\n                smaller = min(current_area, areas[candidate_idx].item())\n                metric = intersection / smaller if smaller &gt; 0 else 0\n            else:\n                raise ValueError(\"Invalid match_metric\")\n\n            # Add to merge list if overlap exceeds threshold\n            if metric &gt;= match_threshold:\n                merge_list.append(candidate_idx)\n                suppressed.add(candidate_idx)\n\n        keep_to_merge_list[int(current_idx)] = [int(idx) for idx in merge_list]\n\n    return keep_to_merge_list\n</code></pre>"},{"location":"postprocess/combine/#sahi.postprocess.combine.greedy_nmm(object_predictions_as_tensor)","title":"<code>object_predictions_as_tensor</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.greedy_nmm(match_metric)","title":"<code>match_metric</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.greedy_nmm(match_threshold)","title":"<code>match_threshold</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.nmm","title":"<code>nmm(object_predictions_as_tensor, match_metric='IOU', match_threshold=0.5)</code>","text":"<p>Apply non-maximum merging to avoid detecting too many overlapping bounding boxes for a given object.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>(tensor) The location preds for the image along with the class predscores, Shape: [num_boxes,5].</p> required <code>str</code> <p>(str) IOU or IOS</p> <code>'IOU'</code> <code>float</code> <p>(float) The overlap thresh for match metric.</p> <code>0.5</code> <p>Returns:     keep_to_merge_list: (Dict[int:List[int]]) mapping from prediction indices     to keep to a list of prediction indices to be merged.</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>def nmm(\n    object_predictions_as_tensor: torch.Tensor,\n    match_metric: str = \"IOU\",\n    match_threshold: float = 0.5,\n):\n    \"\"\"Apply non-maximum merging to avoid detecting too many overlapping bounding boxes for a given object.\n\n    Args:\n        object_predictions_as_tensor: (tensor) The location preds for the image\n            along with the class predscores, Shape: [num_boxes,5].\n        match_metric: (str) IOU or IOS\n        match_threshold: (float) The overlap thresh for match metric.\n    Returns:\n        keep_to_merge_list: (Dict[int:List[int]]) mapping from prediction indices\n        to keep to a list of prediction indices to be merged.\n    \"\"\"\n    # Extract coordinates and scores as tensors\n    x1 = object_predictions_as_tensor[:, 0]\n    y1 = object_predictions_as_tensor[:, 1]\n    x2 = object_predictions_as_tensor[:, 2]\n    y2 = object_predictions_as_tensor[:, 3]\n    scores = object_predictions_as_tensor[:, 4]\n\n    # Calculate areas as tensor (vectorized operation)\n    areas = (x2 - x1) * (y2 - y1)\n\n    # Create Shapely boxes only once\n    boxes = []\n    for i in range(len(object_predictions_as_tensor)):\n        boxes.append(\n            box(\n                x1[i].item(),  # Convert only individual values\n                y1[i].item(),\n                x2[i].item(),\n                y2[i].item(),\n            )\n        )\n\n    # Sort indices by score (descending) using torch\n    sorted_idxs = torch.argsort(scores, descending=True).tolist()\n\n    # Build STRtree\n    tree = STRtree(boxes)\n\n    keep_to_merge_list = {}\n    merge_to_keep = {}\n\n    for current_idx in sorted_idxs:\n        current_box = boxes[current_idx]\n        current_area = areas[current_idx].item()  # Convert only when needed\n\n        # Query potential intersections using STRtree\n        candidate_idxs = tree.query(current_box)\n\n        matched_box_indices = []\n        for candidate_idx in candidate_idxs:\n            if candidate_idx == current_idx:\n                continue\n\n            # Only consider candidates with lower or equal score\n            if scores[candidate_idx] &gt; scores[current_idx]:\n                continue\n\n            # For equal scores, use deterministic tie-breaking based on box coordinates\n            if scores[candidate_idx] == scores[current_idx]:\n                # Use box coordinates for stable ordering\n                current_coords = (\n                    x1[current_idx].item(),\n                    y1[current_idx].item(),\n                    x2[current_idx].item(),\n                    y2[current_idx].item(),\n                )\n                candidate_coords = (\n                    x1[candidate_idx].item(),\n                    y1[candidate_idx].item(),\n                    x2[candidate_idx].item(),\n                    y2[candidate_idx].item(),\n                )\n\n                # Compare coordinates lexicographically\n                if candidate_coords &gt; current_coords:\n                    continue\n\n            # Calculate intersection area\n            candidate_box = boxes[candidate_idx]\n            intersection = current_box.intersection(candidate_box).area\n\n            # Calculate metric\n            if match_metric == \"IOU\":\n                union = current_area + areas[candidate_idx].item() - intersection\n                metric = intersection / union if union &gt; 0 else 0\n            elif match_metric == \"IOS\":\n                smaller = min(current_area, areas[candidate_idx].item())\n                metric = intersection / smaller if smaller &gt; 0 else 0\n            else:\n                raise ValueError(\"Invalid match_metric\")\n\n            # Add to matched list if overlap exceeds threshold\n            if metric &gt;= match_threshold:\n                matched_box_indices.append(candidate_idx)\n\n        # Convert current_idx to native Python int\n        current_idx_native = int(current_idx)\n\n        # Create keep_ind to merge_ind_list mapping\n        if current_idx_native not in merge_to_keep:\n            keep_to_merge_list[current_idx_native] = []\n\n            for matched_box_idx in matched_box_indices:\n                matched_box_idx_native = int(matched_box_idx)\n                if matched_box_idx_native not in merge_to_keep:\n                    keep_to_merge_list[current_idx_native].append(matched_box_idx_native)\n                    merge_to_keep[matched_box_idx_native] = current_idx_native\n        else:\n            keep_idx = merge_to_keep[current_idx_native]\n            for matched_box_idx in matched_box_indices:\n                matched_box_idx_native = int(matched_box_idx)\n                if (\n                    matched_box_idx_native not in keep_to_merge_list.get(keep_idx, [])\n                    and matched_box_idx_native not in merge_to_keep\n                ):\n                    if keep_idx not in keep_to_merge_list:\n                        keep_to_merge_list[keep_idx] = []\n                    keep_to_merge_list[keep_idx].append(matched_box_idx_native)\n                    merge_to_keep[matched_box_idx_native] = keep_idx\n\n    return keep_to_merge_list\n</code></pre>"},{"location":"postprocess/combine/#sahi.postprocess.combine.nmm(object_predictions_as_tensor)","title":"<code>object_predictions_as_tensor</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.nmm(match_metric)","title":"<code>match_metric</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.nmm(match_threshold)","title":"<code>match_threshold</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.nms","title":"<code>nms(predictions, match_metric='IOU', match_threshold=0.5)</code>","text":"<p>Optimized non-maximum suppression for axis-aligned bounding boxes using STRTree.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>(tensor) The location preds for the image along with the class predscores, Shape: [num_boxes,5].</p> required <code>str</code> <p>(str) IOU or IOS</p> <code>'IOU'</code> <code>float</code> <p>(float) The overlap thresh for match metric.</p> <code>0.5</code> <p>Returns:</p> Type Description <p>A list of filtered indexes, Shape: [ ,]</p> Source code in <code>sahi/postprocess/combine.py</code> <pre><code>def nms(\n    predictions: torch.Tensor,\n    match_metric: str = \"IOU\",\n    match_threshold: float = 0.5,\n):\n    \"\"\"\n    Optimized non-maximum suppression for axis-aligned bounding boxes using STRTree.\n\n    Args:\n        predictions: (tensor) The location preds for the image along with the class\n            predscores, Shape: [num_boxes,5].\n        match_metric: (str) IOU or IOS\n        match_threshold: (float) The overlap thresh for match metric.\n\n    Returns:\n        A list of filtered indexes, Shape: [ ,]\n    \"\"\"\n\n    # Extract coordinates and scores as tensors\n    x1 = predictions[:, 0]\n    y1 = predictions[:, 1]\n    x2 = predictions[:, 2]\n    y2 = predictions[:, 3]\n    scores = predictions[:, 4]\n\n    # Calculate areas as tensor (vectorized operation)\n    areas = (x2 - x1) * (y2 - y1)\n\n    # Create Shapely boxes only once\n    boxes = []\n    for i in range(len(predictions)):\n        boxes.append(\n            box(\n                x1[i].item(),  # Convert only individual values\n                y1[i].item(),\n                x2[i].item(),\n                y2[i].item(),\n            )\n        )\n\n    # Sort indices by score (descending) using torch\n    sorted_idxs = torch.argsort(scores, descending=True).tolist()\n\n    # Build STRtree\n    tree = STRtree(boxes)\n\n    keep = []\n    suppressed = set()\n\n    for current_idx in sorted_idxs:\n        if current_idx in suppressed:\n            continue\n\n        keep.append(current_idx)\n        current_box = boxes[current_idx]\n        current_area = areas[current_idx].item()  # Convert only when needed\n\n        # Query potential intersections using STRtree\n        candidate_idxs = tree.query(current_box)\n\n        for candidate_idx in candidate_idxs:\n            if candidate_idx == current_idx or candidate_idx in suppressed:\n                continue\n\n            # Skip candidates with higher scores (already processed)\n            if scores[candidate_idx] &gt; scores[current_idx]:\n                continue\n\n            # For equal scores, use deterministic tie-breaking based on box coordinates\n            if scores[candidate_idx] == scores[current_idx]:\n                # Use box coordinates for stable ordering\n                current_coords = (\n                    x1[current_idx].item(),\n                    y1[current_idx].item(),\n                    x2[current_idx].item(),\n                    y2[current_idx].item(),\n                )\n                candidate_coords = (\n                    x1[candidate_idx].item(),\n                    y1[candidate_idx].item(),\n                    x2[candidate_idx].item(),\n                    y2[candidate_idx].item(),\n                )\n\n                # Compare coordinates lexicographically\n                if candidate_coords &gt; current_coords:\n                    continue\n\n            # Calculate intersection area\n            candidate_box = boxes[candidate_idx]\n            intersection = current_box.intersection(candidate_box).area\n\n            # Calculate metric\n            if match_metric == \"IOU\":\n                union = current_area + areas[candidate_idx].item() - intersection\n                metric = intersection / union if union &gt; 0 else 0\n            elif match_metric == \"IOS\":\n                smaller = min(current_area, areas[candidate_idx].item())\n                metric = intersection / smaller if smaller &gt; 0 else 0\n            else:\n                raise ValueError(\"Invalid match_metric\")\n\n            # Suppress if overlap exceeds threshold\n            if metric &gt;= match_threshold:\n                suppressed.add(candidate_idx)\n\n    return keep\n</code></pre>"},{"location":"postprocess/combine/#sahi.postprocess.combine.nms(predictions)","title":"<code>predictions</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.nms(match_metric)","title":"<code>match_metric</code>","text":""},{"location":"postprocess/combine/#sahi.postprocess.combine.nms(match_threshold)","title":"<code>match_threshold</code>","text":""},{"location":"postprocess/utils/","title":"Utils","text":""},{"location":"postprocess/utils/#sahi.postprocess.utils","title":"<code>sahi.postprocess.utils</code>","text":""},{"location":"postprocess/utils/#sahi.postprocess.utils-classes","title":"Classes","text":""},{"location":"postprocess/utils/#sahi.postprocess.utils-functions","title":"Functions","text":""},{"location":"postprocess/utils/#sahi.postprocess.utils.calculate_area","title":"<code>calculate_area(box)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>List[int]</code> <p>[x1, y1, x2, y2]</p> required Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def calculate_area(box: list[int] | np.ndarray) -&gt; float:\n    \"\"\"\n    Args:\n        box (List[int]): [x1, y1, x2, y2]\n    \"\"\"\n    return (box[2] - box[0]) * (box[3] - box[1])\n</code></pre>"},{"location":"postprocess/utils/#sahi.postprocess.utils.calculate_area(box)","title":"<code>box</code>","text":""},{"location":"postprocess/utils/#sahi.postprocess.utils.calculate_bbox_ios","title":"<code>calculate_bbox_ios(pred1, pred2)</code>","text":"<p>Returns the ratio of intersection area to the smaller box's area.</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def calculate_bbox_ios(pred1: ObjectPrediction, pred2: ObjectPrediction) -&gt; float:\n    \"\"\"Returns the ratio of intersection area to the smaller box's area.\"\"\"\n    box1 = np.array(pred1.bbox.to_xyxy())\n    box2 = np.array(pred2.bbox.to_xyxy())\n    area1 = calculate_area(box1)\n    area2 = calculate_area(box2)\n    intersect = calculate_intersection_area(box1, box2)\n    smaller_area = np.minimum(area1, area2)\n    return intersect / smaller_area\n</code></pre>"},{"location":"postprocess/utils/#sahi.postprocess.utils.calculate_bbox_iou","title":"<code>calculate_bbox_iou(pred1, pred2)</code>","text":"<p>Returns the ratio of intersection area to the union.</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def calculate_bbox_iou(pred1: ObjectPrediction, pred2: ObjectPrediction) -&gt; float:\n    \"\"\"Returns the ratio of intersection area to the union.\"\"\"\n    box1 = np.array(pred1.bbox.to_xyxy())\n    box2 = np.array(pred2.bbox.to_xyxy())\n    area1 = calculate_area(box1)\n    area2 = calculate_area(box2)\n    intersect = calculate_intersection_area(box1, box2)\n    return intersect / (area1 + area2 - intersect)\n</code></pre>"},{"location":"postprocess/utils/#sahi.postprocess.utils.calculate_box_union","title":"<code>calculate_box_union(box1, box2)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>List[int]</code> <p>[x1, y1, x2, y2]</p> required <code>List[int]</code> <p>[x1, y1, x2, y2]</p> required Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def calculate_box_union(box1: list[int] | np.ndarray, box2: list[int] | np.ndarray) -&gt; list[int]:\n    \"\"\"\n    Args:\n        box1 (List[int]): [x1, y1, x2, y2]\n        box2 (List[int]): [x1, y1, x2, y2]\n    \"\"\"\n    box1 = np.array(box1)\n    box2 = np.array(box2)\n    left_top = np.minimum(box1[:2], box2[:2])\n    right_bottom = np.maximum(box1[2:], box2[2:])\n    return list(np.concatenate((left_top, right_bottom)))\n</code></pre>"},{"location":"postprocess/utils/#sahi.postprocess.utils.calculate_box_union(box1)","title":"<code>box1</code>","text":""},{"location":"postprocess/utils/#sahi.postprocess.utils.calculate_box_union(box2)","title":"<code>box2</code>","text":""},{"location":"postprocess/utils/#sahi.postprocess.utils.calculate_intersection_area","title":"<code>calculate_intersection_area(box1, box2)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ndarray</code> <p>np.array([x1, y1, x2, y2])</p> required <code>ndarray</code> <p>np.array([x1, y1, x2, y2])</p> required Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def calculate_intersection_area(box1: np.ndarray, box2: np.ndarray) -&gt; float:\n    \"\"\"\n    Args:\n        box1 (np.ndarray): np.array([x1, y1, x2, y2])\n        box2 (np.ndarray): np.array([x1, y1, x2, y2])\n    \"\"\"\n    left_top = np.maximum(box1[:2], box2[:2])\n    right_bottom = np.minimum(box1[2:], box2[2:])\n    width_height = (right_bottom - left_top).clip(min=0)\n    return width_height[0] * width_height[1]\n</code></pre>"},{"location":"postprocess/utils/#sahi.postprocess.utils.calculate_intersection_area(box1)","title":"<code>box1</code>","text":""},{"location":"postprocess/utils/#sahi.postprocess.utils.calculate_intersection_area(box2)","title":"<code>box2</code>","text":""},{"location":"postprocess/utils/#sahi.postprocess.utils.coco_segmentation_to_shapely","title":"<code>coco_segmentation_to_shapely(segmentation)</code>","text":"<p>Fix segment data in COCO format :param segmentation: segment data in COCO format :return:</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def coco_segmentation_to_shapely(segmentation: list | list[list]):\n    \"\"\"Fix segment data in COCO format :param segmentation: segment data in COCO format :return:\"\"\"\n    if isinstance(segmentation, list) and all([not isinstance(seg, list) for seg in segmentation]):\n        segmentation = [segmentation]\n    elif isinstance(segmentation, list) and all([isinstance(seg, list) for seg in segmentation]):\n        pass\n    else:\n        raise ValueError(\"segmentation must be List or List[List]\")\n\n    polygon_list = []\n\n    for coco_polygon in segmentation:\n        point_list = list(zip(coco_polygon[::2], coco_polygon[1::2]))\n        shapely_polygon = Polygon(point_list)\n        polygon_list.append(repair_polygon(shapely_polygon))\n\n    shapely_multipolygon = repair_multipolygon(MultiPolygon(polygon_list))\n    return shapely_multipolygon\n</code></pre>"},{"location":"postprocess/utils/#sahi.postprocess.utils.object_prediction_list_to_numpy","title":"<code>object_prediction_list_to_numpy(object_prediction_list)</code>","text":"<p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray of size N x [x1, y1, x2, y2, score, category_id]</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def object_prediction_list_to_numpy(object_prediction_list: ObjectPredictionList) -&gt; np.ndarray:\n    \"\"\"\n    Returns:\n        np.ndarray of size N x [x1, y1, x2, y2, score, category_id]\n    \"\"\"\n    num_predictions = len(object_prediction_list)\n    numpy_predictions = np.zeros([num_predictions, 6], dtype=np.float32)\n    for ind, object_prediction in enumerate(object_prediction_list):\n        numpy_predictions[ind, :4] = np.array(object_prediction.tolist().bbox.to_xyxy(), dtype=np.float32)\n        numpy_predictions[ind, 4] = object_prediction.tolist().score.value\n        numpy_predictions[ind, 5] = object_prediction.tolist().category.id\n    return numpy_predictions\n</code></pre>"},{"location":"postprocess/utils/#sahi.postprocess.utils.object_prediction_list_to_torch","title":"<code>object_prediction_list_to_torch(object_prediction_list)</code>","text":"<p>Returns:</p> Type Description <code>tensor</code> <p>torch.tensor of size N x [x1, y1, x2, y2, score, category_id]</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def object_prediction_list_to_torch(object_prediction_list: ObjectPredictionList) -&gt; torch.tensor:\n    \"\"\"\n    Returns:\n        torch.tensor of size N x [x1, y1, x2, y2, score, category_id]\n    \"\"\"\n    num_predictions = len(object_prediction_list)\n    torch_predictions = torch.zeros([num_predictions, 6], dtype=torch.float32)\n    for ind, object_prediction in enumerate(object_prediction_list):\n        torch_predictions[ind, :4] = torch.tensor(object_prediction.tolist().bbox.to_xyxy(), dtype=torch.float32)\n        torch_predictions[ind, 4] = object_prediction.tolist().score.value\n        torch_predictions[ind, 5] = object_prediction.tolist().category.id\n    return torch_predictions\n</code></pre>"},{"location":"postprocess/utils/#sahi.postprocess.utils.repair_multipolygon","title":"<code>repair_multipolygon(shapely_multipolygon)</code>","text":"<p>Fix invalid MultiPolygon objects :param shapely_multipolygon: Imported shapely MultiPolygon object :return:</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def repair_multipolygon(shapely_multipolygon: MultiPolygon) -&gt; MultiPolygon:\n    \"\"\"Fix invalid MultiPolygon objects :param shapely_multipolygon: Imported shapely MultiPolygon object :return:\"\"\"\n    if not shapely_multipolygon.is_valid:\n        fixed_geometry = shapely_multipolygon.buffer(0)\n\n        if fixed_geometry.is_valid:\n            if isinstance(fixed_geometry, MultiPolygon):\n                return fixed_geometry\n            elif isinstance(fixed_geometry, Polygon):\n                return MultiPolygon([fixed_geometry])\n            elif isinstance(fixed_geometry, GeometryCollection):\n                polygons = [geom for geom in fixed_geometry.geoms if isinstance(geom, Polygon)]\n                return MultiPolygon(polygons) if polygons else shapely_multipolygon\n\n    return shapely_multipolygon\n</code></pre>"},{"location":"postprocess/utils/#sahi.postprocess.utils.repair_polygon","title":"<code>repair_polygon(shapely_polygon)</code>","text":"<p>Fix polygons :param shapely_polygon: Shapely polygon object :return:</p> Source code in <code>sahi/postprocess/utils.py</code> <pre><code>def repair_polygon(shapely_polygon: Polygon) -&gt; Polygon:\n    \"\"\"Fix polygons :param shapely_polygon: Shapely polygon object :return:\"\"\"\n    if not shapely_polygon.is_valid:\n        fixed_polygon = shapely_polygon.buffer(0)\n        if fixed_polygon.is_valid:\n            if isinstance(fixed_polygon, Polygon):\n                return fixed_polygon\n            elif isinstance(fixed_polygon, MultiPolygon):\n                return max(fixed_polygon.geoms, key=lambda p: p.area)\n            elif isinstance(fixed_polygon, GeometryCollection):\n                polygons = [geom for geom in fixed_polygon.geoms if isinstance(geom, Polygon)]\n                return max(polygons, key=lambda p: p.area) if polygons else shapely_polygon\n\n    return shapely_polygon\n</code></pre>"},{"location":"utils/coco/","title":"Coco","text":""},{"location":"utils/coco/#sahi.utils.coco","title":"<code>sahi.utils.coco</code>","text":""},{"location":"utils/coco/#sahi.utils.coco-classes","title":"Classes","text":""},{"location":"utils/coco/#sahi.utils.coco.Coco","title":"<code>Coco</code>","text":"Source code in <code>sahi/utils/coco.py</code> <pre><code>class Coco:\n    def __init__(\n        self,\n        name: str | None = None,\n        image_dir: str | None = None,\n        remapping_dict: dict[int, int] | None = None,\n        ignore_negative_samples: bool = False,\n        clip_bboxes_to_img_dims: bool = False,\n        image_id_setting: Literal[\"auto\", \"manual\"] = \"auto\",\n    ):\n        \"\"\"Creates Coco object.\n\n        Args:\n            name: str\n                Name of the Coco dataset, it determines exported json name.\n            image_dir: str\n                Base file directory that contains dataset images. Required for dataset merging.\n            remapping_dict: dict\n                {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1\n            ignore_negative_samples: bool\n                If True ignores images without annotations in all operations.\n            image_id_setting: str\n                how to assign image ids while exporting can be\n                auto -&gt; will assign id from scratch (&lt;CocoImage&gt;.id will be ignored)\n                manual -&gt; you will need to provide image ids in &lt;CocoImage&gt; instances (&lt;CocoImage&gt;.id can not be None)\n        \"\"\"\n        if image_id_setting not in [\"auto\", \"manual\"]:\n            raise ValueError(\"image_id_setting must be either 'auto' or 'manual'\")\n        self.name: str | None = name\n        self.image_dir: str | None = image_dir\n        self.remapping_dict: dict[int, int] | None = remapping_dict\n        self.ignore_negative_samples = ignore_negative_samples\n        self.categories: list[CocoCategory] = []\n        self.images = []\n        self._stats = None\n        self.clip_bboxes_to_img_dims = clip_bboxes_to_img_dims\n        self.image_id_setting = image_id_setting\n\n    def add_categories_from_coco_category_list(self, coco_category_list):\n        \"\"\"Creates CocoCategory object using coco category list.\n\n        Args:\n            coco_category_list: List[Dict]\n                [\n                    {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},\n                    {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"}\n                ]\n        \"\"\"\n\n        for coco_category in coco_category_list:\n            if self.remapping_dict is not None:\n                for source_id in self.remapping_dict.keys():\n                    if coco_category[\"id\"] == source_id:\n                        target_id = self.remapping_dict[source_id]\n                        coco_category[\"id\"] = target_id\n\n            self.add_category(CocoCategory.from_coco_category(coco_category))\n\n    def add_category(self, category):\n        \"\"\"Adds category to this Coco instance.\n\n        Args:\n            category: CocoCategory\n        \"\"\"\n\n        # assert type(category) == CocoCategory, \"category must be a CocoCategory instance\"\n        if not isinstance(category, CocoCategory):\n            raise TypeError(\"category must be a CocoCategory instance\")\n        self.categories.append(category)\n\n    def add_image(self, image):\n        \"\"\"Adds image to this Coco instance.\n\n        Args:\n            image: CocoImage\n        \"\"\"\n\n        if self.image_id_setting == \"manual\" and image.id is None:\n            raise ValueError(\"image id should be manually set for image_id_setting='manual'\")\n        self.images.append(image)\n\n    def update_categories(self, desired_name2id: dict[str, int], update_image_filenames: bool = False):\n        \"\"\"Rearranges category mapping of given COCO object based on given desired_name2id. Can also be used to filter\n        some of the categories.\n\n        Args:\n            desired_name2id: dict\n                {\"big_vehicle\": 1, \"car\": 2, \"human\": 3}\n            update_image_filenames: bool\n                If True, updates coco image file_names with absolute file paths.\n        \"\"\"\n        # init vars\n        currentid2desiredid_mapping: dict[int, int | None] = {}\n        updated_coco = Coco(\n            name=self.name,\n            image_dir=self.image_dir,\n            remapping_dict=self.remapping_dict,\n            ignore_negative_samples=self.ignore_negative_samples,\n        )\n        # create category id mapping (currentid2desiredid_mapping)\n        for coco_category in self.categories:\n            current_category_id = coco_category.id\n            current_category_name = coco_category.name\n            if not current_category_name:\n                logger.warning(\"no category name provided to update categories\")\n                continue\n            if current_category_name in desired_name2id.keys():\n                currentid2desiredid_mapping[current_category_id] = desired_name2id[current_category_name]\n            else:\n                # ignore categories that are not included in desired_name2id\n                currentid2desiredid_mapping[current_category_id] = None\n\n        # add updated categories\n        for name in desired_name2id.keys():\n            updated_coco_category = CocoCategory(id=desired_name2id[name], name=name, supercategory=name)\n            updated_coco.add_category(updated_coco_category)\n\n        # add updated images &amp; annotations\n        for coco_image in copy.deepcopy(self.images):\n            updated_coco_image = CocoImage.from_coco_image_dict(coco_image.json)\n            # update filename to abspath\n            file_name_is_abspath = True if os.path.abspath(coco_image.file_name) == coco_image.file_name else False\n            if update_image_filenames and not file_name_is_abspath:\n                if not self.image_dir:\n                    logger.error(\"image directory not set\")\n                else:\n                    updated_coco_image.file_name = str(Path(os.path.abspath(self.image_dir)) / coco_image.file_name)\n            # update annotations\n            for coco_annotation in coco_image.annotations:\n                current_category_id = coco_annotation.category_id\n                desired_category_id = currentid2desiredid_mapping[current_category_id]\n                # append annotations with category id present in desired_name2id\n                if desired_category_id is not None:\n                    # update cetegory id\n                    coco_annotation.category_id = desired_category_id\n                    # append updated annotation to target coco dict\n                    updated_coco_image.add_annotation(coco_annotation)\n            updated_coco.add_image(updated_coco_image)\n\n        # overwrite instance\n        self.__dict__ = updated_coco.__dict__\n\n    def merge(self, coco, desired_name2id=None, verbose=1):\n        \"\"\"Combines the images/annotations/categories of given coco object with current one.\n\n        Args:\n            coco : sahi.utils.coco.Coco instance\n                A COCO dataset object\n            desired_name2id : dict\n                {\"human\": 1, \"car\": 2, \"big_vehicle\": 3}\n            verbose: bool\n                If True, merging info is printed\n        \"\"\"\n        if self.image_dir is None or coco.image_dir is None:\n            raise ValueError(\"image_dir should be provided for merging.\")\n        if verbose:\n            if not desired_name2id:\n                print(\"'desired_name2id' is not specified, combining all categories.\")\n\n        # create desired_name2id by combining all categories, if desired_name2id is not specified\n        coco1 = self\n        coco2 = coco\n        category_ind = 0\n        if desired_name2id is None:\n            desired_name2id = {}\n            for coco in [coco1, coco2]:\n                temp_categories = copy.deepcopy(coco.json_categories)\n                for temp_category in temp_categories:\n                    if temp_category[\"name\"] not in desired_name2id:\n                        desired_name2id[temp_category[\"name\"]] = category_ind\n                        category_ind += 1\n                    else:\n                        continue\n\n        # update categories and image paths\n        for coco in [coco1, coco2]:\n            coco.update_categories(desired_name2id=desired_name2id, update_image_filenames=True)\n\n        # combine images and categories\n        coco1.images.extend(coco2.images)\n        self.images: list[CocoImage] = coco1.images\n        self.categories = coco1.categories\n\n        # print categories\n        if verbose:\n            print(\n                \"Categories are formed as:\\n\",\n                self.json_categories,\n            )\n\n    @classmethod\n    def from_coco_dict_or_path(\n        cls,\n        coco_dict_or_path: dict | str,\n        image_dir: str | None = None,\n        remapping_dict: dict | None = None,\n        ignore_negative_samples: bool = False,\n        clip_bboxes_to_img_dims: bool = False,\n        use_threads: bool = False,\n        num_threads: int = 10,\n    ):\n        \"\"\"Creates coco object from COCO formatted dict or COCO dataset file path.\n\n        Args:\n            coco_dict_or_path: dict/str or List[dict/str]\n                COCO formatted dict or COCO dataset file path\n                List of COCO formatted dict or COCO dataset file path\n            image_dir: str\n                Base file directory that contains dataset images. Required for merging and yolov5 conversion.\n            remapping_dict: dict\n                {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1\n            ignore_negative_samples: bool\n                If True ignores images without annotations in all operations.\n            clip_bboxes_to_img_dims: bool = False\n                Limits bounding boxes to image dimensions.\n            use_threads: bool = False\n                Use threads when processing the json image list, defaults to False\n            num_threads: int = 10\n                Slice the image list to given number of chunks, defaults to 10\n\n        Properties:\n            images: list of CocoImage\n            category_mapping: dict\n        \"\"\"\n        # init coco object\n        coco = cls(\n            image_dir=image_dir,\n            remapping_dict=remapping_dict,\n            ignore_negative_samples=ignore_negative_samples,\n            clip_bboxes_to_img_dims=clip_bboxes_to_img_dims,\n        )\n\n        if type(coco_dict_or_path) not in [str, dict]:\n            raise TypeError(\"coco_dict_or_path should be a dict or str\")\n\n        # load coco dict if path is given\n        if isinstance(coco_dict_or_path, str):\n            coco_dict = load_json(coco_dict_or_path)\n        else:\n            coco_dict = coco_dict_or_path\n\n        dict_size = len(coco_dict[\"images\"])\n\n        # arrange image id to annotation id mapping\n        coco.add_categories_from_coco_category_list(coco_dict[\"categories\"])\n        image_id_to_annotation_list = get_imageid2annotationlist_mapping(coco_dict)\n        category_mapping = coco.category_mapping\n\n        # https://github.com/obss/sahi/issues/98\n        image_id_set: set = set()\n\n        lock = Lock()\n\n        def fill_image_id_set(start, finish, image_list, _image_id_set, _image_id_to_annotation_list, _coco, lock):\n            for coco_image_dict in tqdm(\n                image_list[start:finish], f\"Loading coco annotations between {start} and {finish}\"\n            ):\n                coco_image = CocoImage.from_coco_image_dict(coco_image_dict)\n                image_id = coco_image_dict[\"id\"]\n                # https://github.com/obss/sahi/issues/98\n                if image_id in _image_id_set:\n                    print(f\"duplicate image_id: {image_id}, will be ignored.\")\n                    continue\n                else:\n                    lock.acquire()\n                    _image_id_set.add(image_id)\n                    lock.release()\n\n                # select annotations of the image\n                annotation_list = _image_id_to_annotation_list[image_id]\n                for coco_annotation_dict in annotation_list:\n                    # apply category remapping if remapping_dict is provided\n                    if _coco.remapping_dict is not None:\n                        # apply category remapping (id:id)\n                        category_id = _coco.remapping_dict[coco_annotation_dict[\"category_id\"]]\n                        # update category id\n                        coco_annotation_dict[\"category_id\"] = category_id\n                    else:\n                        category_id = coco_annotation_dict[\"category_id\"]\n                    # get category name (id:name)\n                    category_name = category_mapping[category_id]\n                    coco_annotation = CocoAnnotation.from_coco_annotation_dict(\n                        category_name=category_name, annotation_dict=coco_annotation_dict\n                    )\n                    coco_image.add_annotation(coco_annotation)\n                _coco.add_image(coco_image)\n\n        chunk_size = dict_size / num_threads\n\n        if use_threads is True:\n            for i in range(num_threads):\n                start = i * chunk_size\n                finish = start + chunk_size\n                if finish &gt; dict_size:\n                    finish = dict_size\n                t = Thread(\n                    target=fill_image_id_set,\n                    args=(start, finish, coco_dict[\"images\"], image_id_set, image_id_to_annotation_list, coco, lock),\n                )\n                t.start()\n\n            main_thread = threading.currentThread()\n            for t in threading.enumerate():\n                if t is not main_thread:\n                    t.join()\n\n        else:\n            for coco_image_dict in tqdm(coco_dict[\"images\"], \"Loading coco annotations\"):\n                coco_image = CocoImage.from_coco_image_dict(coco_image_dict)\n                image_id = coco_image_dict[\"id\"]\n                # https://github.com/obss/sahi/issues/98\n                if image_id in image_id_set:\n                    print(f\"duplicate image_id: {image_id}, will be ignored.\")\n                    continue\n                else:\n                    image_id_set.add(image_id)\n                # select annotations of the image\n                annotation_list = image_id_to_annotation_list[image_id]\n                # TODO: coco_annotation_dict is of type CocoAnnotation according to how image_id_to_annotation_list\n                # was created. Either image_id_to_annotation_list is not defined correctly or the following\n                # loop is wrong as it expects a dict.\n                for coco_annotation_dict in annotation_list:\n                    # apply category remapping if remapping_dict is provided\n                    if coco.remapping_dict is not None:\n                        # apply category remapping (id:id)\n                        category_id = coco.remapping_dict[coco_annotation_dict[\"category_id\"]]\n                        # update category id\n                        coco_annotation_dict[\"category_id\"] = category_id\n                    else:\n                        category_id = coco_annotation_dict[\"category_id\"]\n                    # get category name (id:name)\n                    category_name = category_mapping[category_id]\n                    coco_annotation = CocoAnnotation.from_coco_annotation_dict(\n                        category_name=category_name, annotation_dict=coco_annotation_dict\n                    )\n                    coco_image.add_annotation(coco_annotation)\n                coco.add_image(coco_image)\n\n        if clip_bboxes_to_img_dims:\n            coco = coco.get_coco_with_clipped_bboxes()\n        return coco\n\n    @property\n    def json_categories(self):\n        categories = []\n        for category in self.categories:\n            categories.append(category.json)\n        return categories\n\n    @property\n    def category_mapping(self):\n        category_mapping = {}\n        for category in self.categories:\n            category_mapping[category.id] = category.name\n        return category_mapping\n\n    @property\n    def json(self):\n        return create_coco_dict(\n            images=self.images,\n            categories=self.json_categories,\n            ignore_negative_samples=self.ignore_negative_samples,\n            image_id_setting=self.image_id_setting,\n        )\n\n    @property\n    def prediction_array(self):\n        return create_coco_prediction_array(\n            images=self.images,\n            ignore_negative_samples=self.ignore_negative_samples,\n            image_id_setting=self.image_id_setting,\n        )\n\n    @property\n    def stats(self):\n        if not self._stats:\n            self.calculate_stats()\n        return self._stats\n\n    def calculate_stats(self):\n        \"\"\"Iterates over all annotations and calculates total number of.\"\"\"\n        # init all stats\n        num_annotations = 0\n        num_images = len(self.images)\n        num_negative_images = 0\n        num_categories = len(self.json_categories)\n        category_name_to_zero = {category[\"name\"]: 0 for category in self.json_categories}\n        category_name_to_inf = {category[\"name\"]: float(\"inf\") for category in self.json_categories}\n        num_images_per_category = copy.deepcopy(category_name_to_zero)\n        num_annotations_per_category = copy.deepcopy(category_name_to_zero)\n        min_annotation_area_per_category = copy.deepcopy(category_name_to_inf)\n        max_annotation_area_per_category = copy.deepcopy(category_name_to_zero)\n        min_num_annotations_in_image = float(\"inf\")\n        max_num_annotations_in_image = 0\n        total_annotation_area = 0\n        min_annotation_area = 1e10\n        max_annotation_area = 0\n        for image in self.images:\n            image_contains_category = {}\n            for annotation in image.annotations:\n                annotation_area = annotation.area\n                total_annotation_area += annotation_area\n                num_annotations_per_category[annotation.category_name] += 1\n                image_contains_category[annotation.category_name] = 1\n                # update min&amp;max annotation area\n                if annotation_area &gt; max_annotation_area:\n                    max_annotation_area = annotation_area\n                if annotation_area &lt; min_annotation_area:\n                    min_annotation_area = annotation_area\n                if annotation_area &gt; max_annotation_area_per_category[annotation.category_name]:\n                    max_annotation_area_per_category[annotation.category_name] = annotation_area\n                if annotation_area &lt; min_annotation_area_per_category[annotation.category_name]:\n                    min_annotation_area_per_category[annotation.category_name] = annotation_area\n            # update num_negative_images\n            if len(image.annotations) == 0:\n                num_negative_images += 1\n            # update num_annotations\n            num_annotations += len(image.annotations)\n            # update num_images_per_category\n            num_images_per_category = dict(Counter(num_images_per_category) + Counter(image_contains_category))\n            # update min&amp;max_num_annotations_in_image\n            num_annotations_in_image = len(image.annotations)\n            if num_annotations_in_image &gt; max_num_annotations_in_image:\n                max_num_annotations_in_image = num_annotations_in_image\n            if num_annotations_in_image &lt; min_num_annotations_in_image:\n                min_num_annotations_in_image = num_annotations_in_image\n        if (num_images - num_negative_images) &gt; 0:\n            avg_num_annotations_in_image = num_annotations / (num_images - num_negative_images)\n            avg_annotation_area = total_annotation_area / num_annotations\n        else:\n            avg_num_annotations_in_image = 0\n            avg_annotation_area = 0\n\n        self._stats = {\n            \"num_images\": num_images,\n            \"num_annotations\": num_annotations,\n            \"num_categories\": num_categories,\n            \"num_negative_images\": num_negative_images,\n            \"num_images_per_category\": num_images_per_category,\n            \"num_annotations_per_category\": num_annotations_per_category,\n            \"min_num_annotations_in_image\": min_num_annotations_in_image,\n            \"max_num_annotations_in_image\": max_num_annotations_in_image,\n            \"avg_num_annotations_in_image\": avg_num_annotations_in_image,\n            \"min_annotation_area\": min_annotation_area,\n            \"max_annotation_area\": max_annotation_area,\n            \"avg_annotation_area\": avg_annotation_area,\n            \"min_annotation_area_per_category\": min_annotation_area_per_category,\n            \"max_annotation_area_per_category\": max_annotation_area_per_category,\n        }\n\n    def split_coco_as_train_val(self, train_split_rate=0.9, numpy_seed=0):\n        \"\"\"Split images into train-val and returns them as sahi.utils.coco.Coco objects.\n\n        Args:\n            train_split_rate: float\n            numpy_seed: int\n                random seed. Actually, this doesn't use numpy, but the random package\n                from the standard library, but it is called numpy for compatibility.\n\n        Returns:\n            result : dict\n                {\n                    \"train_coco\": \"\",\n                    \"val_coco\": \"\",\n                }\n        \"\"\"\n        # divide images\n        num_images = len(self.images)\n        shuffled_images = copy.deepcopy(self.images)\n        random.seed(numpy_seed)\n        random.shuffle(shuffled_images)\n        num_train = int(num_images * train_split_rate)\n        train_images = shuffled_images[:num_train]\n        val_images = shuffled_images[num_train:]\n\n        # form train val coco objects\n        train_coco = Coco(\n            name=self.name if self.name else \"split\" + \"_train\",\n            image_dir=self.image_dir,\n        )\n        train_coco.images = train_images\n        train_coco.categories = self.categories\n\n        val_coco = Coco(name=self.name if self.name else \"split\" + \"_val\", image_dir=self.image_dir)\n        val_coco.images = val_images\n        val_coco.categories = self.categories\n\n        # return result\n        return {\n            \"train_coco\": train_coco,\n            \"val_coco\": val_coco,\n        }\n\n    def export_as_yolov5(\n        self,\n        output_dir: str | Path,\n        train_split_rate: float = 1.0,\n        numpy_seed: int = 0,\n        mp: bool = False,\n        disable_symlink: bool = False,\n    ):\n        \"\"\"Deprecated.\n\n        Please use export_as_yolo instead. Calls export_as_yolo with the same arguments.\n        \"\"\"\n        warnings.warn(\n            \"export_as_yolov5 is deprecated. Please use export_as_yolo instead.\",\n            DeprecationWarning,\n        )\n        self.export_as_yolo(\n            output_dir=output_dir,\n            train_split_rate=train_split_rate,\n            numpy_seed=numpy_seed,\n            mp=mp,\n            disable_symlink=disable_symlink,\n        )\n\n    def export_as_yolo(\n        self,\n        output_dir: str | Path,\n        train_split_rate: float = 1.0,\n        numpy_seed: int = 0,\n        mp: bool = False,\n        disable_symlink: bool = False,\n    ):\n        \"\"\"Exports current COCO dataset in ultralytics/yolo format. Creates train val folders with image symlinks and\n        txt files and a data yaml file.\n\n        Args:\n            output_dir: str\n                Export directory.\n            train_split_rate: float\n                If given 1, will be exported as train split.\n                If given 0, will be exported as val split.\n                If in between 0-1, both train/val splits will be calculated and exported.\n            numpy_seed: int\n                To fix the numpy seed.\n            mp: bool\n                If True, multiprocess mode is on.\n                Should be called in 'if __name__ == __main__:' block.\n            disable_symlink: bool\n                If True, symlinks will not be created. Instead, images will be copied.\n        \"\"\"\n        try:\n            import yaml\n        except ImportError:\n            raise ImportError('Please run \"pip install -U pyyaml\" to install yaml first for yolo formatted exporting.')\n\n        # set split_mode\n        if 0 &lt; train_split_rate and train_split_rate &lt; 1:\n            split_mode = \"TRAINVAL\"\n        elif train_split_rate == 0:\n            split_mode = \"VAL\"\n        elif train_split_rate == 1:\n            split_mode = \"TRAIN\"\n        else:\n            raise ValueError(\"train_split_rate cannot be &lt;0 or &gt;1\")\n\n        # split dataset\n        if split_mode == \"TRAINVAL\":\n            result = self.split_coco_as_train_val(\n                train_split_rate=train_split_rate,\n                numpy_seed=numpy_seed,\n            )\n            train_coco = result[\"train_coco\"]\n            val_coco = result[\"val_coco\"]\n        elif split_mode == \"TRAIN\":\n            train_coco = self\n            val_coco = None\n        elif split_mode == \"VAL\":\n            train_coco = None\n            val_coco = self\n\n        # create train val image dirs\n        train_dir = \"\"\n        val_dir = \"\"\n        if split_mode in [\"TRAINVAL\", \"TRAIN\"]:\n            train_dir = Path(os.path.abspath(output_dir)) / \"train/\"\n            train_dir.mkdir(parents=True, exist_ok=True)  # create dir\n        if split_mode in [\"TRAINVAL\", \"VAL\"]:\n            val_dir = Path(os.path.abspath(output_dir)) / \"val/\"\n            val_dir.mkdir(parents=True, exist_ok=True)  # create dir\n\n        # create image symlinks and annotation txts\n        if split_mode in [\"TRAINVAL\", \"TRAIN\"]:\n            export_yolo_images_and_txts_from_coco_object(\n                output_dir=train_dir,\n                coco=train_coco,\n                ignore_negative_samples=self.ignore_negative_samples,\n                mp=mp,\n                disable_symlink=disable_symlink,\n            )\n        if split_mode in [\"TRAINVAL\", \"VAL\"]:\n            export_yolo_images_and_txts_from_coco_object(\n                output_dir=val_dir,\n                coco=val_coco,\n                ignore_negative_samples=self.ignore_negative_samples,\n                mp=mp,\n                disable_symlink=disable_symlink,\n            )\n\n        # create yolov5 data yaml\n        data = {\n            \"train\": str(train_dir),\n            \"val\": str(val_dir),\n            \"nc\": len(self.category_mapping),\n            \"names\": list(self.category_mapping.values()),\n        }\n        yaml_path = str(Path(output_dir) / \"data.yml\")\n        with open(yaml_path, \"w\") as outfile:\n            yaml.dump(data, outfile, default_flow_style=None)\n\n    def get_subsampled_coco(self, subsample_ratio: int = 2, category_id: int | None = None):\n        \"\"\"Subsamples images with subsample_ratio and returns as sahi.utils.coco.Coco object.\n\n        Args:\n            subsample_ratio: int\n                10 means take every 10th image with its annotations\n            category_id: int\n                subsample only images containing given category_id, if -1 then subsamples negative samples\n        Returns:\n            subsampled_coco: sahi.utils.coco.Coco\n        \"\"\"\n        subsampled_coco = Coco(\n            name=self.name,\n            image_dir=self.image_dir,\n            remapping_dict=self.remapping_dict,\n            ignore_negative_samples=self.ignore_negative_samples,\n        )\n        subsampled_coco.add_categories_from_coco_category_list(self.json_categories)\n\n        if category_id is not None:\n            # get images that contain given category id\n            images_that_contain_category: list[CocoImage] = []\n            annotation: CocoAnnotation\n            for image in self.images:\n                category_id_to_contains = defaultdict(int)\n                for annotation in image.annotations:\n                    category_id_to_contains[annotation.category_id] = 1\n                if category_id_to_contains[category_id]:\n                    add_this_image = True\n                elif category_id == -1 and len(image.annotations) == 0:\n                    # if category_id is given as -1, select negative samples\n                    add_this_image = True\n                else:\n                    add_this_image = False\n\n                if add_this_image:\n                    images_that_contain_category.append(image)\n\n            # get images that does not contain given category id\n            images_that_doesnt_contain_category: list[CocoImage] = []\n            for image in self.images:\n                category_id_to_contains = defaultdict(int)\n                for annotation in image.annotations:\n                    category_id_to_contains[annotation.category_id] = 1\n                if category_id_to_contains[category_id]:\n                    add_this_image = False\n                elif category_id == -1 and len(image.annotations) == 0:\n                    # if category_id is given as -1, dont select negative samples\n                    add_this_image = False\n                else:\n                    add_this_image = True\n\n                if add_this_image:\n                    images_that_doesnt_contain_category.append(image)\n\n        if category_id:\n            selected_images = images_that_contain_category\n            # add images that does not contain given category without subsampling\n            for image_ind in range(len(images_that_doesnt_contain_category)):\n                subsampled_coco.add_image(images_that_doesnt_contain_category[image_ind])\n        else:\n            selected_images = self.images\n        for image_ind in range(0, len(selected_images), subsample_ratio):\n            subsampled_coco.add_image(selected_images[image_ind])\n\n        return subsampled_coco\n\n    def get_upsampled_coco(self, upsample_ratio: int = 2, category_id: int | None = None):\n        \"\"\"Upsamples images with upsample_ratio and returns as sahi.utils.coco.Coco object.\n\n        Args:\n            upsample_ratio: int\n                10 means copy each sample 10 times\n            category_id: int\n                upsample only images containing given category_id, if -1 then upsamples negative samples\n        Returns:\n            upsampled_coco: sahi.utils.coco.Coco\n        \"\"\"\n        upsampled_coco = Coco(\n            name=self.name,\n            image_dir=self.image_dir,\n            remapping_dict=self.remapping_dict,\n            ignore_negative_samples=self.ignore_negative_samples,\n        )\n        upsampled_coco.add_categories_from_coco_category_list(self.json_categories)\n        for ind in range(upsample_ratio):\n            for image_ind in range(len(self.images)):\n                # calculate add_this_image\n                if category_id is not None:\n                    category_id_to_contains = defaultdict(int)\n                    annotation: CocoAnnotation\n                    for annotation in self.images[image_ind].annotations:\n                        category_id_to_contains[annotation.category_id] = 1\n                    if category_id_to_contains[category_id]:\n                        add_this_image = True\n                    elif category_id == -1 and len(self.images[image_ind].annotations) == 0:\n                        # if category_id is given as -1, select negative samples\n                        add_this_image = True\n                    elif ind == 0:\n                        # in first iteration add all images\n                        add_this_image = True\n                    else:\n                        add_this_image = False\n                else:\n                    add_this_image = True\n\n                if add_this_image:\n                    upsampled_coco.add_image(self.images[image_ind])\n\n        return upsampled_coco\n\n    def get_area_filtered_coco(self, min=0, max_val=float(\"inf\"), intervals_per_category=None):\n        \"\"\"Filters annotation areas with given min and max values and returns remaining images as sahi.utils.coco.Coco\n        object.\n\n        Args:\n            min: int\n                minimum allowed area\n            max_val: int\n                maximum allowed area\n            intervals_per_category: dict of dicts\n                {\n                    \"human\": {\"min\": 20, \"max\": 10000},\n                    \"vehicle\": {\"min\": 50, \"max\": 15000},\n                }\n        Returns:\n            area_filtered_coco: sahi.utils.coco.Coco\n        \"\"\"\n        area_filtered_coco = Coco(\n            name=self.name,\n            image_dir=self.image_dir,\n            remapping_dict=self.remapping_dict,\n            ignore_negative_samples=self.ignore_negative_samples,\n        )\n        area_filtered_coco.add_categories_from_coco_category_list(self.json_categories)\n        for image in self.images:\n            is_valid_image = True\n            for annotation in image.annotations:\n                if intervals_per_category is not None and annotation.category_name in intervals_per_category.keys():\n                    category_based_min = intervals_per_category[annotation.category_name][\"min\"]\n                    category_based_max = intervals_per_category[annotation.category_name][\"max\"]\n                    if annotation.area &lt; category_based_min or annotation.area &gt; category_based_max:\n                        is_valid_image = False\n                if annotation.area &lt; min or annotation.area &gt; max_val:\n                    is_valid_image = False\n            if is_valid_image:\n                area_filtered_coco.add_image(image)\n\n        return area_filtered_coco\n\n    def get_coco_with_clipped_bboxes(self):\n        \"\"\"Limits overflowing bounding boxes to image dimensions.\"\"\"\n        from sahi.slicing import annotation_inside_slice\n\n        coco = Coco(\n            name=self.name,\n            image_dir=self.image_dir,\n            remapping_dict=self.remapping_dict,\n            ignore_negative_samples=self.ignore_negative_samples,\n        )\n        coco.add_categories_from_coco_category_list(self.json_categories)\n\n        for coco_img in self.images:\n            img_dims = [0, 0, coco_img.width, coco_img.height]\n            coco_image = CocoImage(\n                file_name=coco_img.file_name, height=coco_img.height, width=coco_img.width, id=coco_img.id\n            )\n            for coco_ann in coco_img.annotations:\n                ann_dict: dict = coco_ann.json\n                if annotation_inside_slice(annotation=ann_dict, slice_bbox=img_dims):\n                    shapely_ann = coco_ann.get_sliced_coco_annotation(img_dims)\n                    bbox = ShapelyAnnotation.to_xywh(shapely_ann._shapely_annotation)\n                    coco_ann_from_shapely = CocoAnnotation(\n                        bbox=bbox,\n                        category_id=coco_ann.category_id,\n                        category_name=coco_ann.category_name,\n                        image_id=coco_ann.image_id,\n                    )\n                    coco_image.add_annotation(coco_ann_from_shapely)\n                else:\n                    continue\n            coco.add_image(coco_image)\n        return coco\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco-functions","title":"Functions","text":""},{"location":"utils/coco/#sahi.utils.coco.Coco.__init__","title":"<code>__init__(name=None, image_dir=None, remapping_dict=None, ignore_negative_samples=False, clip_bboxes_to_img_dims=False, image_id_setting='auto')</code>","text":"<p>Creates Coco object.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> \u00b6 <code>str | None</code> <p>str Name of the Coco dataset, it determines exported json name.</p> <code>None</code> <code>image_dir</code> \u00b6 <code>str | None</code> <p>str Base file directory that contains dataset images. Required for dataset merging.</p> <code>None</code> <code>remapping_dict</code> \u00b6 <code>dict[int, int] | None</code> <p>dict {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1</p> <code>None</code> <code>ignore_negative_samples</code> \u00b6 <code>bool</code> <p>bool If True ignores images without annotations in all operations.</p> <code>False</code> <code>image_id_setting</code> \u00b6 <code>Literal['auto', 'manual']</code> <p>str how to assign image ids while exporting can be auto -&gt; will assign id from scratch (.id will be ignored) manual -&gt; you will need to provide image ids in  instances (.id can not be None) <code>'auto'</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(\n    self,\n    name: str | None = None,\n    image_dir: str | None = None,\n    remapping_dict: dict[int, int] | None = None,\n    ignore_negative_samples: bool = False,\n    clip_bboxes_to_img_dims: bool = False,\n    image_id_setting: Literal[\"auto\", \"manual\"] = \"auto\",\n):\n    \"\"\"Creates Coco object.\n\n    Args:\n        name: str\n            Name of the Coco dataset, it determines exported json name.\n        image_dir: str\n            Base file directory that contains dataset images. Required for dataset merging.\n        remapping_dict: dict\n            {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1\n        ignore_negative_samples: bool\n            If True ignores images without annotations in all operations.\n        image_id_setting: str\n            how to assign image ids while exporting can be\n            auto -&gt; will assign id from scratch (&lt;CocoImage&gt;.id will be ignored)\n            manual -&gt; you will need to provide image ids in &lt;CocoImage&gt; instances (&lt;CocoImage&gt;.id can not be None)\n    \"\"\"\n    if image_id_setting not in [\"auto\", \"manual\"]:\n        raise ValueError(\"image_id_setting must be either 'auto' or 'manual'\")\n    self.name: str | None = name\n    self.image_dir: str | None = image_dir\n    self.remapping_dict: dict[int, int] | None = remapping_dict\n    self.ignore_negative_samples = ignore_negative_samples\n    self.categories: list[CocoCategory] = []\n    self.images = []\n    self._stats = None\n    self.clip_bboxes_to_img_dims = clip_bboxes_to_img_dims\n    self.image_id_setting = image_id_setting\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.add_categories_from_coco_category_list","title":"<code>add_categories_from_coco_category_list(coco_category_list)</code>","text":"<p>Creates CocoCategory object using coco category list.</p> <p>Parameters:</p> Name Type Description Default <code>coco_category_list</code> \u00b6 <p>List[Dict] [     {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},     {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"} ]</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_categories_from_coco_category_list(self, coco_category_list):\n    \"\"\"Creates CocoCategory object using coco category list.\n\n    Args:\n        coco_category_list: List[Dict]\n            [\n                {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},\n                {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"}\n            ]\n    \"\"\"\n\n    for coco_category in coco_category_list:\n        if self.remapping_dict is not None:\n            for source_id in self.remapping_dict.keys():\n                if coco_category[\"id\"] == source_id:\n                    target_id = self.remapping_dict[source_id]\n                    coco_category[\"id\"] = target_id\n\n        self.add_category(CocoCategory.from_coco_category(coco_category))\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.add_category","title":"<code>add_category(category)</code>","text":"<p>Adds category to this Coco instance.</p> <p>Parameters:</p> Name Type Description Default <code>category</code> \u00b6 <p>CocoCategory</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_category(self, category):\n    \"\"\"Adds category to this Coco instance.\n\n    Args:\n        category: CocoCategory\n    \"\"\"\n\n    # assert type(category) == CocoCategory, \"category must be a CocoCategory instance\"\n    if not isinstance(category, CocoCategory):\n        raise TypeError(\"category must be a CocoCategory instance\")\n    self.categories.append(category)\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.add_image","title":"<code>add_image(image)</code>","text":"<p>Adds image to this Coco instance.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> \u00b6 <p>CocoImage</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_image(self, image):\n    \"\"\"Adds image to this Coco instance.\n\n    Args:\n        image: CocoImage\n    \"\"\"\n\n    if self.image_id_setting == \"manual\" and image.id is None:\n        raise ValueError(\"image id should be manually set for image_id_setting='manual'\")\n    self.images.append(image)\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.calculate_stats","title":"<code>calculate_stats()</code>","text":"<p>Iterates over all annotations and calculates total number of.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def calculate_stats(self):\n    \"\"\"Iterates over all annotations and calculates total number of.\"\"\"\n    # init all stats\n    num_annotations = 0\n    num_images = len(self.images)\n    num_negative_images = 0\n    num_categories = len(self.json_categories)\n    category_name_to_zero = {category[\"name\"]: 0 for category in self.json_categories}\n    category_name_to_inf = {category[\"name\"]: float(\"inf\") for category in self.json_categories}\n    num_images_per_category = copy.deepcopy(category_name_to_zero)\n    num_annotations_per_category = copy.deepcopy(category_name_to_zero)\n    min_annotation_area_per_category = copy.deepcopy(category_name_to_inf)\n    max_annotation_area_per_category = copy.deepcopy(category_name_to_zero)\n    min_num_annotations_in_image = float(\"inf\")\n    max_num_annotations_in_image = 0\n    total_annotation_area = 0\n    min_annotation_area = 1e10\n    max_annotation_area = 0\n    for image in self.images:\n        image_contains_category = {}\n        for annotation in image.annotations:\n            annotation_area = annotation.area\n            total_annotation_area += annotation_area\n            num_annotations_per_category[annotation.category_name] += 1\n            image_contains_category[annotation.category_name] = 1\n            # update min&amp;max annotation area\n            if annotation_area &gt; max_annotation_area:\n                max_annotation_area = annotation_area\n            if annotation_area &lt; min_annotation_area:\n                min_annotation_area = annotation_area\n            if annotation_area &gt; max_annotation_area_per_category[annotation.category_name]:\n                max_annotation_area_per_category[annotation.category_name] = annotation_area\n            if annotation_area &lt; min_annotation_area_per_category[annotation.category_name]:\n                min_annotation_area_per_category[annotation.category_name] = annotation_area\n        # update num_negative_images\n        if len(image.annotations) == 0:\n            num_negative_images += 1\n        # update num_annotations\n        num_annotations += len(image.annotations)\n        # update num_images_per_category\n        num_images_per_category = dict(Counter(num_images_per_category) + Counter(image_contains_category))\n        # update min&amp;max_num_annotations_in_image\n        num_annotations_in_image = len(image.annotations)\n        if num_annotations_in_image &gt; max_num_annotations_in_image:\n            max_num_annotations_in_image = num_annotations_in_image\n        if num_annotations_in_image &lt; min_num_annotations_in_image:\n            min_num_annotations_in_image = num_annotations_in_image\n    if (num_images - num_negative_images) &gt; 0:\n        avg_num_annotations_in_image = num_annotations / (num_images - num_negative_images)\n        avg_annotation_area = total_annotation_area / num_annotations\n    else:\n        avg_num_annotations_in_image = 0\n        avg_annotation_area = 0\n\n    self._stats = {\n        \"num_images\": num_images,\n        \"num_annotations\": num_annotations,\n        \"num_categories\": num_categories,\n        \"num_negative_images\": num_negative_images,\n        \"num_images_per_category\": num_images_per_category,\n        \"num_annotations_per_category\": num_annotations_per_category,\n        \"min_num_annotations_in_image\": min_num_annotations_in_image,\n        \"max_num_annotations_in_image\": max_num_annotations_in_image,\n        \"avg_num_annotations_in_image\": avg_num_annotations_in_image,\n        \"min_annotation_area\": min_annotation_area,\n        \"max_annotation_area\": max_annotation_area,\n        \"avg_annotation_area\": avg_annotation_area,\n        \"min_annotation_area_per_category\": min_annotation_area_per_category,\n        \"max_annotation_area_per_category\": max_annotation_area_per_category,\n    }\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.export_as_yolo","title":"<code>export_as_yolo(output_dir, train_split_rate=1.0, numpy_seed=0, mp=False, disable_symlink=False)</code>","text":"<p>Exports current COCO dataset in ultralytics/yolo format. Creates train val folders with image symlinks and txt files and a data yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> \u00b6 <code>str | Path</code> <p>str Export directory.</p> required <code>train_split_rate</code> \u00b6 <code>float</code> <p>float If given 1, will be exported as train split. If given 0, will be exported as val split. If in between 0-1, both train/val splits will be calculated and exported.</p> <code>1.0</code> <code>numpy_seed</code> \u00b6 <code>int</code> <p>int To fix the numpy seed.</p> <code>0</code> <code>mp</code> \u00b6 <code>bool</code> <p>bool If True, multiprocess mode is on. Should be called in 'if name == main:' block.</p> <code>False</code> <code>disable_symlink</code> \u00b6 <code>bool</code> <p>bool If True, symlinks will not be created. Instead, images will be copied.</p> <code>False</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_as_yolo(\n    self,\n    output_dir: str | Path,\n    train_split_rate: float = 1.0,\n    numpy_seed: int = 0,\n    mp: bool = False,\n    disable_symlink: bool = False,\n):\n    \"\"\"Exports current COCO dataset in ultralytics/yolo format. Creates train val folders with image symlinks and\n    txt files and a data yaml file.\n\n    Args:\n        output_dir: str\n            Export directory.\n        train_split_rate: float\n            If given 1, will be exported as train split.\n            If given 0, will be exported as val split.\n            If in between 0-1, both train/val splits will be calculated and exported.\n        numpy_seed: int\n            To fix the numpy seed.\n        mp: bool\n            If True, multiprocess mode is on.\n            Should be called in 'if __name__ == __main__:' block.\n        disable_symlink: bool\n            If True, symlinks will not be created. Instead, images will be copied.\n    \"\"\"\n    try:\n        import yaml\n    except ImportError:\n        raise ImportError('Please run \"pip install -U pyyaml\" to install yaml first for yolo formatted exporting.')\n\n    # set split_mode\n    if 0 &lt; train_split_rate and train_split_rate &lt; 1:\n        split_mode = \"TRAINVAL\"\n    elif train_split_rate == 0:\n        split_mode = \"VAL\"\n    elif train_split_rate == 1:\n        split_mode = \"TRAIN\"\n    else:\n        raise ValueError(\"train_split_rate cannot be &lt;0 or &gt;1\")\n\n    # split dataset\n    if split_mode == \"TRAINVAL\":\n        result = self.split_coco_as_train_val(\n            train_split_rate=train_split_rate,\n            numpy_seed=numpy_seed,\n        )\n        train_coco = result[\"train_coco\"]\n        val_coco = result[\"val_coco\"]\n    elif split_mode == \"TRAIN\":\n        train_coco = self\n        val_coco = None\n    elif split_mode == \"VAL\":\n        train_coco = None\n        val_coco = self\n\n    # create train val image dirs\n    train_dir = \"\"\n    val_dir = \"\"\n    if split_mode in [\"TRAINVAL\", \"TRAIN\"]:\n        train_dir = Path(os.path.abspath(output_dir)) / \"train/\"\n        train_dir.mkdir(parents=True, exist_ok=True)  # create dir\n    if split_mode in [\"TRAINVAL\", \"VAL\"]:\n        val_dir = Path(os.path.abspath(output_dir)) / \"val/\"\n        val_dir.mkdir(parents=True, exist_ok=True)  # create dir\n\n    # create image symlinks and annotation txts\n    if split_mode in [\"TRAINVAL\", \"TRAIN\"]:\n        export_yolo_images_and_txts_from_coco_object(\n            output_dir=train_dir,\n            coco=train_coco,\n            ignore_negative_samples=self.ignore_negative_samples,\n            mp=mp,\n            disable_symlink=disable_symlink,\n        )\n    if split_mode in [\"TRAINVAL\", \"VAL\"]:\n        export_yolo_images_and_txts_from_coco_object(\n            output_dir=val_dir,\n            coco=val_coco,\n            ignore_negative_samples=self.ignore_negative_samples,\n            mp=mp,\n            disable_symlink=disable_symlink,\n        )\n\n    # create yolov5 data yaml\n    data = {\n        \"train\": str(train_dir),\n        \"val\": str(val_dir),\n        \"nc\": len(self.category_mapping),\n        \"names\": list(self.category_mapping.values()),\n    }\n    yaml_path = str(Path(output_dir) / \"data.yml\")\n    with open(yaml_path, \"w\") as outfile:\n        yaml.dump(data, outfile, default_flow_style=None)\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.export_as_yolov5","title":"<code>export_as_yolov5(output_dir, train_split_rate=1.0, numpy_seed=0, mp=False, disable_symlink=False)</code>","text":"<p>Deprecated.</p> <p>Please use export_as_yolo instead. Calls export_as_yolo with the same arguments.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_as_yolov5(\n    self,\n    output_dir: str | Path,\n    train_split_rate: float = 1.0,\n    numpy_seed: int = 0,\n    mp: bool = False,\n    disable_symlink: bool = False,\n):\n    \"\"\"Deprecated.\n\n    Please use export_as_yolo instead. Calls export_as_yolo with the same arguments.\n    \"\"\"\n    warnings.warn(\n        \"export_as_yolov5 is deprecated. Please use export_as_yolo instead.\",\n        DeprecationWarning,\n    )\n    self.export_as_yolo(\n        output_dir=output_dir,\n        train_split_rate=train_split_rate,\n        numpy_seed=numpy_seed,\n        mp=mp,\n        disable_symlink=disable_symlink,\n    )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.from_coco_dict_or_path","title":"<code>from_coco_dict_or_path(coco_dict_or_path, image_dir=None, remapping_dict=None, ignore_negative_samples=False, clip_bboxes_to_img_dims=False, use_threads=False, num_threads=10)</code>  <code>classmethod</code>","text":"<p>Creates coco object from COCO formatted dict or COCO dataset file path.</p> <p>Parameters:</p> Name Type Description Default <code>coco_dict_or_path</code> \u00b6 <code>dict | str</code> <p>dict/str or List[dict/str] COCO formatted dict or COCO dataset file path List of COCO formatted dict or COCO dataset file path</p> required <code>image_dir</code> \u00b6 <code>str | None</code> <p>str Base file directory that contains dataset images. Required for merging and yolov5 conversion.</p> <code>None</code> <code>remapping_dict</code> \u00b6 <code>dict | None</code> <p>dict {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1</p> <code>None</code> <code>ignore_negative_samples</code> \u00b6 <code>bool</code> <p>bool If True ignores images without annotations in all operations.</p> <code>False</code> <code>clip_bboxes_to_img_dims</code> \u00b6 <code>bool</code> <p>bool = False Limits bounding boxes to image dimensions.</p> <code>False</code> <code>use_threads</code> \u00b6 <code>bool</code> <p>bool = False Use threads when processing the json image list, defaults to False</p> <code>False</code> <code>num_threads</code> \u00b6 <code>int</code> <p>int = 10 Slice the image list to given number of chunks, defaults to 10</p> <code>10</code> Properties <p>images: list of CocoImage category_mapping: dict</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_dict_or_path(\n    cls,\n    coco_dict_or_path: dict | str,\n    image_dir: str | None = None,\n    remapping_dict: dict | None = None,\n    ignore_negative_samples: bool = False,\n    clip_bboxes_to_img_dims: bool = False,\n    use_threads: bool = False,\n    num_threads: int = 10,\n):\n    \"\"\"Creates coco object from COCO formatted dict or COCO dataset file path.\n\n    Args:\n        coco_dict_or_path: dict/str or List[dict/str]\n            COCO formatted dict or COCO dataset file path\n            List of COCO formatted dict or COCO dataset file path\n        image_dir: str\n            Base file directory that contains dataset images. Required for merging and yolov5 conversion.\n        remapping_dict: dict\n            {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1\n        ignore_negative_samples: bool\n            If True ignores images without annotations in all operations.\n        clip_bboxes_to_img_dims: bool = False\n            Limits bounding boxes to image dimensions.\n        use_threads: bool = False\n            Use threads when processing the json image list, defaults to False\n        num_threads: int = 10\n            Slice the image list to given number of chunks, defaults to 10\n\n    Properties:\n        images: list of CocoImage\n        category_mapping: dict\n    \"\"\"\n    # init coco object\n    coco = cls(\n        image_dir=image_dir,\n        remapping_dict=remapping_dict,\n        ignore_negative_samples=ignore_negative_samples,\n        clip_bboxes_to_img_dims=clip_bboxes_to_img_dims,\n    )\n\n    if type(coco_dict_or_path) not in [str, dict]:\n        raise TypeError(\"coco_dict_or_path should be a dict or str\")\n\n    # load coco dict if path is given\n    if isinstance(coco_dict_or_path, str):\n        coco_dict = load_json(coco_dict_or_path)\n    else:\n        coco_dict = coco_dict_or_path\n\n    dict_size = len(coco_dict[\"images\"])\n\n    # arrange image id to annotation id mapping\n    coco.add_categories_from_coco_category_list(coco_dict[\"categories\"])\n    image_id_to_annotation_list = get_imageid2annotationlist_mapping(coco_dict)\n    category_mapping = coco.category_mapping\n\n    # https://github.com/obss/sahi/issues/98\n    image_id_set: set = set()\n\n    lock = Lock()\n\n    def fill_image_id_set(start, finish, image_list, _image_id_set, _image_id_to_annotation_list, _coco, lock):\n        for coco_image_dict in tqdm(\n            image_list[start:finish], f\"Loading coco annotations between {start} and {finish}\"\n        ):\n            coco_image = CocoImage.from_coco_image_dict(coco_image_dict)\n            image_id = coco_image_dict[\"id\"]\n            # https://github.com/obss/sahi/issues/98\n            if image_id in _image_id_set:\n                print(f\"duplicate image_id: {image_id}, will be ignored.\")\n                continue\n            else:\n                lock.acquire()\n                _image_id_set.add(image_id)\n                lock.release()\n\n            # select annotations of the image\n            annotation_list = _image_id_to_annotation_list[image_id]\n            for coco_annotation_dict in annotation_list:\n                # apply category remapping if remapping_dict is provided\n                if _coco.remapping_dict is not None:\n                    # apply category remapping (id:id)\n                    category_id = _coco.remapping_dict[coco_annotation_dict[\"category_id\"]]\n                    # update category id\n                    coco_annotation_dict[\"category_id\"] = category_id\n                else:\n                    category_id = coco_annotation_dict[\"category_id\"]\n                # get category name (id:name)\n                category_name = category_mapping[category_id]\n                coco_annotation = CocoAnnotation.from_coco_annotation_dict(\n                    category_name=category_name, annotation_dict=coco_annotation_dict\n                )\n                coco_image.add_annotation(coco_annotation)\n            _coco.add_image(coco_image)\n\n    chunk_size = dict_size / num_threads\n\n    if use_threads is True:\n        for i in range(num_threads):\n            start = i * chunk_size\n            finish = start + chunk_size\n            if finish &gt; dict_size:\n                finish = dict_size\n            t = Thread(\n                target=fill_image_id_set,\n                args=(start, finish, coco_dict[\"images\"], image_id_set, image_id_to_annotation_list, coco, lock),\n            )\n            t.start()\n\n        main_thread = threading.currentThread()\n        for t in threading.enumerate():\n            if t is not main_thread:\n                t.join()\n\n    else:\n        for coco_image_dict in tqdm(coco_dict[\"images\"], \"Loading coco annotations\"):\n            coco_image = CocoImage.from_coco_image_dict(coco_image_dict)\n            image_id = coco_image_dict[\"id\"]\n            # https://github.com/obss/sahi/issues/98\n            if image_id in image_id_set:\n                print(f\"duplicate image_id: {image_id}, will be ignored.\")\n                continue\n            else:\n                image_id_set.add(image_id)\n            # select annotations of the image\n            annotation_list = image_id_to_annotation_list[image_id]\n            # TODO: coco_annotation_dict is of type CocoAnnotation according to how image_id_to_annotation_list\n            # was created. Either image_id_to_annotation_list is not defined correctly or the following\n            # loop is wrong as it expects a dict.\n            for coco_annotation_dict in annotation_list:\n                # apply category remapping if remapping_dict is provided\n                if coco.remapping_dict is not None:\n                    # apply category remapping (id:id)\n                    category_id = coco.remapping_dict[coco_annotation_dict[\"category_id\"]]\n                    # update category id\n                    coco_annotation_dict[\"category_id\"] = category_id\n                else:\n                    category_id = coco_annotation_dict[\"category_id\"]\n                # get category name (id:name)\n                category_name = category_mapping[category_id]\n                coco_annotation = CocoAnnotation.from_coco_annotation_dict(\n                    category_name=category_name, annotation_dict=coco_annotation_dict\n                )\n                coco_image.add_annotation(coco_annotation)\n            coco.add_image(coco_image)\n\n    if clip_bboxes_to_img_dims:\n        coco = coco.get_coco_with_clipped_bboxes()\n    return coco\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.get_area_filtered_coco","title":"<code>get_area_filtered_coco(min=0, max_val=float('inf'), intervals_per_category=None)</code>","text":"<p>Filters annotation areas with given min and max values and returns remaining images as sahi.utils.coco.Coco object.</p> <p>Parameters:</p> Name Type Description Default <code>min</code> \u00b6 <p>int minimum allowed area</p> <code>0</code> <code>max_val</code> \u00b6 <p>int maximum allowed area</p> <code>float('inf')</code> <code>intervals_per_category</code> \u00b6 <p>dict of dicts {     \"human\": {\"min\": 20, \"max\": 10000},     \"vehicle\": {\"min\": 50, \"max\": 15000}, }</p> <code>None</code> <p>Returns:     area_filtered_coco: sahi.utils.coco.Coco</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def get_area_filtered_coco(self, min=0, max_val=float(\"inf\"), intervals_per_category=None):\n    \"\"\"Filters annotation areas with given min and max values and returns remaining images as sahi.utils.coco.Coco\n    object.\n\n    Args:\n        min: int\n            minimum allowed area\n        max_val: int\n            maximum allowed area\n        intervals_per_category: dict of dicts\n            {\n                \"human\": {\"min\": 20, \"max\": 10000},\n                \"vehicle\": {\"min\": 50, \"max\": 15000},\n            }\n    Returns:\n        area_filtered_coco: sahi.utils.coco.Coco\n    \"\"\"\n    area_filtered_coco = Coco(\n        name=self.name,\n        image_dir=self.image_dir,\n        remapping_dict=self.remapping_dict,\n        ignore_negative_samples=self.ignore_negative_samples,\n    )\n    area_filtered_coco.add_categories_from_coco_category_list(self.json_categories)\n    for image in self.images:\n        is_valid_image = True\n        for annotation in image.annotations:\n            if intervals_per_category is not None and annotation.category_name in intervals_per_category.keys():\n                category_based_min = intervals_per_category[annotation.category_name][\"min\"]\n                category_based_max = intervals_per_category[annotation.category_name][\"max\"]\n                if annotation.area &lt; category_based_min or annotation.area &gt; category_based_max:\n                    is_valid_image = False\n            if annotation.area &lt; min or annotation.area &gt; max_val:\n                is_valid_image = False\n        if is_valid_image:\n            area_filtered_coco.add_image(image)\n\n    return area_filtered_coco\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.get_coco_with_clipped_bboxes","title":"<code>get_coco_with_clipped_bboxes()</code>","text":"<p>Limits overflowing bounding boxes to image dimensions.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def get_coco_with_clipped_bboxes(self):\n    \"\"\"Limits overflowing bounding boxes to image dimensions.\"\"\"\n    from sahi.slicing import annotation_inside_slice\n\n    coco = Coco(\n        name=self.name,\n        image_dir=self.image_dir,\n        remapping_dict=self.remapping_dict,\n        ignore_negative_samples=self.ignore_negative_samples,\n    )\n    coco.add_categories_from_coco_category_list(self.json_categories)\n\n    for coco_img in self.images:\n        img_dims = [0, 0, coco_img.width, coco_img.height]\n        coco_image = CocoImage(\n            file_name=coco_img.file_name, height=coco_img.height, width=coco_img.width, id=coco_img.id\n        )\n        for coco_ann in coco_img.annotations:\n            ann_dict: dict = coco_ann.json\n            if annotation_inside_slice(annotation=ann_dict, slice_bbox=img_dims):\n                shapely_ann = coco_ann.get_sliced_coco_annotation(img_dims)\n                bbox = ShapelyAnnotation.to_xywh(shapely_ann._shapely_annotation)\n                coco_ann_from_shapely = CocoAnnotation(\n                    bbox=bbox,\n                    category_id=coco_ann.category_id,\n                    category_name=coco_ann.category_name,\n                    image_id=coco_ann.image_id,\n                )\n                coco_image.add_annotation(coco_ann_from_shapely)\n            else:\n                continue\n        coco.add_image(coco_image)\n    return coco\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.get_subsampled_coco","title":"<code>get_subsampled_coco(subsample_ratio=2, category_id=None)</code>","text":"<p>Subsamples images with subsample_ratio and returns as sahi.utils.coco.Coco object.</p> <p>Parameters:</p> Name Type Description Default <code>subsample_ratio</code> \u00b6 <code>int</code> <p>int 10 means take every 10th image with its annotations</p> <code>2</code> <code>category_id</code> \u00b6 <code>int | None</code> <p>int subsample only images containing given category_id, if -1 then subsamples negative samples</p> <code>None</code> <p>Returns:     subsampled_coco: sahi.utils.coco.Coco</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def get_subsampled_coco(self, subsample_ratio: int = 2, category_id: int | None = None):\n    \"\"\"Subsamples images with subsample_ratio and returns as sahi.utils.coco.Coco object.\n\n    Args:\n        subsample_ratio: int\n            10 means take every 10th image with its annotations\n        category_id: int\n            subsample only images containing given category_id, if -1 then subsamples negative samples\n    Returns:\n        subsampled_coco: sahi.utils.coco.Coco\n    \"\"\"\n    subsampled_coco = Coco(\n        name=self.name,\n        image_dir=self.image_dir,\n        remapping_dict=self.remapping_dict,\n        ignore_negative_samples=self.ignore_negative_samples,\n    )\n    subsampled_coco.add_categories_from_coco_category_list(self.json_categories)\n\n    if category_id is not None:\n        # get images that contain given category id\n        images_that_contain_category: list[CocoImage] = []\n        annotation: CocoAnnotation\n        for image in self.images:\n            category_id_to_contains = defaultdict(int)\n            for annotation in image.annotations:\n                category_id_to_contains[annotation.category_id] = 1\n            if category_id_to_contains[category_id]:\n                add_this_image = True\n            elif category_id == -1 and len(image.annotations) == 0:\n                # if category_id is given as -1, select negative samples\n                add_this_image = True\n            else:\n                add_this_image = False\n\n            if add_this_image:\n                images_that_contain_category.append(image)\n\n        # get images that does not contain given category id\n        images_that_doesnt_contain_category: list[CocoImage] = []\n        for image in self.images:\n            category_id_to_contains = defaultdict(int)\n            for annotation in image.annotations:\n                category_id_to_contains[annotation.category_id] = 1\n            if category_id_to_contains[category_id]:\n                add_this_image = False\n            elif category_id == -1 and len(image.annotations) == 0:\n                # if category_id is given as -1, dont select negative samples\n                add_this_image = False\n            else:\n                add_this_image = True\n\n            if add_this_image:\n                images_that_doesnt_contain_category.append(image)\n\n    if category_id:\n        selected_images = images_that_contain_category\n        # add images that does not contain given category without subsampling\n        for image_ind in range(len(images_that_doesnt_contain_category)):\n            subsampled_coco.add_image(images_that_doesnt_contain_category[image_ind])\n    else:\n        selected_images = self.images\n    for image_ind in range(0, len(selected_images), subsample_ratio):\n        subsampled_coco.add_image(selected_images[image_ind])\n\n    return subsampled_coco\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.get_upsampled_coco","title":"<code>get_upsampled_coco(upsample_ratio=2, category_id=None)</code>","text":"<p>Upsamples images with upsample_ratio and returns as sahi.utils.coco.Coco object.</p> <p>Parameters:</p> Name Type Description Default <code>upsample_ratio</code> \u00b6 <code>int</code> <p>int 10 means copy each sample 10 times</p> <code>2</code> <code>category_id</code> \u00b6 <code>int | None</code> <p>int upsample only images containing given category_id, if -1 then upsamples negative samples</p> <code>None</code> <p>Returns:     upsampled_coco: sahi.utils.coco.Coco</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def get_upsampled_coco(self, upsample_ratio: int = 2, category_id: int | None = None):\n    \"\"\"Upsamples images with upsample_ratio and returns as sahi.utils.coco.Coco object.\n\n    Args:\n        upsample_ratio: int\n            10 means copy each sample 10 times\n        category_id: int\n            upsample only images containing given category_id, if -1 then upsamples negative samples\n    Returns:\n        upsampled_coco: sahi.utils.coco.Coco\n    \"\"\"\n    upsampled_coco = Coco(\n        name=self.name,\n        image_dir=self.image_dir,\n        remapping_dict=self.remapping_dict,\n        ignore_negative_samples=self.ignore_negative_samples,\n    )\n    upsampled_coco.add_categories_from_coco_category_list(self.json_categories)\n    for ind in range(upsample_ratio):\n        for image_ind in range(len(self.images)):\n            # calculate add_this_image\n            if category_id is not None:\n                category_id_to_contains = defaultdict(int)\n                annotation: CocoAnnotation\n                for annotation in self.images[image_ind].annotations:\n                    category_id_to_contains[annotation.category_id] = 1\n                if category_id_to_contains[category_id]:\n                    add_this_image = True\n                elif category_id == -1 and len(self.images[image_ind].annotations) == 0:\n                    # if category_id is given as -1, select negative samples\n                    add_this_image = True\n                elif ind == 0:\n                    # in first iteration add all images\n                    add_this_image = True\n                else:\n                    add_this_image = False\n            else:\n                add_this_image = True\n\n            if add_this_image:\n                upsampled_coco.add_image(self.images[image_ind])\n\n    return upsampled_coco\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.merge","title":"<code>merge(coco, desired_name2id=None, verbose=1)</code>","text":"<p>Combines the images/annotations/categories of given coco object with current one.</p> <p>Parameters:</p> Name Type Description Default <code>coco </code> \u00b6 <p>sahi.utils.coco.Coco instance A COCO dataset object</p> required <code>desired_name2id </code> \u00b6 <p>dict</p> required <code>verbose</code> \u00b6 <p>bool If True, merging info is printed</p> <code>1</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def merge(self, coco, desired_name2id=None, verbose=1):\n    \"\"\"Combines the images/annotations/categories of given coco object with current one.\n\n    Args:\n        coco : sahi.utils.coco.Coco instance\n            A COCO dataset object\n        desired_name2id : dict\n            {\"human\": 1, \"car\": 2, \"big_vehicle\": 3}\n        verbose: bool\n            If True, merging info is printed\n    \"\"\"\n    if self.image_dir is None or coco.image_dir is None:\n        raise ValueError(\"image_dir should be provided for merging.\")\n    if verbose:\n        if not desired_name2id:\n            print(\"'desired_name2id' is not specified, combining all categories.\")\n\n    # create desired_name2id by combining all categories, if desired_name2id is not specified\n    coco1 = self\n    coco2 = coco\n    category_ind = 0\n    if desired_name2id is None:\n        desired_name2id = {}\n        for coco in [coco1, coco2]:\n            temp_categories = copy.deepcopy(coco.json_categories)\n            for temp_category in temp_categories:\n                if temp_category[\"name\"] not in desired_name2id:\n                    desired_name2id[temp_category[\"name\"]] = category_ind\n                    category_ind += 1\n                else:\n                    continue\n\n    # update categories and image paths\n    for coco in [coco1, coco2]:\n        coco.update_categories(desired_name2id=desired_name2id, update_image_filenames=True)\n\n    # combine images and categories\n    coco1.images.extend(coco2.images)\n    self.images: list[CocoImage] = coco1.images\n    self.categories = coco1.categories\n\n    # print categories\n    if verbose:\n        print(\n            \"Categories are formed as:\\n\",\n            self.json_categories,\n        )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.split_coco_as_train_val","title":"<code>split_coco_as_train_val(train_split_rate=0.9, numpy_seed=0)</code>","text":"<p>Split images into train-val and returns them as sahi.utils.coco.Coco objects.</p> <p>Parameters:</p> Name Type Description Default <code>train_split_rate</code> \u00b6 <p>float</p> <code>0.9</code> <code>numpy_seed</code> \u00b6 <p>int random seed. Actually, this doesn't use numpy, but the random package from the standard library, but it is called numpy for compatibility.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>result</code> <p>dict {     \"train_coco\": \"\",     \"val_coco\": \"\", }</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def split_coco_as_train_val(self, train_split_rate=0.9, numpy_seed=0):\n    \"\"\"Split images into train-val and returns them as sahi.utils.coco.Coco objects.\n\n    Args:\n        train_split_rate: float\n        numpy_seed: int\n            random seed. Actually, this doesn't use numpy, but the random package\n            from the standard library, but it is called numpy for compatibility.\n\n    Returns:\n        result : dict\n            {\n                \"train_coco\": \"\",\n                \"val_coco\": \"\",\n            }\n    \"\"\"\n    # divide images\n    num_images = len(self.images)\n    shuffled_images = copy.deepcopy(self.images)\n    random.seed(numpy_seed)\n    random.shuffle(shuffled_images)\n    num_train = int(num_images * train_split_rate)\n    train_images = shuffled_images[:num_train]\n    val_images = shuffled_images[num_train:]\n\n    # form train val coco objects\n    train_coco = Coco(\n        name=self.name if self.name else \"split\" + \"_train\",\n        image_dir=self.image_dir,\n    )\n    train_coco.images = train_images\n    train_coco.categories = self.categories\n\n    val_coco = Coco(name=self.name if self.name else \"split\" + \"_val\", image_dir=self.image_dir)\n    val_coco.images = val_images\n    val_coco.categories = self.categories\n\n    # return result\n    return {\n        \"train_coco\": train_coco,\n        \"val_coco\": val_coco,\n    }\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.Coco.update_categories","title":"<code>update_categories(desired_name2id, update_image_filenames=False)</code>","text":"<p>Rearranges category mapping of given COCO object based on given desired_name2id. Can also be used to filter some of the categories.</p> <p>Parameters:</p> Name Type Description Default <code>desired_name2id</code> \u00b6 <code>dict[str, int]</code> <p>dict</p> required <code>update_image_filenames</code> \u00b6 <code>bool</code> <p>bool If True, updates coco image file_names with absolute file paths.</p> <code>False</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def update_categories(self, desired_name2id: dict[str, int], update_image_filenames: bool = False):\n    \"\"\"Rearranges category mapping of given COCO object based on given desired_name2id. Can also be used to filter\n    some of the categories.\n\n    Args:\n        desired_name2id: dict\n            {\"big_vehicle\": 1, \"car\": 2, \"human\": 3}\n        update_image_filenames: bool\n            If True, updates coco image file_names with absolute file paths.\n    \"\"\"\n    # init vars\n    currentid2desiredid_mapping: dict[int, int | None] = {}\n    updated_coco = Coco(\n        name=self.name,\n        image_dir=self.image_dir,\n        remapping_dict=self.remapping_dict,\n        ignore_negative_samples=self.ignore_negative_samples,\n    )\n    # create category id mapping (currentid2desiredid_mapping)\n    for coco_category in self.categories:\n        current_category_id = coco_category.id\n        current_category_name = coco_category.name\n        if not current_category_name:\n            logger.warning(\"no category name provided to update categories\")\n            continue\n        if current_category_name in desired_name2id.keys():\n            currentid2desiredid_mapping[current_category_id] = desired_name2id[current_category_name]\n        else:\n            # ignore categories that are not included in desired_name2id\n            currentid2desiredid_mapping[current_category_id] = None\n\n    # add updated categories\n    for name in desired_name2id.keys():\n        updated_coco_category = CocoCategory(id=desired_name2id[name], name=name, supercategory=name)\n        updated_coco.add_category(updated_coco_category)\n\n    # add updated images &amp; annotations\n    for coco_image in copy.deepcopy(self.images):\n        updated_coco_image = CocoImage.from_coco_image_dict(coco_image.json)\n        # update filename to abspath\n        file_name_is_abspath = True if os.path.abspath(coco_image.file_name) == coco_image.file_name else False\n        if update_image_filenames and not file_name_is_abspath:\n            if not self.image_dir:\n                logger.error(\"image directory not set\")\n            else:\n                updated_coco_image.file_name = str(Path(os.path.abspath(self.image_dir)) / coco_image.file_name)\n        # update annotations\n        for coco_annotation in coco_image.annotations:\n            current_category_id = coco_annotation.category_id\n            desired_category_id = currentid2desiredid_mapping[current_category_id]\n            # append annotations with category id present in desired_name2id\n            if desired_category_id is not None:\n                # update cetegory id\n                coco_annotation.category_id = desired_category_id\n                # append updated annotation to target coco dict\n                updated_coco_image.add_annotation(coco_annotation)\n        updated_coco.add_image(updated_coco_image)\n\n    # overwrite instance\n    self.__dict__ = updated_coco.__dict__\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation","title":"<code>CocoAnnotation</code>","text":"<p>COCO formatted annotation.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoAnnotation:\n    \"\"\"COCO formatted annotation.\"\"\"\n\n    @classmethod\n    def from_coco_segmentation(cls, segmentation, category_id, category_name, iscrowd=0):\n        \"\"\"Creates CocoAnnotation object using coco segmentation.\n\n        Args:\n            segmentation: List[List]\n                [[1, 1, 325, 125, 250, 200, 5, 200]]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            iscrowd: int\n                0 or 1\n        \"\"\"\n        return cls(\n            segmentation=segmentation,\n            category_id=category_id,\n            category_name=category_name,\n            iscrowd=iscrowd,\n        )\n\n    @classmethod\n    def from_coco_bbox(cls, bbox, category_id, category_name, iscrowd=0):\n        \"\"\"Creates CocoAnnotation object using coco bbox.\n\n        Args:\n            bbox: List\n                [xmin, ymin, width, height]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            iscrowd: int\n                0 or 1\n        \"\"\"\n        return cls(\n            bbox=bbox,\n            category_id=category_id,\n            category_name=category_name,\n            iscrowd=iscrowd,\n        )\n\n    @classmethod\n    def from_coco_annotation_dict(cls, annotation_dict: dict, category_name: str | None = None):\n        \"\"\"Creates CocoAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\",\n        \"segmentation\", \"category_id\").\n\n        Args:\n            category_name: str\n                Category name of the annotation\n            annotation_dict: dict\n                COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")\n        \"\"\"\n        if annotation_dict.__contains__(\"segmentation\") and isinstance(annotation_dict[\"segmentation\"], dict):\n            has_rle_segmentation = True\n            logger.warning(\n                f\"Segmentation annotation for id {annotation_dict['id']} is skipped since \"\n                \"RLE segmentation format is not supported.\"\n            )\n        else:\n            has_rle_segmentation = False\n\n        if (\n            annotation_dict.__contains__(\"segmentation\")\n            and annotation_dict[\"segmentation\"]\n            and not has_rle_segmentation\n        ):\n            return cls(\n                segmentation=annotation_dict[\"segmentation\"],\n                category_id=annotation_dict[\"category_id\"],\n                category_name=category_name,\n            )\n        else:\n            return cls(\n                bbox=annotation_dict[\"bbox\"],\n                category_id=annotation_dict[\"category_id\"],\n                category_name=category_name,\n            )\n\n    @classmethod\n    def from_shapely_annotation(\n        cls,\n        shapely_annotation: ShapelyAnnotation,\n        category_id: int,\n        category_name: str,\n        iscrowd: int,\n    ):\n        \"\"\"Creates CocoAnnotation object from ShapelyAnnotation object.\n\n        Args:\n            shapely_annotation (ShapelyAnnotation)\n            category_id (int): Category id of the annotation\n            category_name (str): Category name of the annotation\n            iscrowd (int): 0 or 1\n        \"\"\"\n        coco_annotation = cls(\n            bbox=[0, 0, 0, 0],\n            category_id=category_id,\n            category_name=category_name,\n            iscrowd=iscrowd,\n        )\n        coco_annotation._segmentation = shapely_annotation.to_coco_segmentation()\n        coco_annotation._shapely_annotation = shapely_annotation\n        return coco_annotation\n\n    def __init__(\n        self,\n        category_id: int,\n        category_name: str | None = None,\n        segmentation=None,\n        bbox: list[int] | None = None,\n        image_id=None,\n        iscrowd=0,\n    ):\n        \"\"\"Creates coco annotation object using bbox or segmentation.\n\n        Args:\n            segmentation: List[List]\n                [[1, 1, 325, 125, 250, 200, 5, 200]]\n            bbox: List\n                [xmin, ymin, width, height]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            image_id: int\n                Image ID of the annotation\n            iscrowd: int\n                0 or 1\n        \"\"\"\n        if bbox is None and segmentation is None:\n            raise ValueError(\"you must provide a bbox or polygon\")\n\n        self._segmentation = segmentation\n        self._category_id = category_id\n        self._category_name = category_name\n        self._image_id = image_id\n        self._iscrowd = iscrowd\n\n        if self._segmentation:\n            shapely_annotation = ShapelyAnnotation.from_coco_segmentation(segmentation=self._segmentation)\n        else:\n            if not bbox:\n                raise TypeError(\"Coco bounding box not set\")\n            shapely_annotation = ShapelyAnnotation.from_coco_bbox(bbox=bbox)\n        self._shapely_annotation = shapely_annotation\n\n    def get_sliced_coco_annotation(self, slice_bbox: list[int]):\n        shapely_polygon = box(slice_bbox[0], slice_bbox[1], slice_bbox[2], slice_bbox[3])\n        intersection_shapely_annotation = self._shapely_annotation.get_intersection(shapely_polygon)\n        return CocoAnnotation.from_shapely_annotation(\n            intersection_shapely_annotation,\n            category_id=self.category_id,\n            category_name=self.category_name or \"\",\n            iscrowd=self.iscrowd,\n        )\n\n    @property\n    def area(self):\n        \"\"\"Returns area of annotation polygon (or bbox if no polygon available)\"\"\"\n        return self._shapely_annotation.area\n\n    @property\n    def bbox(self):\n        \"\"\"Returns coco formatted bbox of the annotation as [xmin, ymin, width, height]\"\"\"\n        return self._shapely_annotation.to_xywh()\n\n    @property\n    def segmentation(self):\n        \"\"\"Returns coco formatted segmentation of the annotation as [[1, 1, 325, 125, 250, 200, 5, 200]]\"\"\"\n        if self._segmentation:\n            return self._shapely_annotation.to_coco_segmentation()\n        else:\n            return []\n\n    @property\n    def category_id(self):\n        \"\"\"Returns category id of the annotation as int.\"\"\"\n        return self._category_id\n\n    @category_id.setter\n    def category_id(self, i):\n        if not isinstance(i, int):\n            raise Exception(\"category_id must be an integer\")\n        self._category_id = i\n\n    @property\n    def image_id(self):\n        \"\"\"Returns image id of the annotation as int.\"\"\"\n        return self._image_id\n\n    @image_id.setter\n    def image_id(self, i):\n        if not isinstance(i, int):\n            raise Exception(\"image_id must be an integer\")\n        self._image_id = i\n\n    @property\n    def category_name(self):\n        \"\"\"Returns category name of the annotation as str.\"\"\"\n        return self._category_name\n\n    @category_name.setter\n    def category_name(self, n):\n        if not isinstance(n, str):\n            raise Exception(\"category_name must be a string\")\n        self._category_name = n\n\n    @property\n    def iscrowd(self):\n        \"\"\"Returns iscrowd info of the annotation.\"\"\"\n        return self._iscrowd\n\n    @property\n    def json(self):\n        return {\n            \"image_id\": self.image_id,\n            \"bbox\": self.bbox,\n            \"category_id\": self.category_id,\n            \"segmentation\": self.segmentation,\n            \"iscrowd\": self.iscrowd,\n            \"area\": self.area,\n        }\n\n    def serialize(self):\n        warnings.warn(\"Use json property instead of serialize method\", DeprecationWarning, stacklevel=2)\n        return self.json\n\n    def __repr__(self):\n        return f\"\"\"CocoAnnotation&lt;\n    image_id: {self.image_id},\n    bbox: {self.bbox},\n    segmentation: {self.segmentation},\n    category_id: {self.category_id},\n    category_name: {self.category_name},\n    iscrowd: {self.iscrowd},\n    area: {self.area}&gt;\"\"\"\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation-attributes","title":"Attributes","text":""},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation.area","title":"<code>area</code>  <code>property</code>","text":"<p>Returns area of annotation polygon (or bbox if no polygon available)</p>"},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation.bbox","title":"<code>bbox</code>  <code>property</code>","text":"<p>Returns coco formatted bbox of the annotation as [xmin, ymin, width, height]</p>"},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation.category_id","title":"<code>category_id</code>  <code>property</code> <code>writable</code>","text":"<p>Returns category id of the annotation as int.</p>"},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation.category_name","title":"<code>category_name</code>  <code>property</code> <code>writable</code>","text":"<p>Returns category name of the annotation as str.</p>"},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation.image_id","title":"<code>image_id</code>  <code>property</code> <code>writable</code>","text":"<p>Returns image id of the annotation as int.</p>"},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation.iscrowd","title":"<code>iscrowd</code>  <code>property</code>","text":"<p>Returns iscrowd info of the annotation.</p>"},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation.segmentation","title":"<code>segmentation</code>  <code>property</code>","text":"<p>Returns coco formatted segmentation of the annotation as [[1, 1, 325, 125, 250, 200, 5, 200]]</p>"},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation-functions","title":"Functions","text":""},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation.__init__","title":"<code>__init__(category_id, category_name=None, segmentation=None, bbox=None, image_id=None, iscrowd=0)</code>","text":"<p>Creates coco annotation object using bbox or segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>segmentation</code> \u00b6 <p>List[List][[1, 1, 325, 125, 250, 200, 5, 200]]</p> <code>None</code> <code>bbox</code> \u00b6 <code>list[int] | None</code> <p>List [xmin, ymin, width, height]</p> <code>None</code> <code>category_id</code> \u00b6 <code>int</code> <p>int Category id of the annotation</p> required <code>category_name</code> \u00b6 <code>str | None</code> <p>str Category name of the annotation</p> <code>None</code> <code>image_id</code> \u00b6 <p>int Image ID of the annotation</p> <code>None</code> <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(\n    self,\n    category_id: int,\n    category_name: str | None = None,\n    segmentation=None,\n    bbox: list[int] | None = None,\n    image_id=None,\n    iscrowd=0,\n):\n    \"\"\"Creates coco annotation object using bbox or segmentation.\n\n    Args:\n        segmentation: List[List]\n            [[1, 1, 325, 125, 250, 200, 5, 200]]\n        bbox: List\n            [xmin, ymin, width, height]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        image_id: int\n            Image ID of the annotation\n        iscrowd: int\n            0 or 1\n    \"\"\"\n    if bbox is None and segmentation is None:\n        raise ValueError(\"you must provide a bbox or polygon\")\n\n    self._segmentation = segmentation\n    self._category_id = category_id\n    self._category_name = category_name\n    self._image_id = image_id\n    self._iscrowd = iscrowd\n\n    if self._segmentation:\n        shapely_annotation = ShapelyAnnotation.from_coco_segmentation(segmentation=self._segmentation)\n    else:\n        if not bbox:\n            raise TypeError(\"Coco bounding box not set\")\n        shapely_annotation = ShapelyAnnotation.from_coco_bbox(bbox=bbox)\n    self._shapely_annotation = shapely_annotation\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation.from_coco_annotation_dict","title":"<code>from_coco_annotation_dict(annotation_dict, category_name=None)</code>  <code>classmethod</code>","text":"<p>Creates CocoAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\").</p> <p>Parameters:</p> Name Type Description Default <code>category_name</code> \u00b6 <code>str | None</code> <p>str Category name of the annotation</p> <code>None</code> <code>annotation_dict</code> \u00b6 <code>dict</code> <p>dict COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_annotation_dict(cls, annotation_dict: dict, category_name: str | None = None):\n    \"\"\"Creates CocoAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\",\n    \"segmentation\", \"category_id\").\n\n    Args:\n        category_name: str\n            Category name of the annotation\n        annotation_dict: dict\n            COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")\n    \"\"\"\n    if annotation_dict.__contains__(\"segmentation\") and isinstance(annotation_dict[\"segmentation\"], dict):\n        has_rle_segmentation = True\n        logger.warning(\n            f\"Segmentation annotation for id {annotation_dict['id']} is skipped since \"\n            \"RLE segmentation format is not supported.\"\n        )\n    else:\n        has_rle_segmentation = False\n\n    if (\n        annotation_dict.__contains__(\"segmentation\")\n        and annotation_dict[\"segmentation\"]\n        and not has_rle_segmentation\n    ):\n        return cls(\n            segmentation=annotation_dict[\"segmentation\"],\n            category_id=annotation_dict[\"category_id\"],\n            category_name=category_name,\n        )\n    else:\n        return cls(\n            bbox=annotation_dict[\"bbox\"],\n            category_id=annotation_dict[\"category_id\"],\n            category_name=category_name,\n        )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation.from_coco_bbox","title":"<code>from_coco_bbox(bbox, category_id, category_name, iscrowd=0)</code>  <code>classmethod</code>","text":"<p>Creates CocoAnnotation object using coco bbox.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> \u00b6 <p>List [xmin, ymin, width, height]</p> required <code>category_id</code> \u00b6 <p>int Category id of the annotation</p> required <code>category_name</code> \u00b6 <p>str Category name of the annotation</p> required <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_bbox(cls, bbox, category_id, category_name, iscrowd=0):\n    \"\"\"Creates CocoAnnotation object using coco bbox.\n\n    Args:\n        bbox: List\n            [xmin, ymin, width, height]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        iscrowd: int\n            0 or 1\n    \"\"\"\n    return cls(\n        bbox=bbox,\n        category_id=category_id,\n        category_name=category_name,\n        iscrowd=iscrowd,\n    )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation.from_coco_segmentation","title":"<code>from_coco_segmentation(segmentation, category_id, category_name, iscrowd=0)</code>  <code>classmethod</code>","text":"<p>Creates CocoAnnotation object using coco segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>segmentation</code> \u00b6 <p>List[List][[1, 1, 325, 125, 250, 200, 5, 200]]</p> required <code>category_id</code> \u00b6 <p>int Category id of the annotation</p> required <code>category_name</code> \u00b6 <p>str Category name of the annotation</p> required <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_segmentation(cls, segmentation, category_id, category_name, iscrowd=0):\n    \"\"\"Creates CocoAnnotation object using coco segmentation.\n\n    Args:\n        segmentation: List[List]\n            [[1, 1, 325, 125, 250, 200, 5, 200]]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        iscrowd: int\n            0 or 1\n    \"\"\"\n    return cls(\n        segmentation=segmentation,\n        category_id=category_id,\n        category_name=category_name,\n        iscrowd=iscrowd,\n    )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoAnnotation.from_shapely_annotation","title":"<code>from_shapely_annotation(shapely_annotation, category_id, category_name, iscrowd)</code>  <code>classmethod</code>","text":"<p>Creates CocoAnnotation object from ShapelyAnnotation object.</p> <p>Parameters:</p> Name Type Description Default <code>category_id</code> \u00b6 <code>int</code> <p>Category id of the annotation</p> required <code>category_name</code> \u00b6 <code>str</code> <p>Category name of the annotation</p> required <code>iscrowd</code> \u00b6 <code>int</code> <p>0 or 1</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_shapely_annotation(\n    cls,\n    shapely_annotation: ShapelyAnnotation,\n    category_id: int,\n    category_name: str,\n    iscrowd: int,\n):\n    \"\"\"Creates CocoAnnotation object from ShapelyAnnotation object.\n\n    Args:\n        shapely_annotation (ShapelyAnnotation)\n        category_id (int): Category id of the annotation\n        category_name (str): Category name of the annotation\n        iscrowd (int): 0 or 1\n    \"\"\"\n    coco_annotation = cls(\n        bbox=[0, 0, 0, 0],\n        category_id=category_id,\n        category_name=category_name,\n        iscrowd=iscrowd,\n    )\n    coco_annotation._segmentation = shapely_annotation.to_coco_segmentation()\n    coco_annotation._shapely_annotation = shapely_annotation\n    return coco_annotation\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoCategory","title":"<code>CocoCategory</code>","text":"<p>COCO formatted category.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoCategory:\n    \"\"\"COCO formatted category.\"\"\"\n\n    def __init__(self, id: int = 0, name: str | None = None, supercategory: str | None = None):\n        self.id = int(id)\n        self.name = name\n        self.supercategory = supercategory if supercategory else name\n\n    @classmethod\n    def from_coco_category(cls, category):\n        \"\"\"Creates CocoCategory object using coco category.\n\n        Args:\n            category: Dict\n                {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},\n        \"\"\"\n        return cls(\n            id=category[\"id\"],\n            name=category[\"name\"],\n            supercategory=category[\"supercategory\"] if \"supercategory\" in category else category[\"name\"],\n        )\n\n    @property\n    def json(self):\n        return {\n            \"id\": self.id,\n            \"name\": self.name,\n            \"supercategory\": self.supercategory,\n        }\n\n    def __repr__(self):\n        return f\"\"\"CocoCategory&lt;\n    id: {self.id},\n    name: {self.name},\n    supercategory: {self.supercategory}&gt;\"\"\"\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoCategory-functions","title":"Functions","text":""},{"location":"utils/coco/#sahi.utils.coco.CocoCategory.from_coco_category","title":"<code>from_coco_category(category)</code>  <code>classmethod</code>","text":"<p>Creates CocoCategory object using coco category.</p> <p>Parameters:</p> Name Type Description Default <code>category</code> \u00b6 <p>Dict {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_category(cls, category):\n    \"\"\"Creates CocoCategory object using coco category.\n\n    Args:\n        category: Dict\n            {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},\n    \"\"\"\n    return cls(\n        id=category[\"id\"],\n        name=category[\"name\"],\n        supercategory=category[\"supercategory\"] if \"supercategory\" in category else category[\"name\"],\n    )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoImage","title":"<code>CocoImage</code>","text":"Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoImage:\n    @classmethod\n    def from_coco_image_dict(cls, image_dict):\n        \"\"\"Creates CocoImage object from COCO formatted image dict (with fields \"id\", \"file_name\", \"height\" and\n        \"weight\").\n\n        Args:\n            image_dict: dict\n                COCO formatted image dict (with fields \"id\", \"file_name\", \"height\" and \"weight\")\n        \"\"\"\n        return cls(\n            id=image_dict[\"id\"],\n            file_name=image_dict[\"file_name\"],\n            height=image_dict[\"height\"],\n            width=image_dict[\"width\"],\n        )\n\n    def __init__(self, file_name: str, height: int, width: int, id: int | None = None):\n        \"\"\"Creates CocoImage object.\n\n        Args:\n            id : int\n                Image id\n            file_name : str\n                Image path\n            height : int\n                Image height in pixels\n            width : int\n                Image width in pixels\n        \"\"\"\n        self.id = int(id) if id else id\n        self.file_name = file_name\n        self.height = int(height)\n        self.width = int(width)\n        self.annotations = []  # list of CocoAnnotation that belong to this image\n        self.predictions = []  # list of CocoPrediction that belong to this image\n\n    def add_annotation(self, annotation):\n        \"\"\"Adds annotation to this CocoImage instance.\n\n        annotation : CocoAnnotation\n        \"\"\"\n\n        if not isinstance(annotation, CocoAnnotation):\n            raise TypeError(\"annotation must be a CocoAnnotation instance\")\n        self.annotations.append(annotation)\n\n    def add_prediction(self, prediction):\n        \"\"\"Adds prediction to this CocoImage instance.\n\n        prediction : CocoPrediction\n        \"\"\"\n\n        if not isinstance(prediction, CocoPrediction):\n            raise TypeError(\"prediction must be a CocoPrediction instance\")\n        self.predictions.append(prediction)\n\n    @property\n    def json(self):\n        return {\n            \"id\": self.id,\n            \"file_name\": self.file_name,\n            \"height\": self.height,\n            \"width\": self.width,\n        }\n\n    def __repr__(self):\n        return f\"\"\"CocoImage&lt;\n    id: {self.id},\n    file_name: {self.file_name},\n    height: {self.height},\n    width: {self.width},\n    annotations: List[CocoAnnotation],\n    predictions: List[CocoPrediction]&gt;\"\"\"\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoImage-functions","title":"Functions","text":""},{"location":"utils/coco/#sahi.utils.coco.CocoImage.__init__","title":"<code>__init__(file_name, height, width, id=None)</code>","text":"<p>Creates CocoImage object.</p> <p>Parameters:</p> Name Type Description Default <code>id </code> \u00b6 <p>int Image id</p> required <code>file_name </code> \u00b6 <p>str Image path</p> required <code>height </code> \u00b6 <p>int Image height in pixels</p> required <code>width </code> \u00b6 <p>int Image width in pixels</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(self, file_name: str, height: int, width: int, id: int | None = None):\n    \"\"\"Creates CocoImage object.\n\n    Args:\n        id : int\n            Image id\n        file_name : str\n            Image path\n        height : int\n            Image height in pixels\n        width : int\n            Image width in pixels\n    \"\"\"\n    self.id = int(id) if id else id\n    self.file_name = file_name\n    self.height = int(height)\n    self.width = int(width)\n    self.annotations = []  # list of CocoAnnotation that belong to this image\n    self.predictions = []  # list of CocoPrediction that belong to this image\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoImage.add_annotation","title":"<code>add_annotation(annotation)</code>","text":"<p>Adds annotation to this CocoImage instance.</p> <p>annotation : CocoAnnotation</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_annotation(self, annotation):\n    \"\"\"Adds annotation to this CocoImage instance.\n\n    annotation : CocoAnnotation\n    \"\"\"\n\n    if not isinstance(annotation, CocoAnnotation):\n        raise TypeError(\"annotation must be a CocoAnnotation instance\")\n    self.annotations.append(annotation)\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoImage.add_prediction","title":"<code>add_prediction(prediction)</code>","text":"<p>Adds prediction to this CocoImage instance.</p> <p>prediction : CocoPrediction</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_prediction(self, prediction):\n    \"\"\"Adds prediction to this CocoImage instance.\n\n    prediction : CocoPrediction\n    \"\"\"\n\n    if not isinstance(prediction, CocoPrediction):\n        raise TypeError(\"prediction must be a CocoPrediction instance\")\n    self.predictions.append(prediction)\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoImage.from_coco_image_dict","title":"<code>from_coco_image_dict(image_dict)</code>  <code>classmethod</code>","text":"<p>Creates CocoImage object from COCO formatted image dict (with fields \"id\", \"file_name\", \"height\" and \"weight\").</p> <p>Parameters:</p> Name Type Description Default <code>image_dict</code> \u00b6 <p>dict COCO formatted image dict (with fields \"id\", \"file_name\", \"height\" and \"weight\")</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_image_dict(cls, image_dict):\n    \"\"\"Creates CocoImage object from COCO formatted image dict (with fields \"id\", \"file_name\", \"height\" and\n    \"weight\").\n\n    Args:\n        image_dict: dict\n            COCO formatted image dict (with fields \"id\", \"file_name\", \"height\" and \"weight\")\n    \"\"\"\n    return cls(\n        id=image_dict[\"id\"],\n        file_name=image_dict[\"file_name\"],\n        height=image_dict[\"height\"],\n        width=image_dict[\"width\"],\n    )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoPrediction","title":"<code>CocoPrediction</code>","text":"<p>               Bases: <code>CocoAnnotation</code></p> <p>Class for handling predictions in coco format.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoPrediction(CocoAnnotation):\n    \"\"\"Class for handling predictions in coco format.\"\"\"\n\n    @classmethod\n    def from_coco_segmentation(cls, segmentation, category_id, category_name, score, iscrowd=0, image_id=None):\n        \"\"\"Creates CocoAnnotation object using coco segmentation.\n\n        Args:\n            segmentation: List[List]\n                [[1, 1, 325, 125, 250, 200, 5, 200]]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            score: float\n                Prediction score between 0 and 1\n            iscrowd: int\n                0 or 1\n        \"\"\"\n        return cls(\n            segmentation=segmentation,\n            category_id=category_id,\n            category_name=category_name,\n            score=score,\n            iscrowd=iscrowd,\n            image_id=image_id,\n        )\n\n    @classmethod\n    def from_coco_bbox(cls, bbox, category_id, category_name, score, iscrowd=0, image_id=None):\n        \"\"\"Creates CocoAnnotation object using coco bbox.\n\n        Args:\n            bbox: List\n                [xmin, ymin, width, height]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            score: float\n                Prediction score between 0 and 1\n            iscrowd: int\n                0 or 1\n        \"\"\"\n        return cls(\n            bbox=bbox,\n            category_id=category_id,\n            category_name=category_name,\n            score=score,\n            iscrowd=iscrowd,\n            image_id=image_id,\n        )\n\n    @classmethod\n    def from_coco_annotation_dict(cls, category_name, annotation_dict, score, image_id=None):\n        \"\"\"Creates CocoAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\",\n        \"segmentation\", \"category_id\").\n\n        Args:\n            category_name: str\n                Category name of the annotation\n            annotation_dict: dict\n                COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")\n            score: float\n                Prediction score between 0 and 1\n        \"\"\"\n        if annotation_dict[\"segmentation\"]:\n            return cls(\n                segmentation=annotation_dict[\"segmentation\"],\n                category_id=annotation_dict[\"category_id\"],\n                category_name=category_name,\n                score=score,\n                image_id=image_id,\n            )\n        else:\n            return cls(\n                bbox=annotation_dict[\"bbox\"],\n                category_id=annotation_dict[\"category_id\"],\n                category_name=category_name,\n                image_id=image_id,\n            )\n\n    def __init__(\n        self,\n        segmentation=None,\n        bbox=None,\n        category_id: int = 0,\n        category_name: str = \"\",\n        image_id=None,\n        score=None,\n        iscrowd=0,\n    ):\n        \"\"\"\n\n        Args:\n            segmentation: List[List]\n                [[1, 1, 325, 125, 250, 200, 5, 200]]\n            bbox: List\n                [xmin, ymin, width, height]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            image_id: int\n                Image ID of the annotation\n            score: float\n                Prediction score between 0 and 1\n            iscrowd: int\n                0 or 1\n        \"\"\"\n        self.score = score\n        super().__init__(\n            segmentation=segmentation,\n            bbox=bbox,\n            category_id=category_id,\n            category_name=category_name,\n            image_id=image_id,\n            iscrowd=iscrowd,\n        )\n\n    @property\n    def json(self):\n        return {\n            \"image_id\": self.image_id,\n            \"bbox\": self.bbox,\n            \"score\": self.score,\n            \"category_id\": self.category_id,\n            \"category_name\": self.category_name,\n            \"segmentation\": self.segmentation,\n            \"iscrowd\": self.iscrowd,\n            \"area\": self.area,\n        }\n\n    def serialize(self):\n        warnings.warn(\"Use json property instead of serialize method\", DeprecationWarning, stacklevel=2)\n\n    def __repr__(self):\n        return f\"\"\"CocoPrediction&lt;\n    image_id: {self.image_id},\n    bbox: {self.bbox},\n    segmentation: {self.segmentation},\n    score: {self.score},\n    category_id: {self.category_id},\n    category_name: {self.category_name},\n    iscrowd: {self.iscrowd},\n    area: {self.area}&gt;\"\"\"\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoPrediction-functions","title":"Functions","text":""},{"location":"utils/coco/#sahi.utils.coco.CocoPrediction.__init__","title":"<code>__init__(segmentation=None, bbox=None, category_id=0, category_name='', image_id=None, score=None, iscrowd=0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>segmentation</code> \u00b6 <p>List[List][[1, 1, 325, 125, 250, 200, 5, 200]]</p> <code>None</code> <code>bbox</code> \u00b6 <p>List [xmin, ymin, width, height]</p> <code>None</code> <code>category_id</code> \u00b6 <code>int</code> <p>int Category id of the annotation</p> <code>0</code> <code>category_name</code> \u00b6 <code>str</code> <p>str Category name of the annotation</p> <code>''</code> <code>image_id</code> \u00b6 <p>int Image ID of the annotation</p> <code>None</code> <code>score</code> \u00b6 <p>float Prediction score between 0 and 1</p> <code>None</code> <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(\n    self,\n    segmentation=None,\n    bbox=None,\n    category_id: int = 0,\n    category_name: str = \"\",\n    image_id=None,\n    score=None,\n    iscrowd=0,\n):\n    \"\"\"\n\n    Args:\n        segmentation: List[List]\n            [[1, 1, 325, 125, 250, 200, 5, 200]]\n        bbox: List\n            [xmin, ymin, width, height]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        image_id: int\n            Image ID of the annotation\n        score: float\n            Prediction score between 0 and 1\n        iscrowd: int\n            0 or 1\n    \"\"\"\n    self.score = score\n    super().__init__(\n        segmentation=segmentation,\n        bbox=bbox,\n        category_id=category_id,\n        category_name=category_name,\n        image_id=image_id,\n        iscrowd=iscrowd,\n    )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoPrediction.from_coco_annotation_dict","title":"<code>from_coco_annotation_dict(category_name, annotation_dict, score, image_id=None)</code>  <code>classmethod</code>","text":"<p>Creates CocoAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\").</p> <p>Parameters:</p> Name Type Description Default <code>category_name</code> \u00b6 <p>str Category name of the annotation</p> required <code>annotation_dict</code> \u00b6 <p>dict COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")</p> required <code>score</code> \u00b6 <p>float Prediction score between 0 and 1</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_annotation_dict(cls, category_name, annotation_dict, score, image_id=None):\n    \"\"\"Creates CocoAnnotation object from category name and COCO formatted annotation dict (with fields \"bbox\",\n    \"segmentation\", \"category_id\").\n\n    Args:\n        category_name: str\n            Category name of the annotation\n        annotation_dict: dict\n            COCO formatted annotation dict (with fields \"bbox\", \"segmentation\", \"category_id\")\n        score: float\n            Prediction score between 0 and 1\n    \"\"\"\n    if annotation_dict[\"segmentation\"]:\n        return cls(\n            segmentation=annotation_dict[\"segmentation\"],\n            category_id=annotation_dict[\"category_id\"],\n            category_name=category_name,\n            score=score,\n            image_id=image_id,\n        )\n    else:\n        return cls(\n            bbox=annotation_dict[\"bbox\"],\n            category_id=annotation_dict[\"category_id\"],\n            category_name=category_name,\n            image_id=image_id,\n        )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoPrediction.from_coco_bbox","title":"<code>from_coco_bbox(bbox, category_id, category_name, score, iscrowd=0, image_id=None)</code>  <code>classmethod</code>","text":"<p>Creates CocoAnnotation object using coco bbox.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> \u00b6 <p>List [xmin, ymin, width, height]</p> required <code>category_id</code> \u00b6 <p>int Category id of the annotation</p> required <code>category_name</code> \u00b6 <p>str Category name of the annotation</p> required <code>score</code> \u00b6 <p>float Prediction score between 0 and 1</p> required <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_bbox(cls, bbox, category_id, category_name, score, iscrowd=0, image_id=None):\n    \"\"\"Creates CocoAnnotation object using coco bbox.\n\n    Args:\n        bbox: List\n            [xmin, ymin, width, height]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        score: float\n            Prediction score between 0 and 1\n        iscrowd: int\n            0 or 1\n    \"\"\"\n    return cls(\n        bbox=bbox,\n        category_id=category_id,\n        category_name=category_name,\n        score=score,\n        iscrowd=iscrowd,\n        image_id=image_id,\n    )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoPrediction.from_coco_segmentation","title":"<code>from_coco_segmentation(segmentation, category_id, category_name, score, iscrowd=0, image_id=None)</code>  <code>classmethod</code>","text":"<p>Creates CocoAnnotation object using coco segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>segmentation</code> \u00b6 <p>List[List][[1, 1, 325, 125, 250, 200, 5, 200]]</p> required <code>category_id</code> \u00b6 <p>int Category id of the annotation</p> required <code>category_name</code> \u00b6 <p>str Category name of the annotation</p> required <code>score</code> \u00b6 <p>float Prediction score between 0 and 1</p> required <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_segmentation(cls, segmentation, category_id, category_name, score, iscrowd=0, image_id=None):\n    \"\"\"Creates CocoAnnotation object using coco segmentation.\n\n    Args:\n        segmentation: List[List]\n            [[1, 1, 325, 125, 250, 200, 5, 200]]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        score: float\n            Prediction score between 0 and 1\n        iscrowd: int\n            0 or 1\n    \"\"\"\n    return cls(\n        segmentation=segmentation,\n        category_id=category_id,\n        category_name=category_name,\n        score=score,\n        iscrowd=iscrowd,\n        image_id=image_id,\n    )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVid","title":"<code>CocoVid</code>","text":"Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoVid:\n    def __init__(self, name=None, remapping_dict=None):\n        \"\"\"Creates CocoVid object.\n\n        Args:\n            name: str\n                Name of the CocoVid dataset, it determines exported json name.\n            remapping_dict: dict\n                {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1\n        \"\"\"\n        self.name = name\n        self.remapping_dict = remapping_dict\n        self.categories = []\n        self.videos = []\n\n    def add_categories_from_coco_category_list(self, coco_category_list):\n        \"\"\"Creates CocoCategory object using coco category list.\n\n        Args:\n            coco_category_list: List[Dict]\n                [\n                    {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},\n                    {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"}\n                ]\n        \"\"\"\n\n        for coco_category in coco_category_list:\n            if self.remapping_dict is not None:\n                for source_id in self.remapping_dict.keys():\n                    if coco_category[\"id\"] == source_id:\n                        target_id = self.remapping_dict[source_id]\n                        coco_category[\"id\"] = target_id\n\n            self.add_category(CocoCategory.from_coco_category(coco_category))\n\n    def add_category(self, category: CocoCategory):\n        \"\"\"Adds category to this CocoVid instance.\n\n        Args:\n            category: CocoCategory\n        \"\"\"\n\n        if not isinstance(category, CocoCategory):\n            raise TypeError(\"category must be a CocoCategory instance\")  # type: ignore\n        self.categories.append(category)\n\n    @property\n    def json_categories(self):\n        categories = []\n        for category in self.categories:\n            categories.append(category.json)\n        return categories\n\n    @property\n    def category_mapping(self):\n        category_mapping = {}\n        for category in self.categories:\n            category_mapping[category.id] = category.name\n        return category_mapping\n\n    def add_video(self, video: CocoVideo):\n        \"\"\"Adds video to this CocoVid instance.\n\n        Args:\n            video: CocoVideo\n        \"\"\"\n\n        if not isinstance(video, CocoVideo):\n            raise TypeError(\"video must be a CocoVideo instance\")  # type: ignore\n        self.videos.append(video)\n\n    @property\n    def json(self):\n        coco_dict = {\n            \"videos\": [],\n            \"images\": [],\n            \"annotations\": [],\n            \"categories\": self.json_categories,\n        }\n        annotation_id = 1\n        image_id = 1\n        video_id = 1\n        global_instance_id = 1\n        for coco_video in self.videos:\n            coco_video.id = video_id\n            coco_dict[\"videos\"].append(coco_video.json)\n\n            frame_id = 0\n            instance_id_set = set()\n            for cocovid_image in coco_video.images:\n                cocovid_image.id = image_id\n                cocovid_image.frame_id = frame_id\n                cocovid_image.video_id = coco_video.id\n                coco_dict[\"images\"].append(cocovid_image.json)\n\n                for cocovid_annotation in cocovid_image.annotations:\n                    instance_id_set.add(cocovid_annotation.instance_id)\n                    cocovid_annotation.instance_id += global_instance_id\n\n                    cocovid_annotation.id = annotation_id\n                    cocovid_annotation.image_id = cocovid_image.id\n                    coco_dict[\"annotations\"].append(cocovid_annotation.json)\n\n                    # increment annotation_id\n                    annotation_id = copy.deepcopy(annotation_id + 1)\n                # increment image_id and frame_id\n                image_id = copy.deepcopy(image_id + 1)\n                frame_id = copy.deepcopy(frame_id + 1)\n            # increment video_id and global_instance_id\n            video_id = copy.deepcopy(video_id + 1)\n            global_instance_id += len(instance_id_set)\n\n        return coco_dict\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVid-functions","title":"Functions","text":""},{"location":"utils/coco/#sahi.utils.coco.CocoVid.__init__","title":"<code>__init__(name=None, remapping_dict=None)</code>","text":"<p>Creates CocoVid object.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> \u00b6 <p>str Name of the CocoVid dataset, it determines exported json name.</p> <code>None</code> <code>remapping_dict</code> \u00b6 <p>dict {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1</p> <code>None</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(self, name=None, remapping_dict=None):\n    \"\"\"Creates CocoVid object.\n\n    Args:\n        name: str\n            Name of the CocoVid dataset, it determines exported json name.\n        remapping_dict: dict\n            {1:0, 2:1} maps category id 1 to 0 and category id 2 to 1\n    \"\"\"\n    self.name = name\n    self.remapping_dict = remapping_dict\n    self.categories = []\n    self.videos = []\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVid.add_categories_from_coco_category_list","title":"<code>add_categories_from_coco_category_list(coco_category_list)</code>","text":"<p>Creates CocoCategory object using coco category list.</p> <p>Parameters:</p> Name Type Description Default <code>coco_category_list</code> \u00b6 <p>List[Dict] [     {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},     {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"} ]</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_categories_from_coco_category_list(self, coco_category_list):\n    \"\"\"Creates CocoCategory object using coco category list.\n\n    Args:\n        coco_category_list: List[Dict]\n            [\n                {\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\"},\n                {\"supercategory\": \"vehicle\", \"id\": 2, \"name\": \"bicycle\"}\n            ]\n    \"\"\"\n\n    for coco_category in coco_category_list:\n        if self.remapping_dict is not None:\n            for source_id in self.remapping_dict.keys():\n                if coco_category[\"id\"] == source_id:\n                    target_id = self.remapping_dict[source_id]\n                    coco_category[\"id\"] = target_id\n\n        self.add_category(CocoCategory.from_coco_category(coco_category))\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVid.add_category","title":"<code>add_category(category)</code>","text":"<p>Adds category to this CocoVid instance.</p> <p>Parameters:</p> Name Type Description Default <code>category</code> \u00b6 <code>CocoCategory</code> <p>CocoCategory</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_category(self, category: CocoCategory):\n    \"\"\"Adds category to this CocoVid instance.\n\n    Args:\n        category: CocoCategory\n    \"\"\"\n\n    if not isinstance(category, CocoCategory):\n        raise TypeError(\"category must be a CocoCategory instance\")  # type: ignore\n    self.categories.append(category)\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVid.add_video","title":"<code>add_video(video)</code>","text":"<p>Adds video to this CocoVid instance.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> \u00b6 <code>CocoVideo</code> <p>CocoVideo</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_video(self, video: CocoVideo):\n    \"\"\"Adds video to this CocoVid instance.\n\n    Args:\n        video: CocoVideo\n    \"\"\"\n\n    if not isinstance(video, CocoVideo):\n        raise TypeError(\"video must be a CocoVideo instance\")  # type: ignore\n    self.videos.append(video)\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVidAnnotation","title":"<code>CocoVidAnnotation</code>","text":"<p>               Bases: <code>CocoAnnotation</code></p> <p>COCOVid formatted annotation.</p> <p>https://github.com/open-mmlab/mmtracking/blob/master/docs/tutorials/customize_dataset.md#the-cocovid-annotation-file</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoVidAnnotation(CocoAnnotation):\n    \"\"\"COCOVid formatted annotation.\n\n    https://github.com/open-mmlab/mmtracking/blob/master/docs/tutorials/customize_dataset.md#the-cocovid-annotation-file\n    \"\"\"\n\n    def __init__(\n        self,\n        category_id: int,\n        category_name: str,\n        bbox: list[int],\n        image_id=None,\n        instance_id=None,\n        iscrowd=0,\n        id=None,\n    ):\n        \"\"\"\n        Args:\n            bbox: List\n                [xmin, ymin, width, height]\n            category_id: int\n                Category id of the annotation\n            category_name: str\n                Category name of the annotation\n            image_id: int\n                Image ID of the annotation\n            instance_id: int\n                Used for tracking\n            iscrowd: int\n                0 or 1\n            id: int\n                Annotation id\n        \"\"\"\n        super().__init__(\n            bbox=bbox,\n            category_id=category_id,\n            category_name=category_name,\n            image_id=image_id,\n            iscrowd=iscrowd,\n        )\n        self.instance_id = instance_id\n        self.id = id\n\n    @property\n    def json(self):\n        return {\n            \"id\": self.id,\n            \"image_id\": self.image_id,\n            \"bbox\": self.bbox,\n            \"segmentation\": self.segmentation,\n            \"category_id\": self.category_id,\n            \"category_name\": self.category_name,\n            \"instance_id\": self.instance_id,\n            \"iscrowd\": self.iscrowd,\n            \"area\": self.area,\n        }\n\n    def __repr__(self):\n        return f\"\"\"CocoAnnotation&lt;\n    id: {self.id},\n    image_id: {self.image_id},\n    bbox: {self.bbox},\n    segmentation: {self.segmentation},\n    category_id: {self.category_id},\n    category_name: {self.category_name},\n    instance_id: {self.instance_id},\n    iscrowd: {self.iscrowd},\n    area: {self.area}&gt;\"\"\"\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVidAnnotation-functions","title":"Functions","text":""},{"location":"utils/coco/#sahi.utils.coco.CocoVidAnnotation.__init__","title":"<code>__init__(category_id, category_name, bbox, image_id=None, instance_id=None, iscrowd=0, id=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>bbox</code> \u00b6 <code>list[int]</code> <p>List [xmin, ymin, width, height]</p> required <code>category_id</code> \u00b6 <code>int</code> <p>int Category id of the annotation</p> required <code>category_name</code> \u00b6 <code>str</code> <p>str Category name of the annotation</p> required <code>image_id</code> \u00b6 <p>int Image ID of the annotation</p> <code>None</code> <code>instance_id</code> \u00b6 <p>int Used for tracking</p> <code>None</code> <code>iscrowd</code> \u00b6 <p>int 0 or 1</p> <code>0</code> <code>id</code> \u00b6 <p>int Annotation id</p> <code>None</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(\n    self,\n    category_id: int,\n    category_name: str,\n    bbox: list[int],\n    image_id=None,\n    instance_id=None,\n    iscrowd=0,\n    id=None,\n):\n    \"\"\"\n    Args:\n        bbox: List\n            [xmin, ymin, width, height]\n        category_id: int\n            Category id of the annotation\n        category_name: str\n            Category name of the annotation\n        image_id: int\n            Image ID of the annotation\n        instance_id: int\n            Used for tracking\n        iscrowd: int\n            0 or 1\n        id: int\n            Annotation id\n    \"\"\"\n    super().__init__(\n        bbox=bbox,\n        category_id=category_id,\n        category_name=category_name,\n        image_id=image_id,\n        iscrowd=iscrowd,\n    )\n    self.instance_id = instance_id\n    self.id = id\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVidImage","title":"<code>CocoVidImage</code>","text":"<p>               Bases: <code>CocoImage</code></p> <p>COCOVid formatted image.</p> <p>https://github.com/open-mmlab/mmtracking/blob/master/docs/tutorials/customize_dataset.md#the-cocovid-annotation-file</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoVidImage(CocoImage):\n    \"\"\"COCOVid formatted image.\n\n    https://github.com/open-mmlab/mmtracking/blob/master/docs/tutorials/customize_dataset.md#the-cocovid-annotation-file\n    \"\"\"\n\n    def __init__(\n        self,\n        file_name,\n        height,\n        width,\n        video_id=None,\n        frame_id=None,\n        id=None,\n    ):\n        \"\"\"Creates CocoVidImage object.\n\n        Args:\n            id: int\n                Image id\n            file_name: str\n                Image path\n            height: int\n                Image height in pixels\n            width: int\n                Image width in pixels\n            frame_id: int\n                0-indexed frame id\n            video_id: int\n                Video id\n        \"\"\"\n        super().__init__(file_name=file_name, height=height, width=width, id=id)\n        self.frame_id = frame_id\n        self.video_id = video_id\n\n    @classmethod\n    def from_coco_image(cls, coco_image, video_id=None, frame_id=None):\n        \"\"\"Creates CocoVidImage object using CocoImage object.\n\n        Args:\n            coco_image: CocoImage\n            frame_id: int\n                0-indexed frame id\n            video_id: int\n                Video id\n        \"\"\"\n        return cls(\n            file_name=coco_image.file_name,\n            height=coco_image.height,\n            width=coco_image.width,\n            id=coco_image.id,\n            video_id=video_id,\n            frame_id=frame_id,\n        )\n\n    def add_annotation(self, annotation):\n        \"\"\"\n        Adds annotation to this CocoImage instance\n        annotation : CocoVidAnnotation\n        \"\"\"\n\n        if not isinstance(annotation, CocoVidAnnotation):\n            raise TypeError(\"annotation must be a CocoVidAnnotation instance\")\n        self.annotations.append(annotation)\n\n    @property\n    def json(self):\n        return {\n            \"file_name\": self.file_name,\n            \"height\": self.height,\n            \"width\": self.width,\n            \"id\": self.id,\n            \"video_id\": self.video_id,\n            \"frame_id\": self.frame_id,\n        }\n\n    def __repr__(self):\n        return f\"\"\"CocoVidImage&lt;\n    file_name: {self.file_name},\n    height: {self.height},\n    width: {self.width},\n    id: {self.id},\n    video_id: {self.video_id},\n    frame_id: {self.frame_id},\n    annotations: List[CocoVidAnnotation]&gt;\"\"\"\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVidImage-functions","title":"Functions","text":""},{"location":"utils/coco/#sahi.utils.coco.CocoVidImage.__init__","title":"<code>__init__(file_name, height, width, video_id=None, frame_id=None, id=None)</code>","text":"<p>Creates CocoVidImage object.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> \u00b6 <p>int Image id</p> <code>None</code> <code>file_name</code> \u00b6 <p>str Image path</p> required <code>height</code> \u00b6 <p>int Image height in pixels</p> required <code>width</code> \u00b6 <p>int Image width in pixels</p> required <code>frame_id</code> \u00b6 <p>int 0-indexed frame id</p> <code>None</code> <code>video_id</code> \u00b6 <p>int Video id</p> <code>None</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(\n    self,\n    file_name,\n    height,\n    width,\n    video_id=None,\n    frame_id=None,\n    id=None,\n):\n    \"\"\"Creates CocoVidImage object.\n\n    Args:\n        id: int\n            Image id\n        file_name: str\n            Image path\n        height: int\n            Image height in pixels\n        width: int\n            Image width in pixels\n        frame_id: int\n            0-indexed frame id\n        video_id: int\n            Video id\n    \"\"\"\n    super().__init__(file_name=file_name, height=height, width=width, id=id)\n    self.frame_id = frame_id\n    self.video_id = video_id\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVidImage.add_annotation","title":"<code>add_annotation(annotation)</code>","text":"<p>Adds annotation to this CocoImage instance annotation : CocoVidAnnotation</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_annotation(self, annotation):\n    \"\"\"\n    Adds annotation to this CocoImage instance\n    annotation : CocoVidAnnotation\n    \"\"\"\n\n    if not isinstance(annotation, CocoVidAnnotation):\n        raise TypeError(\"annotation must be a CocoVidAnnotation instance\")\n    self.annotations.append(annotation)\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVidImage.from_coco_image","title":"<code>from_coco_image(coco_image, video_id=None, frame_id=None)</code>  <code>classmethod</code>","text":"<p>Creates CocoVidImage object using CocoImage object.</p> <p>Parameters:</p> Name Type Description Default <code>coco_image</code> \u00b6 <p>CocoImage</p> required <code>frame_id</code> \u00b6 <p>int 0-indexed frame id</p> <code>None</code> <code>video_id</code> \u00b6 <p>int Video id</p> <code>None</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>@classmethod\ndef from_coco_image(cls, coco_image, video_id=None, frame_id=None):\n    \"\"\"Creates CocoVidImage object using CocoImage object.\n\n    Args:\n        coco_image: CocoImage\n        frame_id: int\n            0-indexed frame id\n        video_id: int\n            Video id\n    \"\"\"\n    return cls(\n        file_name=coco_image.file_name,\n        height=coco_image.height,\n        width=coco_image.width,\n        id=coco_image.id,\n        video_id=video_id,\n        frame_id=frame_id,\n    )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVideo","title":"<code>CocoVideo</code>","text":"<p>COCO formatted video.</p> <p>https://github.com/open-mmlab/mmtracking/blob/master/docs/tutorials/customize_dataset.md#the-cocovid-annotation-file</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>class CocoVideo:\n    \"\"\"COCO formatted video.\n\n    https://github.com/open-mmlab/mmtracking/blob/master/docs/tutorials/customize_dataset.md#the-cocovid-annotation-file\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        id: int | None = None,\n        fps: float | None = None,\n        height: int | None = None,\n        width: int | None = None,\n    ):\n        \"\"\"Creates CocoVideo object.\n\n        Args:\n            name: str\n                Video name\n            id: int\n                Video id\n            fps: float\n                Video fps\n            height: int\n                Video height in pixels\n            width: int\n                Video width in pixels\n        \"\"\"\n        self.name = name\n        self.id = id\n        self.fps = fps\n        self.height = height\n        self.width = width\n        self.images = []  # list of CocoImage that belong to this video\n\n    def add_image(self, image):\n        \"\"\"\n        Adds image to this CocoVideo instance\n        Args:\n            image: CocoImage\n        \"\"\"\n\n        if not isinstance(image, CocoImage):\n            raise TypeError(\"image must be a CocoImage instance\")\n        self.images.append(CocoVidImage.from_coco_image(image))\n\n    def add_cocovidimage(self, cocovidimage):\n        \"\"\"\n        Adds CocoVidImage to this CocoVideo instance\n        Args:\n            cocovidimage: CocoVidImage\n        \"\"\"\n\n        if not isinstance(cocovidimage, CocoVidImage):\n            raise TypeError(\"cocovidimage must be a CocoVidImage instance\")\n        self.images.append(cocovidimage)\n\n    @property\n    def json(self):\n        return {\n            \"name\": self.name,\n            \"id\": self.id,\n            \"fps\": self.fps,\n            \"height\": self.height,\n            \"width\": self.width,\n        }\n\n    def __repr__(self):\n        return f\"\"\"CocoVideo&lt;\n    id: {self.id},\n    name: {self.name},\n    fps: {self.fps},\n    height: {self.height},\n    width: {self.width},\n    images: List[CocoVidImage]&gt;\"\"\"\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVideo-functions","title":"Functions","text":""},{"location":"utils/coco/#sahi.utils.coco.CocoVideo.__init__","title":"<code>__init__(name, id=None, fps=None, height=None, width=None)</code>","text":"<p>Creates CocoVideo object.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> \u00b6 <code>str</code> <p>str Video name</p> required <code>id</code> \u00b6 <code>int | None</code> <p>int Video id</p> <code>None</code> <code>fps</code> \u00b6 <code>float | None</code> <p>float Video fps</p> <code>None</code> <code>height</code> \u00b6 <code>int | None</code> <p>int Video height in pixels</p> <code>None</code> <code>width</code> \u00b6 <code>int | None</code> <p>int Video width in pixels</p> <code>None</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    id: int | None = None,\n    fps: float | None = None,\n    height: int | None = None,\n    width: int | None = None,\n):\n    \"\"\"Creates CocoVideo object.\n\n    Args:\n        name: str\n            Video name\n        id: int\n            Video id\n        fps: float\n            Video fps\n        height: int\n            Video height in pixels\n        width: int\n            Video width in pixels\n    \"\"\"\n    self.name = name\n    self.id = id\n    self.fps = fps\n    self.height = height\n    self.width = width\n    self.images = []  # list of CocoImage that belong to this video\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVideo.add_cocovidimage","title":"<code>add_cocovidimage(cocovidimage)</code>","text":"<p>Adds CocoVidImage to this CocoVideo instance Args:     cocovidimage: CocoVidImage</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_cocovidimage(self, cocovidimage):\n    \"\"\"\n    Adds CocoVidImage to this CocoVideo instance\n    Args:\n        cocovidimage: CocoVidImage\n    \"\"\"\n\n    if not isinstance(cocovidimage, CocoVidImage):\n        raise TypeError(\"cocovidimage must be a CocoVidImage instance\")\n    self.images.append(cocovidimage)\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.CocoVideo.add_image","title":"<code>add_image(image)</code>","text":"<p>Adds image to this CocoVideo instance Args:     image: CocoImage</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_image(self, image):\n    \"\"\"\n    Adds image to this CocoVideo instance\n    Args:\n        image: CocoImage\n    \"\"\"\n\n    if not isinstance(image, CocoImage):\n        raise TypeError(\"image must be a CocoImage instance\")\n    self.images.append(CocoVidImage.from_coco_image(image))\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.DatasetClassCounts","title":"<code>DatasetClassCounts</code>  <code>dataclass</code>","text":"<p>Stores the number of images that include each category in a dataset.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>@dataclass\nclass DatasetClassCounts:\n    \"\"\"Stores the number of images that include each category in a dataset.\"\"\"\n\n    counts: dict\n    total_images: int\n\n    def frequencies(self):\n        \"\"\"Calculates the frequency of images that contain each category.\"\"\"\n        return {cid: count / self.total_images for cid, count in self.counts.items()}\n\n    def __add__(self, o):\n        total = self.total_images + o.total_images\n        exclusive_keys = set(o.counts.keys()) - set(self.counts.keys())\n        counts = {}\n        for k, v in self.counts.items():\n            counts[k] = v + o.counts.get(k, 0)\n        for k in exclusive_keys:\n            counts[k] = o.counts[k]\n        return DatasetClassCounts(counts, total)\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.DatasetClassCounts-functions","title":"Functions","text":""},{"location":"utils/coco/#sahi.utils.coco.DatasetClassCounts.frequencies","title":"<code>frequencies()</code>","text":"<p>Calculates the frequency of images that contain each category.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def frequencies(self):\n    \"\"\"Calculates the frequency of images that contain each category.\"\"\"\n    return {cid: count / self.total_images for cid, count in self.counts.items()}\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco-functions","title":"Functions","text":""},{"location":"utils/coco/#sahi.utils.coco.add_bbox_and_area_to_coco","title":"<code>add_bbox_and_area_to_coco(source_coco_path='', target_coco_path='', add_bbox=True, add_area=True)</code>","text":"<p>Takes single coco dataset file path, calculates and fills bbox and area fields of the annotations and exports the updated coco dict.</p> <p>coco_dict : dict     Updated coco dict</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def add_bbox_and_area_to_coco(\n    source_coco_path: str = \"\",\n    target_coco_path: str = \"\",\n    add_bbox: bool = True,\n    add_area: bool = True,\n) -&gt; dict:\n    \"\"\"Takes single coco dataset file path, calculates and fills bbox and area fields of the annotations and exports the\n    updated coco dict.\n\n    Returns:\n    coco_dict : dict\n        Updated coco dict\n    \"\"\"\n    coco_dict = load_json(source_coco_path)\n    coco_dict = copy.deepcopy(coco_dict)\n\n    annotations = coco_dict[\"annotations\"]\n    for ind, annotation in enumerate(annotations):\n        # assign annotation bbox\n        if add_bbox:\n            coco_polygons = []\n            [coco_polygons.extend(coco_polygon) for coco_polygon in annotation[\"segmentation\"]]\n            minx, miny, maxx, maxy = list(\n                [\n                    min(coco_polygons[0::2]),\n                    min(coco_polygons[1::2]),\n                    max(coco_polygons[0::2]),\n                    max(coco_polygons[1::2]),\n                ]\n            )\n            x, y, width, height = (\n                minx,\n                miny,\n                maxx - minx,\n                maxy - miny,\n            )\n            annotations[ind][\"bbox\"] = [x, y, width, height]\n\n        # assign annotation area\n        if add_area:\n            shapely_multipolygon = get_shapely_multipolygon(coco_segmentation=annotation[\"segmentation\"])\n            annotations[ind][\"area\"] = shapely_multipolygon.area\n\n    coco_dict[\"annotations\"] = annotations\n    save_json(coco_dict, target_coco_path)\n    return coco_dict\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.count_images_with_category","title":"<code>count_images_with_category(coco_file_path)</code>","text":"<p>Reads a coco dataset file and returns an DatasetClassCounts object  that stores the number of images that include each category in a dataset Returns: DatasetClassCounts object coco_file_path : str     path to coco dataset file</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def count_images_with_category(coco_file_path):\n    \"\"\"Reads a coco dataset file and returns an DatasetClassCounts object\n     that stores the number of images that include each category in a dataset\n    Returns: DatasetClassCounts object\n    coco_file_path : str\n        path to coco dataset file\n    \"\"\"\n\n    image_id_2_category_2_count = defaultdict(lambda: defaultdict(int))\n    coco = load_json(coco_file_path)\n    for annotation in coco[\"annotations\"]:\n        image_id = annotation[\"image_id\"]\n        cid = annotation[\"category_id\"]\n        image_id_2_category_2_count[image_id][cid] = image_id_2_category_2_count[image_id][cid] + 1\n\n    category_2_count = defaultdict(int)\n    for image_id, image_category_2_count in image_id_2_category_2_count.items():\n        for cid, count in image_category_2_count.items():\n            if count &gt; 0:\n                category_2_count[cid] = category_2_count[cid] + 1\n\n    category_2_count = dict(category_2_count)\n    total_images = len(image_id_2_category_2_count.keys())\n    return DatasetClassCounts(category_2_count, total_images)\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.create_coco_dict","title":"<code>create_coco_dict(images, categories, ignore_negative_samples=False, image_id_setting='auto')</code>","text":"<p>Creates COCO dict with fields \"images\", \"annotations\", \"categories\".</p> <p>Args</p> <pre><code>images : List of CocoImage containing a list of CocoAnnotation\ncategories : List of Dict\n    COCO categories\nignore_negative_samples : Bool\n    If True, images without annotations are ignored\nimage_id_setting: str\n    how to assign image ids while exporting can be\n        auto --&gt; will assign id from scratch (&lt;CocoImage&gt;.id will be ignored)\n        manual --&gt; you will need to provide image ids in &lt;CocoImage&gt; instances (&lt;CocoImage&gt;.id can not be None)\n</code></pre> <p>Returns</p> <pre><code>coco_dict : Dict\n    COCO dict with fields \"images\", \"annotations\", \"categories\"\n</code></pre> Source code in <code>sahi/utils/coco.py</code> <pre><code>def create_coco_dict(images, categories, ignore_negative_samples=False, image_id_setting=\"auto\"):\n    \"\"\"Creates COCO dict with fields \"images\", \"annotations\", \"categories\".\n\n    Args\n\n        images : List of CocoImage containing a list of CocoAnnotation\n        categories : List of Dict\n            COCO categories\n        ignore_negative_samples : Bool\n            If True, images without annotations are ignored\n        image_id_setting: str\n            how to assign image ids while exporting can be\n                auto --&gt; will assign id from scratch (&lt;CocoImage&gt;.id will be ignored)\n                manual --&gt; you will need to provide image ids in &lt;CocoImage&gt; instances (&lt;CocoImage&gt;.id can not be None)\n    Returns\n\n        coco_dict : Dict\n            COCO dict with fields \"images\", \"annotations\", \"categories\"\n    \"\"\"\n    # assertion of parameters\n    if image_id_setting not in [\"auto\", \"manual\"]:\n        raise ValueError(\"'image_id_setting' should be one of ['auto', 'manual']\")\n\n    # define accumulators\n    image_index = 1\n    annotation_id = 1\n    coco_dict = dict(images=[], annotations=[], categories=categories)\n    for coco_image in images:\n        # get coco annotations\n        coco_annotations = coco_image.annotations\n        # get num annotations\n        num_annotations = len(coco_annotations)\n        # if ignore_negative_samples is True and no annotations, skip image\n        if ignore_negative_samples and num_annotations == 0:\n            continue\n        else:\n            # get image_id\n            if image_id_setting == \"auto\":\n                image_id = image_index\n                image_index += 1\n            elif image_id_setting == \"manual\":\n                if coco_image.id is None:\n                    raise ValueError(\"'coco_image.id' should be set manually when image_id_setting == 'manual'\")\n                image_id = coco_image.id\n\n            # create coco image object\n            out_image = {\n                \"height\": coco_image.height,\n                \"width\": coco_image.width,\n                \"id\": image_id,\n                \"file_name\": coco_image.file_name,\n            }\n            coco_dict[\"images\"].append(out_image)\n\n            # do the same for image annotations\n            for coco_annotation in coco_annotations:\n                # create coco annotation object\n                out_annotation = {\n                    \"iscrowd\": 0,\n                    \"image_id\": image_id,\n                    \"bbox\": coco_annotation.bbox,\n                    \"segmentation\": coco_annotation.segmentation,\n                    \"category_id\": coco_annotation.category_id,\n                    \"id\": annotation_id,\n                    \"area\": coco_annotation.area,\n                }\n                coco_dict[\"annotations\"].append(out_annotation)\n                # increment annotation id\n                annotation_id += 1\n\n    # return coco dict\n    return coco_dict\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.create_coco_prediction_array","title":"<code>create_coco_prediction_array(images, ignore_negative_samples=False, image_id_setting='auto')</code>","text":"<p>Creates COCO prediction array which is list of predictions.</p> <p>Args</p> <pre><code>images : List of CocoImage containing a list of CocoAnnotation\nignore_negative_samples : Bool\n    If True, images without predictions are ignored\nimage_id_setting: str\n    how to assign image ids while exporting can be\n        auto --&gt; will assign id from scratch (&lt;CocoImage&gt;.id will be ignored)\n        manual --&gt; you will need to provide image ids in &lt;CocoImage&gt; instances (&lt;CocoImage&gt;.id can not be None)\n</code></pre> <p>Returns</p> <pre><code>coco_prediction_array : List\n    COCO predictions array\n</code></pre> Source code in <code>sahi/utils/coco.py</code> <pre><code>def create_coco_prediction_array(images, ignore_negative_samples=False, image_id_setting=\"auto\"):\n    \"\"\"Creates COCO prediction array which is list of predictions.\n\n    Args\n\n        images : List of CocoImage containing a list of CocoAnnotation\n        ignore_negative_samples : Bool\n            If True, images without predictions are ignored\n        image_id_setting: str\n            how to assign image ids while exporting can be\n                auto --&gt; will assign id from scratch (&lt;CocoImage&gt;.id will be ignored)\n                manual --&gt; you will need to provide image ids in &lt;CocoImage&gt; instances (&lt;CocoImage&gt;.id can not be None)\n    Returns\n\n        coco_prediction_array : List\n            COCO predictions array\n    \"\"\"\n    # assertion of parameters\n    if image_id_setting not in [\"auto\", \"manual\"]:\n        raise ValueError(\"'image_id_setting' should be one of ['auto', 'manual']\")\n    # define accumulators\n    image_index = 1\n    prediction_id = 1\n    predictions_array = []\n    for coco_image in images:\n        # get coco predictions\n        coco_predictions = coco_image.predictions\n        # get num predictions\n        num_predictions = len(coco_predictions)\n        # if ignore_negative_samples is True and no annotations, skip image\n        if ignore_negative_samples and num_predictions == 0:\n            continue\n        else:\n            # get image_id\n            if image_id_setting == \"auto\":\n                image_id = image_index\n                image_index += 1\n            elif image_id_setting == \"manual\":\n                if coco_image.id is None:\n                    raise ValueError(\"'coco_image.id' should be set manually when image_id_setting == 'manual'\")\n                image_id = coco_image.id\n\n            # create coco prediction object\n            for prediction_index, coco_prediction in enumerate(coco_predictions):\n                # create coco prediction object\n                out_prediction = {\n                    \"id\": prediction_id,\n                    \"image_id\": image_id,\n                    \"bbox\": coco_prediction.bbox,\n                    \"score\": coco_prediction.score,\n                    \"category_id\": coco_prediction.category_id,\n                    \"segmentation\": coco_prediction.segmentation,\n                    \"iscrowd\": coco_prediction.iscrowd,\n                    \"area\": coco_prediction.area,\n                }\n                predictions_array.append(out_prediction)\n\n                # increment prediction id\n                prediction_id += 1\n\n    # return predictions array\n    return predictions_array\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolo","title":"<code>export_coco_as_yolo(output_dir, train_coco=None, val_coco=None, train_split_rate=0.9, numpy_seed=0, disable_symlink=False)</code>","text":"<p>Exports current COCO dataset in ultralytics/YOLO format. Creates train val folders with image symlinks and txt files and a data yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>str Export directory.</p> required <code>Coco | None</code> <p>Coco coco object for training</p> <code>None</code> <code>Coco | None</code> <p>Coco coco object for val</p> <code>None</code> <code>float</code> <p>float train split rate between 0 and 1. will be used when val_coco is None.</p> <code>0.9</code> <p>int To fix the numpy seed.</p> <code>0</code> <p>bool If True, copy images instead of creating symlinks.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>yaml_path</code> <p>str Path for the exported YOLO data.yml</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_coco_as_yolo(\n    output_dir: str,\n    train_coco: Coco | None = None,\n    val_coco: Coco | None = None,\n    train_split_rate: float = 0.9,\n    numpy_seed=0,\n    disable_symlink=False,\n):\n    \"\"\"Exports current COCO dataset in ultralytics/YOLO format. Creates train val folders with image symlinks and txt\n    files and a data yaml file.\n\n    Args:\n        output_dir: str\n            Export directory.\n        train_coco: Coco\n            coco object for training\n        val_coco: Coco\n            coco object for val\n        train_split_rate: float\n            train split rate between 0 and 1. will be used when val_coco is None.\n        numpy_seed: int\n            To fix the numpy seed.\n        disable_symlink: bool\n            If True, copy images instead of creating symlinks.\n\n    Returns:\n        yaml_path: str\n            Path for the exported YOLO data.yml\n    \"\"\"\n    try:\n        import yaml\n    except ImportError:\n        raise ImportError('Please run \"pip install -U pyyaml\" to install yaml first for YOLO formatted exporting.')\n\n    # set split_mode\n    if train_coco and not val_coco:\n        split_mode = True\n    elif train_coco and val_coco:\n        split_mode = False\n    else:\n        raise ValueError(\"'train_coco' have to be provided\")\n\n    # check train_split_rate\n    if split_mode and not (0 &lt; train_split_rate &lt; 1):\n        raise ValueError(\"train_split_rate cannot be &lt;0 or &gt;1\")\n\n    # split dataset\n    if split_mode:\n        result = train_coco.split_coco_as_train_val(\n            train_split_rate=train_split_rate,\n            numpy_seed=numpy_seed,\n        )\n        train_coco = result[\"train_coco\"]\n        val_coco = result[\"val_coco\"]\n\n    # create train val image dirs\n    train_dir = Path(os.path.abspath(output_dir)) / \"train/\"\n    train_dir.mkdir(parents=True, exist_ok=True)  # create dir\n    val_dir = Path(os.path.abspath(output_dir)) / \"val/\"\n    val_dir.mkdir(parents=True, exist_ok=True)  # create dir\n\n    # create image symlinks and annotation txts\n    export_yolo_images_and_txts_from_coco_object(\n        output_dir=train_dir,\n        coco=train_coco,\n        ignore_negative_samples=train_coco.ignore_negative_samples,\n        mp=False,\n        disable_symlink=disable_symlink,\n    )\n    assert val_coco, \"Validation Coco object not set\"\n    export_yolo_images_and_txts_from_coco_object(\n        output_dir=val_dir,\n        coco=val_coco,\n        ignore_negative_samples=val_coco.ignore_negative_samples,\n        mp=False,\n        disable_symlink=disable_symlink,\n    )\n\n    # create yolov5 data yaml\n    data = {\n        \"train\": str(train_dir).replace(\"\\\\\", \"/\"),\n        \"val\": str(val_dir).replace(\"\\\\\", \"/\"),\n        \"nc\": len(train_coco.category_mapping),\n        \"names\": list(train_coco.category_mapping.values()),\n    }\n    yaml_path = str(Path(output_dir) / \"data.yml\")\n    with open(yaml_path, \"w\") as outfile:\n        yaml.dump(data, outfile, default_flow_style=False)\n\n    return yaml_path\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolo(output_dir)","title":"<code>output_dir</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolo(train_coco)","title":"<code>train_coco</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolo(val_coco)","title":"<code>val_coco</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolo(train_split_rate)","title":"<code>train_split_rate</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolo(numpy_seed)","title":"<code>numpy_seed</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolo(disable_symlink)","title":"<code>disable_symlink</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolo_via_yml","title":"<code>export_coco_as_yolo_via_yml(yml_path, output_dir, train_split_rate=0.9, numpy_seed=0, disable_symlink=False)</code>","text":"<p>Exports current COCO dataset in ultralytics/YOLO format. Creates train val folders with image symlinks and txt files and a data yaml file. Uses a yml file as input.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>str file should contain these fields:     train_json_path: str     train_image_dir: str     val_json_path: str     val_image_dir: str</p> required <code>str</code> <p>str Export directory.</p> required <code>float</code> <p>float train split rate between 0 and 1. will be used when val_json_path is None.</p> <code>0.9</code> <p>int To fix the numpy seed.</p> <code>0</code> <p>bool If True, copy images instead of creating symlinks.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>yaml_path</code> <p>str Path for the exported YOLO data.yml</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_coco_as_yolo_via_yml(\n    yml_path: str, output_dir: str, train_split_rate: float = 0.9, numpy_seed=0, disable_symlink=False\n):\n    \"\"\"Exports current COCO dataset in ultralytics/YOLO format. Creates train val folders with image symlinks and txt\n    files and a data yaml file. Uses a yml file as input.\n\n    Args:\n        yml_path: str\n            file should contain these fields:\n                train_json_path: str\n                train_image_dir: str\n                val_json_path: str\n                val_image_dir: str\n        output_dir: str\n            Export directory.\n        train_split_rate: float\n            train split rate between 0 and 1. will be used when val_json_path is None.\n        numpy_seed: int\n            To fix the numpy seed.\n        disable_symlink: bool\n            If True, copy images instead of creating symlinks.\n\n    Returns:\n        yaml_path: str\n            Path for the exported YOLO data.yml\n    \"\"\"\n    try:\n        import yaml\n    except ImportError:\n        raise ImportError('Please run \"pip install -U pyyaml\" to install yaml first for YOLO formatted exporting.')\n\n    with open(yml_path) as stream:\n        config_dict = yaml.safe_load(stream)\n\n    if config_dict[\"train_json_path\"]:\n        if not config_dict[\"train_image_dir\"]:\n            raise ValueError(f\"{yml_path} is missing `train_image_dir`\")\n        train_coco = Coco.from_coco_dict_or_path(\n            config_dict[\"train_json_path\"], image_dir=config_dict[\"train_image_dir\"]\n        )\n    else:\n        train_coco = None\n\n    if config_dict[\"val_json_path\"]:\n        if not config_dict[\"val_image_dir\"]:\n            raise ValueError(f\"{yml_path} is missing `val_image_dir`\")\n        val_coco = Coco.from_coco_dict_or_path(config_dict[\"val_json_path\"], image_dir=config_dict[\"val_image_dir\"])\n    else:\n        val_coco = None\n\n    yaml_path = export_coco_as_yolo(\n        output_dir=output_dir,\n        train_coco=train_coco,\n        val_coco=val_coco,\n        train_split_rate=train_split_rate,\n        numpy_seed=numpy_seed,\n        disable_symlink=disable_symlink,\n    )\n\n    return yaml_path\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolo_via_yml(yml_path)","title":"<code>yml_path</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolo_via_yml(output_dir)","title":"<code>output_dir</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolo_via_yml(train_split_rate)","title":"<code>train_split_rate</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolo_via_yml(numpy_seed)","title":"<code>numpy_seed</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolo_via_yml(disable_symlink)","title":"<code>disable_symlink</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolov5","title":"<code>export_coco_as_yolov5(output_dir, train_coco=None, val_coco=None, train_split_rate=0.9, numpy_seed=0, disable_symlink=False)</code>","text":"<p>Deprecated.</p> <p>Please use export_coco_as_yolo instead. Calls export_coco_as_yolo with the same arguments.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_coco_as_yolov5(\n    output_dir: str,\n    train_coco: Coco | None = None,\n    val_coco: Coco | None = None,\n    train_split_rate: float = 0.9,\n    numpy_seed=0,\n    disable_symlink=False,\n):\n    \"\"\"Deprecated.\n\n    Please use export_coco_as_yolo instead. Calls export_coco_as_yolo with the same arguments.\n    \"\"\"\n    warnings.warn(\n        \"export_coco_as_yolov5 is deprecated. Please use export_coco_as_yolo instead.\",\n        DeprecationWarning,\n    )\n    export_coco_as_yolo(\n        output_dir=output_dir,\n        train_coco=train_coco,\n        val_coco=val_coco,\n        train_split_rate=train_split_rate,\n        numpy_seed=numpy_seed,\n        disable_symlink=disable_symlink,\n    )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.export_coco_as_yolov5_via_yml","title":"<code>export_coco_as_yolov5_via_yml(yml_path, output_dir, train_split_rate=0.9, numpy_seed=0, disable_symlink=False)</code>","text":"<p>Deprecated.</p> <p>Please use export_coco_as_yolo_via_yml instead. Calls export_coco_as_yolo_via_yml with the same arguments.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_coco_as_yolov5_via_yml(\n    yml_path: str, output_dir: str, train_split_rate: float = 0.9, numpy_seed=0, disable_symlink=False\n):\n    \"\"\"Deprecated.\n\n    Please use export_coco_as_yolo_via_yml instead. Calls export_coco_as_yolo_via_yml with the same arguments.\n    \"\"\"\n    warnings.warn(\n        \"export_coco_as_yolov5_via_yml is deprecated. Please use export_coco_as_yolo_via_yml instead.\",\n        DeprecationWarning,\n    )\n    export_coco_as_yolo_via_yml(\n        yml_path=yml_path,\n        output_dir=output_dir,\n        train_split_rate=train_split_rate,\n        numpy_seed=numpy_seed,\n        disable_symlink=disable_symlink,\n    )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.export_single_yolo_image_and_corresponding_txt","title":"<code>export_single_yolo_image_and_corresponding_txt(coco_image, coco_image_dir, output_dir, ignore_negative_samples=False, disable_symlink=False)</code>","text":"<p>Generates YOLO formatted image symlink and annotation txt file.</p> <p>Parameters:</p> Name Type Description Default <p>sahi.utils.coco.CocoImage</p> required <p>str</p> required <p>str Export directory.</p> required <p>bool If True ignores images without annotations in all operations.</p> <code>False</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_single_yolo_image_and_corresponding_txt(\n    coco_image, coco_image_dir, output_dir, ignore_negative_samples=False, disable_symlink=False\n):\n    \"\"\"Generates YOLO formatted image symlink and annotation txt file.\n\n    Args:\n        coco_image: sahi.utils.coco.CocoImage\n        coco_image_dir: str\n        output_dir: str\n            Export directory.\n        ignore_negative_samples: bool\n            If True ignores images without annotations in all operations.\n    \"\"\"\n    # if coco_image contains any invalid annotations, skip it\n    contains_invalid_annotations = False\n    for coco_annotation in coco_image.annotations:\n        if len(coco_annotation.bbox) != 4:\n            contains_invalid_annotations = True\n            break\n    if contains_invalid_annotations:\n        return\n    # skip images without annotations\n    if len(coco_image.annotations) == 0 and ignore_negative_samples:\n        return\n    # skip images without suffix\n    # https://github.com/obss/sahi/issues/114\n    if Path(coco_image.file_name).suffix == \"\":\n        print(f\"image file has no suffix, skipping it: '{coco_image.file_name}'\")\n        return\n    elif Path(coco_image.file_name).suffix in [\".txt\"]:  # TODO: extend this list\n        print(f\"image file has incorrect suffix, skipping it: '{coco_image.file_name}'\")\n        return\n    # set coco and yolo image paths\n    if Path(coco_image.file_name).is_file():\n        coco_image_path = os.path.abspath(coco_image.file_name)\n    else:\n        if coco_image_dir is None:\n            raise ValueError(\"You have to specify image_dir of Coco object for yolo conversion.\")\n\n        coco_image_path = os.path.abspath(str(Path(coco_image_dir) / coco_image.file_name))\n\n    yolo_image_path_temp = str(Path(output_dir) / Path(coco_image.file_name).name)\n    # increment target file name if already present\n    yolo_image_path = copy.deepcopy(yolo_image_path_temp)\n    name_increment = 2\n    while Path(yolo_image_path).is_file():\n        parent_dir = Path(yolo_image_path_temp).parent\n        filename = Path(yolo_image_path_temp).stem\n        filesuffix = Path(yolo_image_path_temp).suffix\n        filename = filename + \"_\" + str(name_increment)\n        yolo_image_path = str(parent_dir / (filename + filesuffix))\n        name_increment += 1\n    # create a symbolic link pointing to coco_image_path named yolo_image_path\n    if disable_symlink:\n        import shutil\n\n        shutil.copy(coco_image_path, yolo_image_path)\n    else:\n        os.symlink(coco_image_path, yolo_image_path)\n    # calculate annotation normalization ratios\n    width = coco_image.width\n    height = coco_image.height\n    dw = 1.0 / (width)\n    dh = 1.0 / (height)\n    # set annotation filepath\n    image_file_suffix = Path(yolo_image_path).suffix\n    yolo_annotation_path = yolo_image_path.replace(image_file_suffix, \".txt\")\n    # create annotation file\n    annotations = coco_image.annotations\n    with open(yolo_annotation_path, \"w\") as outfile:\n        for annotation in annotations:\n            # convert coco bbox to yolo bbox\n            x_center = annotation.bbox[0] + annotation.bbox[2] / 2.0\n            y_center = annotation.bbox[1] + annotation.bbox[3] / 2.0\n            bbox_width = annotation.bbox[2]\n            bbox_height = annotation.bbox[3]\n            x_center = x_center * dw\n            y_center = y_center * dh\n            bbox_width = bbox_width * dw\n            bbox_height = bbox_height * dh\n            category_id = annotation.category_id\n            yolo_bbox = (x_center, y_center, bbox_width, bbox_height)\n            # save yolo annotation\n            outfile.write(str(category_id) + \" \" + \" \".join([str(value) for value in yolo_bbox]) + \"\\n\")\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.export_single_yolo_image_and_corresponding_txt(coco_image)","title":"<code>coco_image</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_single_yolo_image_and_corresponding_txt(coco_image_dir)","title":"<code>coco_image_dir</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_single_yolo_image_and_corresponding_txt(output_dir)","title":"<code>output_dir</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_single_yolo_image_and_corresponding_txt(ignore_negative_samples)","title":"<code>ignore_negative_samples</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_yolo_images_and_txts_from_coco_object","title":"<code>export_yolo_images_and_txts_from_coco_object(output_dir, coco, ignore_negative_samples=False, mp=False, disable_symlink=False)</code>","text":"<p>Creates image symlinks and annotation txts in yolo format from coco dataset.</p> <p>Parameters:</p> Name Type Description Default <p>str Export directory.</p> required <p>sahi.utils.coco.Coco Initialized Coco object that contains images and categories.</p> required <p>bool If True ignores images without annotations in all operations.</p> <code>False</code> <p>bool If True, multiprocess mode is on. Should be called in 'if name == main:' block.</p> <code>False</code> <p>bool If True, symlinks are not created. Instead images are copied.</p> <code>False</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def export_yolo_images_and_txts_from_coco_object(\n    output_dir, coco, ignore_negative_samples=False, mp=False, disable_symlink=False\n):\n    \"\"\"Creates image symlinks and annotation txts in yolo format from coco dataset.\n\n    Args:\n        output_dir: str\n            Export directory.\n        coco: sahi.utils.coco.Coco\n            Initialized Coco object that contains images and categories.\n        ignore_negative_samples: bool\n            If True ignores images without annotations in all operations.\n        mp: bool\n            If True, multiprocess mode is on.\n            Should be called in 'if __name__ == __main__:' block.\n        disable_symlink: bool\n            If True, symlinks are not created. Instead images are copied.\n    \"\"\"\n    logger.info(\"generating image symlinks and annotation files for yolo...\")\n    # symlink is not supported in colab\n    if is_colab() and not disable_symlink:\n        logger.warning(\"symlink is not supported in colab, disabling it...\")\n        disable_symlink = True\n    if mp:\n        with Pool(processes=48) as pool:\n            args = [\n                (coco_image, coco.image_dir, output_dir, ignore_negative_samples, disable_symlink)\n                for coco_image in coco.images\n            ]\n            pool.starmap(\n                export_single_yolo_image_and_corresponding_txt,\n                tqdm(args, total=len(args)),\n            )\n    else:\n        for coco_image in tqdm(coco.images):\n            export_single_yolo_image_and_corresponding_txt(\n                coco_image, coco.image_dir, output_dir, ignore_negative_samples, disable_symlink\n            )\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.export_yolo_images_and_txts_from_coco_object(output_dir)","title":"<code>output_dir</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_yolo_images_and_txts_from_coco_object(coco)","title":"<code>coco</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_yolo_images_and_txts_from_coco_object(ignore_negative_samples)","title":"<code>ignore_negative_samples</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_yolo_images_and_txts_from_coco_object(mp)","title":"<code>mp</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.export_yolo_images_and_txts_from_coco_object(disable_symlink)","title":"<code>disable_symlink</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.get_imageid2annotationlist_mapping","title":"<code>get_imageid2annotationlist_mapping(coco_dict)</code>","text":"<p>Get image_id to annotationlist mapping for faster indexing.</p> <p>Args</p> <pre><code>coco_dict : dict\n    coco dict with fields \"images\", \"annotations\", \"categories\"\n</code></pre> <p>Returns</p> <pre><code>image_id_to_annotation_list : dict\n{\n    1: [CocoAnnotation, CocoAnnotation, CocoAnnotation],\n    2: [CocoAnnotation]\n}\n\nwhere\nCocoAnnotation = {\n    'area': 2795520,\n    'bbox': [491.0, 1035.0, 153.0, 182.0],\n    'category_id': 1,\n    'id': 1,\n    'image_id': 1,\n    'iscrowd': 0,\n    'segmentation': [[491.0, 1035.0, 644.0, 1035.0, 644.0, 1217.0, 491.0, 1217.0]]\n}\n</code></pre> Source code in <code>sahi/utils/coco.py</code> <pre><code>def get_imageid2annotationlist_mapping(coco_dict: dict) -&gt; dict[int, list[CocoAnnotation]]:\n    \"\"\"Get image_id to annotationlist mapping for faster indexing.\n\n    Args\n\n        coco_dict : dict\n            coco dict with fields \"images\", \"annotations\", \"categories\"\n    Returns\n\n        image_id_to_annotation_list : dict\n        {\n            1: [CocoAnnotation, CocoAnnotation, CocoAnnotation],\n            2: [CocoAnnotation]\n        }\n\n        where\n        CocoAnnotation = {\n            'area': 2795520,\n            'bbox': [491.0, 1035.0, 153.0, 182.0],\n            'category_id': 1,\n            'id': 1,\n            'image_id': 1,\n            'iscrowd': 0,\n            'segmentation': [[491.0, 1035.0, 644.0, 1035.0, 644.0, 1217.0, 491.0, 1217.0]]\n        }\n    \"\"\"\n    image_id_to_annotation_list: dict = defaultdict(list)\n    logger.debug(\"indexing coco dataset annotations...\")\n    for annotation in coco_dict[\"annotations\"]:\n        image_id = annotation[\"image_id\"]\n        image_id_to_annotation_list[image_id].append(annotation)\n\n    return image_id_to_annotation_list\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.merge","title":"<code>merge(coco_dict1, coco_dict2, desired_name2id=None)</code>","text":"<p>Combines 2 coco formatted annotations dicts, and returns the combined coco dict.</p> <p>Parameters:</p> Name Type Description Default <p>dict First coco dictionary.</p> required <p>dict Second coco dictionary.</p> required <p>dict</p> required <p>Returns:     merged_coco_dict : dict         Merged COCO dict.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def merge(coco_dict1: dict, coco_dict2: dict, desired_name2id: dict | None = None) -&gt; dict:\n    \"\"\"Combines 2 coco formatted annotations dicts, and returns the combined coco dict.\n\n    Args:\n        coco_dict1 : dict\n            First coco dictionary.\n        coco_dict2 : dict\n            Second coco dictionary.\n        desired_name2id : dict\n            {\"human\": 1, \"car\": 2, \"big_vehicle\": 3}\n    Returns:\n        merged_coco_dict : dict\n            Merged COCO dict.\n    \"\"\"\n\n    # copy input dicts so that original dicts are not affected\n    temp_coco_dict1 = copy.deepcopy(coco_dict1)\n    temp_coco_dict2 = copy.deepcopy(coco_dict2)\n\n    # rearrange categories if any desired_name2id mapping is given\n    if desired_name2id is not None:\n        temp_coco_dict1 = update_categories(desired_name2id, temp_coco_dict1)\n        temp_coco_dict2 = update_categories(desired_name2id, temp_coco_dict2)\n\n    # rearrange categories of the second coco based on first, if their categories are not the same\n    if temp_coco_dict1[\"categories\"] != temp_coco_dict2[\"categories\"]:\n        desired_name2id = {category[\"name\"]: category[\"id\"] for category in temp_coco_dict1[\"categories\"]}\n        temp_coco_dict2 = update_categories(desired_name2id, temp_coco_dict2)\n\n    # calculate first image and annotation index of the second coco file\n    max_image_id = np.array([image[\"id\"] for image in coco_dict1[\"images\"]]).max()\n    max_annotation_id = np.array([annotation[\"id\"] for annotation in coco_dict1[\"annotations\"]]).max()\n\n    merged_coco_dict = temp_coco_dict1\n\n    for image in temp_coco_dict2[\"images\"]:\n        image[\"id\"] += max_image_id + 1\n        merged_coco_dict[\"images\"].append(image)\n\n    for annotation in temp_coco_dict2[\"annotations\"]:\n        annotation[\"image_id\"] += max_image_id + 1\n        annotation[\"id\"] += max_annotation_id + 1\n        merged_coco_dict[\"annotations\"].append(annotation)\n\n    return merged_coco_dict\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.merge(coco_dict1 )","title":"<code>coco_dict1 </code>","text":""},{"location":"utils/coco/#sahi.utils.coco.merge(coco_dict2 )","title":"<code>coco_dict2 </code>","text":""},{"location":"utils/coco/#sahi.utils.coco.merge(desired_name2id )","title":"<code>desired_name2id </code>","text":""},{"location":"utils/coco/#sahi.utils.coco.merge_from_file","title":"<code>merge_from_file(coco_path1, coco_path2, save_path)</code>","text":"<p>Combines 2 coco formatted annotations files given their paths, and saves the combined file to save_path.</p> <p>Args:</p> <pre><code>coco_path1 : str\n    Path for the first coco file.\ncoco_path2 : str\n    Path for the second coco file.\nsave_path : str\n    \"dirname/coco.json\"\n</code></pre> Source code in <code>sahi/utils/coco.py</code> <pre><code>def merge_from_file(coco_path1: str, coco_path2: str, save_path: str):\n    \"\"\"Combines 2 coco formatted annotations files given their paths, and saves the combined file to save_path.\n\n    Args:\n\n        coco_path1 : str\n            Path for the first coco file.\n        coco_path2 : str\n            Path for the second coco file.\n        save_path : str\n            \"dirname/coco.json\"\n    \"\"\"\n\n    # load coco files to be combined\n    coco_dict1 = load_json(coco_path1)\n    coco_dict2 = load_json(coco_path2)\n\n    # merge coco dicts\n    merged_coco_dict = merge(coco_dict1, coco_dict2)\n\n    # save merged coco dict\n    save_json(merged_coco_dict, save_path)\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.merge_from_list","title":"<code>merge_from_list(coco_dict_list, desired_name2id=None, verbose=1)</code>","text":"<p>Combines a list of coco formatted annotations dicts, and returns the combined coco dict.</p> <p>Args:</p> <pre><code>coco_dict_list: list of dict\n    A list of coco dicts\ndesired_name2id: dict\n    {\"human\": 1, \"car\": 2, \"big_vehicle\": 3}\nverbose: bool\n    If True, merging info is printed\n</code></pre> <p>Returns:</p> <pre><code>merged_coco_dict: dict\n    Merged COCO dict.\n</code></pre> Source code in <code>sahi/utils/coco.py</code> <pre><code>def merge_from_list(coco_dict_list, desired_name2id=None, verbose=1):\n    \"\"\"Combines a list of coco formatted annotations dicts, and returns the combined coco dict.\n\n    Args:\n\n        coco_dict_list: list of dict\n            A list of coco dicts\n        desired_name2id: dict\n            {\"human\": 1, \"car\": 2, \"big_vehicle\": 3}\n        verbose: bool\n            If True, merging info is printed\n    Returns:\n\n        merged_coco_dict: dict\n            Merged COCO dict.\n    \"\"\"\n    if verbose:\n        if not desired_name2id:\n            print(\"'desired_name2id' is not specified, combining all categories.\")\n\n    # create desired_name2id by combinin all categories, if desired_name2id is not specified\n    if desired_name2id is None:\n        desired_name2id = {}\n        ind = 0\n        for coco_dict in coco_dict_list:\n            temp_categories = copy.deepcopy(coco_dict[\"categories\"])\n            for temp_category in temp_categories:\n                if temp_category[\"name\"] not in desired_name2id:\n                    desired_name2id[temp_category[\"name\"]] = ind\n                    ind += 1\n                else:\n                    continue\n\n    for ind, coco_dict in enumerate(coco_dict_list):\n        if ind == 0:\n            merged_coco_dict = copy.deepcopy(coco_dict)\n        else:\n            merged_coco_dict = merge(merged_coco_dict, coco_dict, desired_name2id)\n\n    # print categories\n    if verbose:\n        print(\n            \"Categories are formed as:\\n\",\n            merged_coco_dict[\"categories\"],\n        )\n\n    return merged_coco_dict\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.remove_invalid_coco_results","title":"<code>remove_invalid_coco_results(result_list_or_path, dataset_dict_or_path=None)</code>","text":"Removes invalid predictions from coco result such as <ul> <li>negative bbox value</li> <li>extreme bbox value</li> </ul> <p>Parameters:</p> Name Type Description Default <code>list | str</code> <p>path or list for coco result json</p> required <code>optional</code> <p>path or dict for coco dataset json</p> <code>None</code> Source code in <code>sahi/utils/coco.py</code> <pre><code>def remove_invalid_coco_results(result_list_or_path: list | str, dataset_dict_or_path: dict | str | None = None):\n    \"\"\"\n    Removes invalid predictions from coco result such as:\n        - negative bbox value\n        - extreme bbox value\n\n    Args:\n        result_list_or_path: path or list for coco result json\n        dataset_dict_or_path (optional): path or dict for coco dataset json\n    \"\"\"\n\n    # prepare coco results\n    if isinstance(result_list_or_path, str):\n        result_list = load_json(result_list_or_path)\n    elif isinstance(result_list_or_path, list):\n        result_list = result_list_or_path\n    else:\n        raise TypeError('incorrect type for \"result_list_or_path\"')  # type: ignore\n\n    # prepare image info from coco dataset\n    if dataset_dict_or_path is not None:\n        if isinstance(dataset_dict_or_path, str):\n            dataset_dict = load_json(dataset_dict_or_path)\n        elif isinstance(dataset_dict_or_path, dict):\n            dataset_dict = dataset_dict_or_path\n        else:\n            raise TypeError('incorrect type for \"dataset_dict\"')  # type: ignore\n        image_id_to_height = {}\n        image_id_to_width = {}\n        for coco_image in dataset_dict[\"images\"]:\n            image_id_to_height[coco_image[\"id\"]] = coco_image[\"height\"]\n            image_id_to_width[coco_image[\"id\"]] = coco_image[\"width\"]\n\n    # remove invalid predictions\n    fixed_result_list = []\n    for coco_result in result_list:\n        bbox = coco_result[\"bbox\"]\n        # ignore invalid predictions\n        if not bbox:\n            print(\"ignoring invalid prediction with empty bbox\")\n            continue\n        if bbox[0] &lt; 0 or bbox[1] &lt; 0 or bbox[2] &lt; 0 or bbox[3] &lt; 0:\n            print(f\"ignoring invalid prediction with bbox: {bbox}\")\n            continue\n        if dataset_dict_or_path is not None:\n            if (\n                bbox[1] &gt; image_id_to_height[coco_result[\"image_id\"]]\n                or bbox[3] &gt; image_id_to_height[coco_result[\"image_id\"]]\n                or bbox[0] &gt; image_id_to_width[coco_result[\"image_id\"]]\n                or bbox[2] &gt; image_id_to_width[coco_result[\"image_id\"]]\n            ):\n                print(f\"ignoring invalid prediction with bbox: {bbox}\")\n                continue\n        fixed_result_list.append(coco_result)\n    return fixed_result_list\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.remove_invalid_coco_results(result_list_or_path)","title":"<code>result_list_or_path</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.remove_invalid_coco_results(dataset_dict_or_path)","title":"<code>dataset_dict_or_path</code>","text":""},{"location":"utils/coco/#sahi.utils.coco.update_categories","title":"<code>update_categories(desired_name2id, coco_dict)</code>","text":"<p>Rearranges category mapping of given COCO dictionary based on given category_mapping. Can also be used to filter some of the categories.</p> <p>Args:</p> <pre><code>desired_name2id : dict\n    {\"big_vehicle\": 1, \"car\": 2, \"human\": 3}\ncoco_dict : dict\n    COCO formatted dictionary.\n</code></pre> <p>Returns:</p> Name Type Description <code>coco_target</code> <code>dict</code> <p>dict COCO dict with updated/filtered categories.</p> Source code in <code>sahi/utils/coco.py</code> <pre><code>def update_categories(desired_name2id: dict, coco_dict: dict) -&gt; dict:\n    \"\"\"Rearranges category mapping of given COCO dictionary based on given category_mapping. Can also be used to filter\n    some of the categories.\n\n    Args:\n\n        desired_name2id : dict\n            {\"big_vehicle\": 1, \"car\": 2, \"human\": 3}\n        coco_dict : dict\n            COCO formatted dictionary.\n\n    Returns:\n        coco_target : dict\n            COCO dict with updated/filtered categories.\n    \"\"\"\n    # so that original variable doesn't get affected\n    coco_source = copy.deepcopy(coco_dict)\n\n    # init target coco dict\n    coco_target = {\"images\": [], \"annotations\": [], \"categories\": []}\n\n    # init vars\n    currentid2desiredid_mapping = {}\n    # create category id mapping (currentid2desiredid_mapping)\n    for category in coco_source[\"categories\"]:\n        current_category_id = category[\"id\"]\n        current_category_name = category[\"name\"]\n        if current_category_name in desired_name2id.keys():\n            currentid2desiredid_mapping[current_category_id] = desired_name2id[current_category_name]\n        else:\n            # ignore categories that are not included in desired_name2id\n            currentid2desiredid_mapping[current_category_id] = -1\n\n    # update annotations\n    for annotation in coco_source[\"annotations\"]:\n        current_category_id = annotation[\"category_id\"]\n        desired_category_id = currentid2desiredid_mapping[current_category_id]\n        # append annotations with category id present in desired_name2id\n        if desired_category_id != -1:\n            # update cetegory id\n            annotation[\"category_id\"] = desired_category_id\n            # append updated annotation to target coco dict\n            coco_target[\"annotations\"].append(annotation)\n\n    # create desired categories\n    categories = []\n    for name in desired_name2id.keys():\n        category = {}\n        category[\"name\"] = category[\"supercategory\"] = name\n        category[\"id\"] = desired_name2id[name]\n        categories.append(category)\n\n    # update categories\n    coco_target[\"categories\"] = categories\n\n    # update images\n    coco_target[\"images\"] = coco_source[\"images\"]\n\n    return coco_target\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.update_categories_from_file","title":"<code>update_categories_from_file(desired_name2id, coco_path, save_path)</code>","text":"<p>Rearranges category mapping of a COCO dictionary in coco_path based on given category_mapping. Can also be used to filter some of the categories.</p> <p>Parameters:</p> Name Type Description Default <p>dict</p> required <p>str \"dirname/coco.json\"</p> required Source code in <code>sahi/utils/coco.py</code> <pre><code>def update_categories_from_file(desired_name2id: dict, coco_path: str, save_path: str) -&gt; None:\n    \"\"\"Rearranges category mapping of a COCO dictionary in coco_path based on given category_mapping. Can also be used\n    to filter some of the categories.\n\n    Args:\n        desired_name2id : dict\n            {\"human\": 1, \"car\": 2, \"big_vehicle\": 3}\n        coco_path : str\n            \"dirname/coco.json\"\n    \"\"\"\n    # load source coco dict\n    coco_source = load_json(coco_path)\n\n    # update categories\n    coco_target = update_categories(desired_name2id, coco_source)\n\n    # save modified coco file\n    save_json(coco_target, save_path)\n</code></pre>"},{"location":"utils/coco/#sahi.utils.coco.update_categories_from_file(desired_name2id )","title":"<code>desired_name2id </code>","text":""},{"location":"utils/coco/#sahi.utils.coco.update_categories_from_file(coco_path )","title":"<code>coco_path </code>","text":""},{"location":"utils/cv/","title":"CV","text":""},{"location":"utils/cv/#sahi.utils.cv","title":"<code>sahi.utils.cv</code>","text":""},{"location":"utils/cv/#sahi.utils.cv-classes","title":"Classes","text":""},{"location":"utils/cv/#sahi.utils.cv.Colors","title":"<code>Colors</code>","text":"Source code in <code>sahi/utils/cv.py</code> <pre><code>class Colors:\n    def __init__(self):\n        hex_colors = (\n            \"FF3838 2C99A8 FF701F 6473FF CFD231 48F90A 92CC17 3DDB86 1A9334 00D4BB \"\n            \"FF9D97 00C2FF 344593 FFB21D 0018EC 8438FF 520085 CB38FF FF95C8 FF37C7\"\n        )\n\n        self.palette = [self.hex_to_rgb(f\"#{c}\") for c in hex_colors.split()]\n        self.n = len(self.palette)\n\n    def __call__(self, ind, bgr: bool = False):\n        \"\"\"Convert an index to a color code.\n\n        Args:\n            ind (int): The index to convert.\n            bgr (bool, optional): Whether to return the color code in BGR format. Defaults to False.\n\n        Returns:\n            tuple: The color code in RGB or BGR format, depending on the value of `bgr`.\n        \"\"\"\n        color_codes = self.palette[int(ind) % self.n]\n        return (color_codes[2], color_codes[1], color_codes[0]) if bgr else color_codes\n\n    @staticmethod\n    def hex_to_rgb(hex_code):\n        \"\"\"Converts a hexadecimal color code to RGB format.\n\n        Args:\n            hex_code (str): The hexadecimal color code to convert.\n\n        Returns:\n            tuple: A tuple representing the RGB values in the order (R, G, B).\n        \"\"\"\n        rgb = []\n        for i in (0, 2, 4):\n            rgb.append(int(hex_code[1 + i : 1 + i + 2], 16))\n        return tuple(rgb)\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.Colors-functions","title":"Functions","text":""},{"location":"utils/cv/#sahi.utils.cv.Colors.__call__","title":"<code>__call__(ind, bgr=False)</code>","text":"<p>Convert an index to a color code.</p> <p>Parameters:</p> Name Type Description Default <code>ind</code> \u00b6 <code>int</code> <p>The index to convert.</p> required <code>bgr</code> \u00b6 <code>bool</code> <p>Whether to return the color code in BGR format. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>The color code in RGB or BGR format, depending on the value of <code>bgr</code>.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def __call__(self, ind, bgr: bool = False):\n    \"\"\"Convert an index to a color code.\n\n    Args:\n        ind (int): The index to convert.\n        bgr (bool, optional): Whether to return the color code in BGR format. Defaults to False.\n\n    Returns:\n        tuple: The color code in RGB or BGR format, depending on the value of `bgr`.\n    \"\"\"\n    color_codes = self.palette[int(ind) % self.n]\n    return (color_codes[2], color_codes[1], color_codes[0]) if bgr else color_codes\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.Colors.hex_to_rgb","title":"<code>hex_to_rgb(hex_code)</code>  <code>staticmethod</code>","text":"<p>Converts a hexadecimal color code to RGB format.</p> <p>Parameters:</p> Name Type Description Default <code>hex_code</code> \u00b6 <code>str</code> <p>The hexadecimal color code to convert.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple representing the RGB values in the order (R, G, B).</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>@staticmethod\ndef hex_to_rgb(hex_code):\n    \"\"\"Converts a hexadecimal color code to RGB format.\n\n    Args:\n        hex_code (str): The hexadecimal color code to convert.\n\n    Returns:\n        tuple: A tuple representing the RGB values in the order (R, G, B).\n    \"\"\"\n    rgb = []\n    for i in (0, 2, 4):\n        rgb.append(int(hex_code[1 + i : 1 + i + 2], 16))\n    return tuple(rgb)\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv-functions","title":"Functions","text":""},{"location":"utils/cv/#sahi.utils.cv.apply_color_mask","title":"<code>apply_color_mask(image, color)</code>","text":"<p>Applies color mask to given input image.</p> <p>Parameters:</p> Name Type Description Default <code>ndarray</code> <p>The input image to apply the color mask to.</p> required <code>tuple</code> <p>The RGB color tuple to use for the mask.</p> required <p>Returns:</p> Type Description <p>np.ndarray: The resulting image with the applied color mask.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def apply_color_mask(image: np.ndarray, color: tuple[int, int, int]):\n    \"\"\"Applies color mask to given input image.\n\n    Args:\n        image (np.ndarray): The input image to apply the color mask to.\n        color (tuple): The RGB color tuple to use for the mask.\n\n    Returns:\n        np.ndarray: The resulting image with the applied color mask.\n    \"\"\"\n    r = np.zeros_like(image).astype(np.uint8)\n    g = np.zeros_like(image).astype(np.uint8)\n    b = np.zeros_like(image).astype(np.uint8)\n\n    (r[image == 1], g[image == 1], b[image == 1]) = color\n    colored_mask = np.stack([r, g, b], axis=2)\n    return colored_mask\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.apply_color_mask(image)","title":"<code>image</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.apply_color_mask(color)","title":"<code>color</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.convert_image_to","title":"<code>convert_image_to(read_path, extension='jpg', grayscale=False)</code>","text":"<p>Reads an image from the given path and saves it with the specified extension.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The path to the image file.</p> required <code>str</code> <p>The desired file extension for the saved image. Defaults to \"jpg\".</p> <code>'jpg'</code> <code>bool</code> <p>Whether to convert the image to grayscale. Defaults to False.</p> <code>False</code> Source code in <code>sahi/utils/cv.py</code> <pre><code>def convert_image_to(read_path, extension: str = \"jpg\", grayscale: bool = False):\n    \"\"\"Reads an image from the given path and saves it with the specified extension.\n\n    Args:\n        read_path (str): The path to the image file.\n        extension (str, optional): The desired file extension for the saved image. Defaults to \"jpg\".\n        grayscale (bool, optional): Whether to convert the image to grayscale. Defaults to False.\n    \"\"\"\n    image = cv2.imread(read_path)\n    pre, _ = os.path.splitext(read_path)\n    if grayscale:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        pre = pre + \"_gray\"\n    save_path = pre + \".\" + extension\n    cv2.imwrite(save_path, image)\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.convert_image_to(read_path)","title":"<code>read_path</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.convert_image_to(extension)","title":"<code>extension</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.convert_image_to(grayscale)","title":"<code>grayscale</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.crop_object_predictions","title":"<code>crop_object_predictions(image, object_prediction_list, output_dir='', file_name='prediction_visual', export_format='png')</code>","text":"<p>Crops bounding boxes over the source image and exports it to the output folder.</p> <p>Parameters:</p> Name Type Description Default <code>ndarray</code> <p>The source image to crop bounding boxes from.</p> required <p>A list of object predictions.</p> required <code>str</code> <p>The directory where the resulting visualizations will be exported. Defaults to an empty string.</p> <code>''</code> <code>str</code> <p>The name of the exported file. The exported file will be saved as <code>output_dir + file_name + \".png\"</code>. Defaults to \"prediction_visual\".</p> <code>'prediction_visual'</code> <code>str</code> <p>The format of the exported file. Can be specified as 'jpg' or 'png'. Defaults to \"png\".</p> <code>'png'</code> Source code in <code>sahi/utils/cv.py</code> <pre><code>def crop_object_predictions(\n    image: np.ndarray,\n    object_prediction_list,\n    output_dir: str = \"\",\n    file_name: str = \"prediction_visual\",\n    export_format: str = \"png\",\n):\n    \"\"\"Crops bounding boxes over the source image and exports it to the output folder.\n\n    Args:\n        image (np.ndarray): The source image to crop bounding boxes from.\n        object_prediction_list: A list of object predictions.\n        output_dir (str): The directory where the resulting visualizations will be exported. Defaults to an empty string.\n        file_name (str): The name of the exported file. The exported file will be saved as `output_dir + file_name + \".png\"`. Defaults to \"prediction_visual\".\n        export_format (str): The format of the exported file. Can be specified as 'jpg' or 'png'. Defaults to \"png\".\n    \"\"\"  # noqa\n\n    # create output folder if not present\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\n    # add bbox and mask to image if present\n    for ind, object_prediction in enumerate(object_prediction_list):\n        # deepcopy object_prediction_list so that the original is not altered\n        object_prediction = object_prediction.deepcopy()\n        bbox = object_prediction.bbox.to_xyxy()\n        category_id = object_prediction.category.id\n        # crop detections\n        # deepcopy crops so that the original is not altered\n        cropped_img = copy.deepcopy(\n            image[\n                int(bbox[1]) : int(bbox[3]),\n                int(bbox[0]) : int(bbox[2]),\n                :,\n            ]\n        )\n        save_path = os.path.join(\n            output_dir,\n            file_name + \"_box\" + str(ind) + \"_class\" + str(category_id) + \".\" + export_format,\n        )\n        cv2.imwrite(save_path, cv2.cvtColor(cropped_img, cv2.COLOR_RGB2BGR))\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.crop_object_predictions(image)","title":"<code>image</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.crop_object_predictions(object_prediction_list)","title":"<code>object_prediction_list</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.crop_object_predictions(output_dir)","title":"<code>output_dir</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.crop_object_predictions(file_name)","title":"<code>file_name</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.crop_object_predictions(export_format)","title":"<code>export_format</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.get_bbox_from_bool_mask","title":"<code>get_bbox_from_bool_mask(bool_mask)</code>","text":"<p>Generate VOC bounding box [xmin, ymin, xmax, ymax] from given boolean mask.</p> <p>Parameters:</p> Name Type Description Default <code>ndarray</code> <p>2D boolean mask.</p> required <p>Returns:</p> Type Description <code>list[int] | None</code> <p>Optional[List[int]]: VOC bounding box [xmin, ymin, xmax, ymax] or None if no bounding box is found.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def get_bbox_from_bool_mask(bool_mask: np.ndarray) -&gt; list[int] | None:\n    \"\"\"Generate VOC bounding box [xmin, ymin, xmax, ymax] from given boolean mask.\n\n    Args:\n        bool_mask (np.ndarray): 2D boolean mask.\n\n    Returns:\n        Optional[List[int]]: VOC bounding box [xmin, ymin, xmax, ymax] or None if no bounding box is found.\n    \"\"\"\n    rows = np.any(bool_mask, axis=1)\n    cols = np.any(bool_mask, axis=0)\n\n    if not np.any(rows) or not np.any(cols):\n        return None\n\n    ymin, ymax = np.where(rows)[0][[0, -1]]\n    xmin, xmax = np.where(cols)[0][[0, -1]]\n    width = xmax - xmin\n    height = ymax - ymin\n\n    if width == 0 or height == 0:\n        return None\n\n    return [xmin, ymin, xmax, ymax]\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.get_bbox_from_bool_mask(bool_mask)","title":"<code>bool_mask</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.get_bbox_from_coco_segmentation","title":"<code>get_bbox_from_coco_segmentation(coco_segmentation)</code>","text":"<p>Generate voc box ([xmin, ymin, xmax, ymax]) from given coco segmentation.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def get_bbox_from_coco_segmentation(coco_segmentation):\n    \"\"\"Generate voc box ([xmin, ymin, xmax, ymax]) from given coco segmentation.\"\"\"\n    xs = []\n    ys = []\n    for segm in coco_segmentation:\n        xs.extend(segm[::2])\n        ys.extend(segm[1::2])\n    if len(xs) == 0 or len(ys) == 0:\n        return None\n    xmin = min(xs)\n    xmax = max(xs)\n    ymin = min(ys)\n    ymax = max(ys)\n    return [xmin, ymin, xmax, ymax]\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.get_bool_mask_from_coco_segmentation","title":"<code>get_bool_mask_from_coco_segmentation(coco_segmentation, width, height)</code>","text":"<p>Convert coco segmentation to 2D boolean mask of given height and width.</p> <p>Parameters: - coco_segmentation: list of points representing the coco segmentation - width: width of the boolean mask - height: height of the boolean mask</p> <p>Returns: - bool_mask: 2D boolean mask of size (height, width)</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def get_bool_mask_from_coco_segmentation(coco_segmentation: list[list[float]], width: int, height: int) -&gt; np.ndarray:\n    \"\"\"Convert coco segmentation to 2D boolean mask of given height and width.\n\n    Parameters:\n    - coco_segmentation: list of points representing the coco segmentation\n    - width: width of the boolean mask\n    - height: height of the boolean mask\n\n    Returns:\n    - bool_mask: 2D boolean mask of size (height, width)\n    \"\"\"\n    size = [height, width]\n    points = [np.array(point).reshape(-1, 2).round().astype(int) for point in coco_segmentation]\n    bool_mask = np.zeros(size)\n    bool_mask = cv2.fillPoly(bool_mask, points, (1.0,))\n    bool_mask.astype(bool)\n    return bool_mask\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.get_coco_segmentation_from_bool_mask","title":"<code>get_coco_segmentation_from_bool_mask(bool_mask)</code>","text":"<p>Convert boolean mask to coco segmentation format [     [x1, y1, x2, y2, x3, y3, ...],     [x1, y1, x2, y2, x3, y3, ...],     ... ]</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def get_coco_segmentation_from_bool_mask(bool_mask: np.ndarray) -&gt; list[list[float]]:\n    \"\"\"\n    Convert boolean mask to coco segmentation format\n    [\n        [x1, y1, x2, y2, x3, y3, ...],\n        [x1, y1, x2, y2, x3, y3, ...],\n        ...\n    ]\n    \"\"\"\n    # Generate polygons from mask\n    mask = np.squeeze(bool_mask)\n    mask = mask.astype(np.uint8)\n    mask = cv2.copyMakeBorder(mask, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n    polygons = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE, offset=(-1, -1))\n    polygons = polygons[0] if len(polygons) == 2 else polygons[1]\n    # Convert polygon to coco segmentation\n    coco_segmentation = []\n    for polygon in polygons:\n        segmentation = polygon.flatten().tolist()\n        # at least 3 points needed for a polygon\n        if len(segmentation) &gt;= 6:\n            coco_segmentation.append(segmentation)\n    return coco_segmentation\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.get_coco_segmentation_from_obb_points","title":"<code>get_coco_segmentation_from_obb_points(obb_points)</code>","text":"<p>Convert OBB (Oriented Bounding Box) points to COCO polygon format.</p> <p>Parameters:</p> Name Type Description Default <code>ndarray</code> <p>np.ndarray OBB points tensor from ultralytics.engine.results.OBB Shape: (4, 2) containing 4 points with (x,y) coordinates each</p> required <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>List[List[float]]: Polygon points in COCO format [[x1, y1, x2, y2, x3, y3, x4, y4], [...], ...]</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def get_coco_segmentation_from_obb_points(obb_points: np.ndarray) -&gt; list[list[float]]:\n    \"\"\"Convert OBB (Oriented Bounding Box) points to COCO polygon format.\n\n    Args:\n        obb_points: np.ndarray\n            OBB points tensor from ultralytics.engine.results.OBB\n            Shape: (4, 2) containing 4 points with (x,y) coordinates each\n\n    Returns:\n        List[List[float]]: Polygon points in COCO format\n            [[x1, y1, x2, y2, x3, y3, x4, y4], [...], ...]\n    \"\"\"\n    # Convert from (4,2) to [x1,y1,x2,y2,x3,y3,x4,y4] format\n    points = obb_points.reshape(-1).tolist()\n\n    # Create polygon from points and close it by repeating first point\n    polygons = []\n    # Add first point to end to close polygon\n    closed_polygon = [*points, points[0], points[1]]\n    polygons.append(closed_polygon)\n\n    return polygons\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.get_coco_segmentation_from_obb_points(obb_points)","title":"<code>obb_points</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.get_video_reader","title":"<code>get_video_reader(source, save_dir, frame_skip_interval, export_visual=False, view_visual=False)</code>","text":"<p>Creates OpenCV video capture object from given video file path.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Video file path</p> required <code>str</code> <p>Video export directory</p> required <code>int</code> <p>Frame skip interval</p> required <code>bool</code> <p>Set True if you want to export visuals</p> <code>False</code> <code>bool</code> <p>Set True if you want to render visual</p> <code>False</code> <p>Returns:</p> Name Type Description <code>iterator</code> <code>Generator[Image]</code> <p>Pillow Image</p> <code>video_writer</code> <code>VideoWriter | None</code> <p>cv2.VideoWriter</p> <code>video_file_name</code> <code>str</code> <p>video name with extension</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def get_video_reader(\n    source: str,\n    save_dir: str,\n    frame_skip_interval: int,\n    export_visual: bool = False,\n    view_visual: bool = False,\n) -&gt; tuple[Generator[Image.Image], cv2.VideoWriter | None, str, int]:\n    \"\"\"Creates OpenCV video capture object from given video file path.\n\n    Args:\n        source: Video file path\n        save_dir: Video export directory\n        frame_skip_interval: Frame skip interval\n        export_visual: Set True if you want to export visuals\n        view_visual: Set True if you want to render visual\n\n    Returns:\n        iterator: Pillow Image\n        video_writer: cv2.VideoWriter\n        video_file_name: video name with extension\n    \"\"\"\n    # get video name with extension\n    video_file_name = os.path.basename(source)\n    # get video from video path\n    video_capture = cv2.VideoCapture(source)\n\n    num_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    if view_visual:\n        num_frames /= frame_skip_interval + 1\n        num_frames = int(num_frames)\n\n    def read_video_frame(video_capture, frame_skip_interval) -&gt; Generator[Image.Image]:\n        if view_visual:\n            window_name = f\"Prediction of {video_file_name!s}\"\n            cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)\n            default_image = np.zeros((480, 640, 3), dtype=np.uint8)\n            cv2.imshow(window_name, default_image)\n\n            while video_capture.isOpened:\n                frame_num = video_capture.get(cv2.CAP_PROP_POS_FRAMES)\n                video_capture.set(cv2.CAP_PROP_POS_FRAMES, frame_num + frame_skip_interval)\n\n                k = cv2.waitKey(20)\n                frame_num = video_capture.get(cv2.CAP_PROP_POS_FRAMES)\n\n                if k == 27:\n                    print(\n                        \"\\n===========================Closing===========================\"\n                    )  # Exit the prediction, Key = Esc\n                    exit()\n                if k == 100:\n                    frame_num += 100  # Skip 100 frames, Key = d\n                if k == 97:\n                    frame_num -= 100  # Prev 100 frames, Key = a\n                if k == 103:\n                    frame_num += 20  # Skip 20 frames, Key = g\n                if k == 102:\n                    frame_num -= 20  # Prev 20 frames, Key = f\n                video_capture.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n\n                ret, frame = video_capture.read()\n                if not ret:\n                    print(\"\\n=========================== Video Ended ===========================\")\n                    break\n                yield Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\n        else:\n            while video_capture.isOpened:\n                frame_num = video_capture.get(cv2.CAP_PROP_POS_FRAMES)\n                video_capture.set(cv2.CAP_PROP_POS_FRAMES, frame_num + frame_skip_interval)\n\n                ret, frame = video_capture.read()\n                if not ret:\n                    print(\"\\n=========================== Video Ended ===========================\")\n                    break\n                yield Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\n    if export_visual:\n        # get video properties and create VideoWriter object\n        if frame_skip_interval != 0:\n            fps = video_capture.get(cv2.CAP_PROP_FPS)  # original fps of video\n            # The fps of export video is increasing during view_image because frame is skipped\n            fps = (\n                fps / frame_skip_interval\n            )  # How many time_interval equals to original fps. One time_interval skip x frames.\n        else:\n            fps = video_capture.get(cv2.CAP_PROP_FPS)\n\n        w = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n        h = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        size = (w, h)\n        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # pyright: ignore[reportAttributeAccessIssue]\n        video_writer = cv2.VideoWriter(os.path.join(save_dir, video_file_name), fourcc, fps, size)\n    else:\n        video_writer = None\n\n    return read_video_frame(video_capture, frame_skip_interval), video_writer, video_file_name, num_frames\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.get_video_reader(source)","title":"<code>source</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.get_video_reader(save_dir)","title":"<code>save_dir</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.get_video_reader(frame_skip_interval)","title":"<code>frame_skip_interval</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.get_video_reader(export_visual)","title":"<code>export_visual</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.get_video_reader(view_visual)","title":"<code>view_visual</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.ipython_display","title":"<code>ipython_display(image)</code>","text":"<p>Displays numpy image in notebook.</p> <p>If input image is in range 0..1, please first multiply img by 255 Assumes image is ndarray of shape [height, width, channels] where channels can be 1, 3 or 4</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def ipython_display(image: np.ndarray):\n    \"\"\"Displays numpy image in notebook.\n\n    If input image is in range 0..1, please first multiply img by 255\n    Assumes image is ndarray of shape [height, width, channels] where channels can be 1, 3 or 4\n    \"\"\"\n    import IPython\n\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    _, ret = cv2.imencode(\".png\", image)\n    i = IPython.display.Image(data=ret)  # type: ignore\n    IPython.display.display(i)  # type: ignore\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.normalize_numpy_image","title":"<code>normalize_numpy_image(image)</code>","text":"<p>Normalizes numpy image.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def normalize_numpy_image(image: np.ndarray):\n    \"\"\"Normalizes numpy image.\"\"\"\n    return image / np.max(image)\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.read_image","title":"<code>read_image(image_path)</code>","text":"<p>Loads image as a numpy array from the given path.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The path to the image file.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: The loaded image as a numpy array.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def read_image(image_path: str) -&gt; np.ndarray:\n    \"\"\"Loads image as a numpy array from the given path.\n\n    Args:\n        image_path (str): The path to the image file.\n\n    Returns:\n        numpy.ndarray: The loaded image as a numpy array.\n    \"\"\"\n    # read image\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    # return image\n    return image\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.read_image(image_path)","title":"<code>image_path</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.read_image_as_pil","title":"<code>read_image_as_pil(image, exif_fix=True)</code>","text":"<p>Loads an image as PIL.Image.Image.</p> <p>Parameters:</p> Name Type Description Default <code>Union[Image, str, ndarray]</code> <p>The image to be loaded. It can be an image path or URL (str), a numpy image (np.ndarray), or a PIL.Image object.</p> required <code>bool</code> <p>Whether to apply an EXIF fix to the image. Defaults to False.</p> <code>True</code> <p>Returns:</p> Type Description <code>Image</code> <p>PIL.Image.Image: The loaded image as a PIL.Image object.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def read_image_as_pil(image: Image.Image | str | np.ndarray, exif_fix: bool = True) -&gt; Image.Image:\n    \"\"\"Loads an image as PIL.Image.Image.\n\n    Args:\n        image (Union[Image.Image, str, np.ndarray]): The image to be loaded. It can be an image path or URL (str),\n            a numpy image (np.ndarray), or a PIL.Image object.\n        exif_fix (bool, optional): Whether to apply an EXIF fix to the image. Defaults to False.\n\n    Returns:\n        PIL.Image.Image: The loaded image as a PIL.Image object.\n    \"\"\"\n    # https://stackoverflow.com/questions/56174099/how-to-load-images-larger-than-max-image-pixels-with-pil\n    Image.MAX_IMAGE_PIXELS = None\n\n    if isinstance(image, Image.Image):\n        image_pil = image\n    elif isinstance(image, str):\n        # read image if str image path is provided\n        try:\n            image_pil = Image.open(\n                BytesIO(requests.get(image, stream=True).content) if str(image).startswith(\"http\") else image\n            ).convert(\"RGB\")\n            if exif_fix:\n                ImageOps.exif_transpose(image_pil, in_place=True)\n        except Exception as e:  # handle large/tiff image reading\n            logger.error(f\"PIL failed reading image with error {e}, trying skimage instead\")\n            try:\n                import skimage.io\n            except ImportError:\n                raise ImportError(\"Please run 'pip install -U scikit-image imagecodecs' for large image handling.\")\n            image_sk = skimage.io.imread(image).astype(np.uint8)\n            if len(image_sk.shape) == 2:  # b&amp;w\n                image_pil = Image.fromarray(image_sk, mode=\"1\")\n            elif image_sk.shape[2] == 4:  # rgba\n                image_pil = Image.fromarray(image_sk, mode=\"RGBA\")\n            elif image_sk.shape[2] == 3:  # rgb\n                image_pil = Image.fromarray(image_sk, mode=\"RGB\")\n            else:\n                raise TypeError(f\"image with shape: {image_sk.shape[3]} is not supported.\")\n    elif isinstance(image, np.ndarray):\n        if image.shape[0] &lt; 5:  # image in CHW\n            image = image[:, :, ::-1]\n        image_pil = Image.fromarray(image)\n    else:\n        raise TypeError(\"read image with 'pillow' using 'Image.open()'\")\n    return image_pil\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.read_image_as_pil(image)","title":"<code>image</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.read_image_as_pil(exif_fix)","title":"<code>exif_fix</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.read_large_image","title":"<code>read_large_image(image_path)</code>","text":"<p>Reads a large image from the specified image path.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The path to the image file.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the image data and a flag indicating whether cv2 was used to read the image. The image data is a numpy array representing the image in RGB format. The flag is True if cv2 was used, False otherwise.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def read_large_image(image_path: str):\n    \"\"\"Reads a large image from the specified image path.\n\n    Args:\n        image_path (str): The path to the image file.\n\n    Returns:\n        tuple: A tuple containing the image data and a flag indicating whether cv2 was used to read the image.\n            The image data is a numpy array representing the image in RGB format.\n            The flag is True if cv2 was used, False otherwise.\n    \"\"\"\n    use_cv2 = True\n    # read image, cv2 fails on large files\n    try:\n        # convert to rgb (cv2 reads in bgr)\n        img_cv2 = cv2.imread(image_path, 1)\n        image0 = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n    except Exception as e:\n        logger.error(f\"OpenCV failed reading image with error {e}, trying skimage instead\")\n        try:\n            import skimage.io\n        except ImportError:\n            raise ImportError(\n                'Please run \"pip install -U scikit-image\" to install scikit-image first for large image handling.'\n            )\n        image0 = skimage.io.imread(image_path, as_grey=False).astype(np.uint8)  # [::-1]\n        use_cv2 = False\n    return image0, use_cv2\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.read_large_image(image_path)","title":"<code>image_path</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.select_random_color","title":"<code>select_random_color()</code>","text":"<p>Selects a random color from a predefined list of colors.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list representing the RGB values of the selected color.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def select_random_color():\n    \"\"\"Selects a random color from a predefined list of colors.\n\n    Returns:\n        list: A list representing the RGB values of the selected color.\n    \"\"\"\n    colors = [\n        [0, 255, 0],\n        [0, 0, 255],\n        [255, 0, 0],\n        [0, 255, 255],\n        [255, 255, 0],\n        [255, 0, 255],\n        [80, 70, 180],\n        [250, 80, 190],\n        [245, 145, 50],\n        [70, 150, 250],\n        [50, 190, 190],\n    ]\n    return colors[random.randrange(0, 10)]\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.visualize_object_predictions","title":"<code>visualize_object_predictions(image, object_prediction_list, rect_th=None, text_size=None, text_th=None, color=None, hide_labels=False, hide_conf=False, output_dir=None, file_name='prediction_visual', export_format='png')</code>","text":"<p>Visualizes prediction category names, bounding boxes over the source image and exports it to output folder.</p> <p>Parameters:</p> Name Type Description Default <p>a list of prediction.ObjectPrediction</p> required <code>int | None</code> <p>rectangle thickness</p> <code>None</code> <code>float | None</code> <p>size of the category name over box</p> <code>None</code> <code>int | None</code> <p>text thickness</p> <code>None</code> <code>tuple | None</code> <p>annotation color in the form: (0, 255, 0)</p> <code>None</code> <code>bool</code> <p>hide labels</p> <code>False</code> <code>bool</code> <p>hide confidence</p> <code>False</code> <code>str | None</code> <p>directory for resulting visualization to be exported</p> <code>None</code> <code>str | None</code> <p>exported file will be saved as: output_dir+file_name+\".png\"</p> <code>'prediction_visual'</code> <code>str | None</code> <p>can be specified as 'jpg' or 'png'</p> <code>'png'</code> Source code in <code>sahi/utils/cv.py</code> <pre><code>def visualize_object_predictions(\n    image: np.ndarray,\n    object_prediction_list,\n    rect_th: int | None = None,\n    text_size: float | None = None,\n    text_th: int | None = None,\n    color: tuple | None = None,\n    hide_labels: bool = False,\n    hide_conf: bool = False,\n    output_dir: str | None = None,\n    file_name: str | None = \"prediction_visual\",\n    export_format: str | None = \"png\",\n):\n    \"\"\"Visualizes prediction category names, bounding boxes over the source image and exports it to output folder.\n\n    Args:\n        object_prediction_list: a list of prediction.ObjectPrediction\n        rect_th: rectangle thickness\n        text_size: size of the category name over box\n        text_th: text thickness\n        color: annotation color in the form: (0, 255, 0)\n        hide_labels: hide labels\n        hide_conf: hide confidence\n        output_dir: directory for resulting visualization to be exported\n        file_name: exported file will be saved as: output_dir+file_name+\".png\"\n        export_format: can be specified as 'jpg' or 'png'\n    \"\"\"\n    elapsed_time = time.time()\n    # deepcopy image so that original is not altered\n    image = copy.deepcopy(image)\n    # select predefined classwise color palette if not specified\n    if color is None:\n        colors = Colors()\n    else:\n        colors = None\n    # set rect_th for boxes\n    rect_th = rect_th or max(round(sum(image.shape) / 2 * 0.003), 2)\n    # set text_th for category names\n    text_th = text_th or max(rect_th - 1, 1)\n    # set text_size for category names\n    text_size = text_size or rect_th / 3\n\n    # add masks or obb polygons to image if present\n    for object_prediction in object_prediction_list:\n        # deepcopy object_prediction_list so that original is not altered\n        object_prediction = object_prediction.deepcopy()\n        # arange label to be displayed\n        label = f\"{object_prediction.category.name}\"\n        if not hide_conf:\n            label += f\" {object_prediction.score.value:.2f}\"\n        # set color\n        if colors is not None:\n            color = colors(object_prediction.category.id)\n        # visualize masks or obb polygons if present\n        has_mask = object_prediction.mask is not None\n        is_obb_pred = False\n        if has_mask:\n            segmentation = object_prediction.mask.segmentation\n            if len(segmentation) == 1 and len(segmentation[0]) == 8:\n                is_obb_pred = True\n\n            if is_obb_pred:\n                points = np.array(segmentation).reshape((-1, 1, 2)).astype(np.int32)\n                cv2.polylines(image, [points], isClosed=True, color=color or (0, 0, 0), thickness=rect_th)\n\n                if not hide_labels:\n                    lowest_point = points[points[:, :, 1].argmax()][0]\n                    box_width, box_height = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[0]\n                    outside = lowest_point[1] - box_height - 3 &gt;= 0\n                    text_bg_point1 = (\n                        lowest_point[0],\n                        lowest_point[1] - box_height - 3 if outside else lowest_point[1] + 3,\n                    )\n                    text_bg_point2 = (lowest_point[0] + box_width, lowest_point[1])\n                    cv2.rectangle(\n                        image, text_bg_point1, text_bg_point2, color or (0, 0, 0), thickness=-1, lineType=cv2.LINE_AA\n                    )\n                    cv2.putText(\n                        image,\n                        label,\n                        (lowest_point[0], lowest_point[1] - 2 if outside else lowest_point[1] + box_height + 2),\n                        0,\n                        text_size,\n                        (255, 255, 255),\n                        thickness=text_th,\n                    )\n            else:\n                # draw mask\n                rgb_mask = apply_color_mask(object_prediction.mask.bool_mask, color or (0, 0, 0))\n                image = cv2.addWeighted(image, 1, rgb_mask, 0.6, 0)\n\n        # add bboxes to image if is_obb_pred=False\n        if not is_obb_pred:\n            bbox = object_prediction.bbox.to_xyxy()\n\n            # set bbox points\n            point1, point2 = (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3]))\n            # visualize boxes\n            cv2.rectangle(\n                image,\n                point1,\n                point2,\n                color=color or (0, 0, 0),\n                thickness=rect_th,\n            )\n\n            if not hide_labels:\n                box_width, box_height = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[\n                    0\n                ]  # label width, height\n                outside = point1[1] - box_height - 3 &gt;= 0  # label fits outside box\n                point2 = point1[0] + box_width, point1[1] - box_height - 3 if outside else point1[1] + box_height + 3\n                # add bounding box text\n                cv2.rectangle(image, point1, point2, color or (0, 0, 0), -1, cv2.LINE_AA)  # filled\n                cv2.putText(\n                    image,\n                    label,\n                    (point1[0], point1[1] - 2 if outside else point1[1] + box_height + 2),\n                    0,\n                    text_size,\n                    (255, 255, 255),\n                    thickness=text_th,\n                )\n\n    # export if output_dir is present\n    if output_dir is not None:\n        # export image with predictions\n        Path(output_dir).mkdir(parents=True, exist_ok=True)\n        # save inference result\n        save_path = str(Path(output_dir) / ((file_name or \"\") + \".\" + (export_format or \"\")))\n        cv2.imwrite(save_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n\n    elapsed_time = time.time() - elapsed_time\n    return {\"image\": image, \"elapsed_time\": elapsed_time}\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.visualize_object_predictions(object_prediction_list)","title":"<code>object_prediction_list</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_object_predictions(rect_th)","title":"<code>rect_th</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_object_predictions(text_size)","title":"<code>text_size</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_object_predictions(text_th)","title":"<code>text_th</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_object_predictions(color)","title":"<code>color</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_object_predictions(hide_labels)","title":"<code>hide_labels</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_object_predictions(hide_conf)","title":"<code>hide_conf</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_object_predictions(output_dir)","title":"<code>output_dir</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_object_predictions(file_name)","title":"<code>file_name</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_object_predictions(export_format)","title":"<code>export_format</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_prediction","title":"<code>visualize_prediction(image, boxes, classes, masks=None, rect_th=None, text_size=None, text_th=None, color=None, hide_labels=False, output_dir=None, file_name='prediction_visual')</code>","text":"<p>Visualizes prediction classes, bounding boxes over the source image and exports it to output folder.</p> <p>Parameters:</p> Name Type Description Default <code>ndarray</code> <p>The source image.</p> required <code>List[List]</code> <p>List of bounding boxes coordinates.</p> required <code>List[str]</code> <p>List of class labels corresponding to each bounding box.</p> required <code>Optional[List[ndarray]]</code> <p>List of masks corresponding to each bounding box. Defaults to None.</p> <code>None</code> <code>int</code> <p>Thickness of the bounding box rectangle. Defaults to None.</p> <code>None</code> <code>float</code> <p>Size of the text for class labels. Defaults to None.</p> <code>None</code> <code>int</code> <p>Thickness of the text for class labels. Defaults to None.</p> <code>None</code> <code>tuple</code> <p>Color of the bounding box and text. Defaults to None.</p> <code>None</code> <code>bool</code> <p>Whether to hide the class labels. Defaults to False.</p> <code>False</code> <code>Optional[str]</code> <p>Output directory to save the visualization. Defaults to None.</p> <code>None</code> <code>Optional[str]</code> <p>File name for the saved visualization. Defaults to \"prediction_visual\".</p> <code>'prediction_visual'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the visualized image and the elapsed time for the visualization process.</p> Source code in <code>sahi/utils/cv.py</code> <pre><code>def visualize_prediction(\n    image: np.ndarray,\n    boxes: list[list],\n    classes: list[str],\n    masks: list[np.ndarray] | None = None,\n    rect_th: int | None = None,\n    text_size: float | None = None,\n    text_th: int | None = None,\n    color: tuple | None = None,\n    hide_labels: bool = False,\n    output_dir: str | None = None,\n    file_name: str | None = \"prediction_visual\",\n):\n    \"\"\"Visualizes prediction classes, bounding boxes over the source image and exports it to output folder.\n\n    Args:\n        image (np.ndarray): The source image.\n        boxes (List[List]): List of bounding boxes coordinates.\n        classes (List[str]): List of class labels corresponding to each bounding box.\n        masks (Optional[List[np.ndarray]], optional): List of masks corresponding to each bounding box. Defaults to None.\n        rect_th (int, optional): Thickness of the bounding box rectangle. Defaults to None.\n        text_size (float, optional): Size of the text for class labels. Defaults to None.\n        text_th (int, optional): Thickness of the text for class labels. Defaults to None.\n        color (tuple, optional): Color of the bounding box and text. Defaults to None.\n        hide_labels (bool, optional): Whether to hide the class labels. Defaults to False.\n        output_dir (Optional[str], optional): Output directory to save the visualization. Defaults to None.\n        file_name (Optional[str], optional): File name for the saved visualization. Defaults to \"prediction_visual\".\n\n    Returns:\n        dict: A dictionary containing the visualized image and the elapsed time for the visualization process.\n    \"\"\"  # noqa\n\n    elapsed_time = time.time()\n    # deepcopy image so that original is not altered\n    image = copy.deepcopy(image)\n    # select predefined classwise color palette if not specified\n    if color is None:\n        colors = Colors()\n    else:\n        colors = None\n    # set rect_th for boxes\n    rect_th = rect_th or max(round(sum(image.shape) / 2 * 0.003), 2)\n    # set text_th for category names\n    text_th = text_th or max(rect_th - 1, 1)\n    # set text_size for category names\n    text_size = text_size or rect_th / 3\n\n    # add masks to image if present\n    if masks is not None and color is None:\n        logger.error(\"Cannot add mask, no color tuple given\")\n    elif masks is not None and color is not None:\n        for mask in masks:\n            # deepcopy mask so that original is not altered\n            mask = copy.deepcopy(mask)\n            # draw mask\n            rgb_mask = apply_color_mask(np.squeeze(mask), color)\n            image = cv2.addWeighted(image, 1, rgb_mask, 0.6, 0)\n\n    # add bboxes to image if present\n    for box_indice in range(len(boxes)):\n        # deepcopy boxso that original is not altered\n        box = copy.deepcopy(boxes[box_indice])\n        class_ = classes[box_indice]\n\n        # set color\n        if colors is not None:\n            mycolor = colors(class_)\n        elif color is not None:\n            mycolor = color\n        else:\n            logger.error(\"color cannot be defined\")\n            continue\n\n        # set bbox points\n        point1, point2 = [int(box[0]), int(box[1])], [int(box[2]), int(box[3])]\n        # visualize boxes\n        cv2.rectangle(\n            image,\n            point1,\n            point2,\n            color=mycolor,\n            thickness=rect_th,\n        )\n\n        if not hide_labels:\n            # arange bounding box text location\n            label = f\"{class_}\"\n            box_width, box_height = cv2.getTextSize(label, 0, fontScale=text_size, thickness=text_th)[\n                0\n            ]  # label width, height\n            outside = point1[1] - box_height - 3 &gt;= 0  # label fits outside box\n            point2 = point1[0] + box_width, point1[1] - box_height - 3 if outside else point1[1] + box_height + 3\n            # add bounding box text\n            cv2.rectangle(image, point1, point2, color or (0, 0, 0), -1, cv2.LINE_AA)  # filled\n            cv2.putText(\n                image,\n                label,\n                (point1[0], point1[1] - 2 if outside else point1[1] + box_height + 2),\n                0,\n                text_size,\n                (255, 255, 255),\n                thickness=text_th,\n            )\n    if output_dir:\n        # create output folder if not present\n        Path(output_dir).mkdir(parents=True, exist_ok=True)\n        # save inference result\n        save_path = os.path.join(output_dir, (file_name or \"unknown\") + \".png\")\n        cv2.imwrite(save_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n\n    elapsed_time = time.time() - elapsed_time\n    return {\"image\": image, \"elapsed_time\": elapsed_time}\n</code></pre>"},{"location":"utils/cv/#sahi.utils.cv.visualize_prediction(image)","title":"<code>image</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_prediction(boxes)","title":"<code>boxes</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_prediction(classes)","title":"<code>classes</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_prediction(masks)","title":"<code>masks</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_prediction(rect_th)","title":"<code>rect_th</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_prediction(text_size)","title":"<code>text_size</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_prediction(text_th)","title":"<code>text_th</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_prediction(color)","title":"<code>color</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_prediction(hide_labels)","title":"<code>hide_labels</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_prediction(output_dir)","title":"<code>output_dir</code>","text":""},{"location":"utils/cv/#sahi.utils.cv.visualize_prediction(file_name)","title":"<code>file_name</code>","text":""},{"location":"utils/file/","title":"File Utilities","text":""},{"location":"utils/file/#sahi.utils.file","title":"<code>sahi.utils.file</code>","text":""},{"location":"utils/file/#sahi.utils.file-functions","title":"Functions","text":""},{"location":"utils/file/#sahi.utils.file.download_from_url","title":"<code>download_from_url(from_url, to_path)</code>","text":"<p>Downloads a file from the given URL and saves it to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The URL of the file to download.</p> required <code>str</code> <p>The path where the downloaded file should be saved.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def download_from_url(from_url: str, to_path: str):\n    \"\"\"Downloads a file from the given URL and saves it to the specified path.\n\n    Args:\n        from_url (str): The URL of the file to download.\n        to_path (str): The path where the downloaded file should be saved.\n\n    Returns:\n        None\n    \"\"\"\n    Path(to_path).parent.mkdir(parents=True, exist_ok=True)\n\n    if not os.path.exists(to_path):\n        urllib.request.urlretrieve(\n            from_url,\n            to_path,\n        )\n</code></pre>"},{"location":"utils/file/#sahi.utils.file.download_from_url(from_url)","title":"<code>from_url</code>","text":""},{"location":"utils/file/#sahi.utils.file.download_from_url(to_path)","title":"<code>to_path</code>","text":""},{"location":"utils/file/#sahi.utils.file.get_base_filename","title":"<code>get_base_filename(path)</code>","text":"<p>Takes a file path, returns (base_filename_with_extension, base_filename_without_extension)</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def get_base_filename(path: str):\n    \"\"\"Takes a file path, returns (base_filename_with_extension, base_filename_without_extension)\"\"\"\n    base_filename_with_extension = ntpath.basename(path)\n    base_filename_without_extension, _ = os.path.splitext(base_filename_with_extension)\n    return base_filename_with_extension, base_filename_without_extension\n</code></pre>"},{"location":"utils/file/#sahi.utils.file.get_file_extension","title":"<code>get_file_extension(path)</code>","text":"<p>Get the file extension from a given file path.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The file path.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The file extension.</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def get_file_extension(path: str):\n    \"\"\"Get the file extension from a given file path.\n\n    Args:\n        path (str): The file path.\n\n    Returns:\n        str: The file extension.\n    \"\"\"\n    _, file_extension = os.path.splitext(path)\n    return file_extension\n</code></pre>"},{"location":"utils/file/#sahi.utils.file.get_file_extension(path)","title":"<code>path</code>","text":""},{"location":"utils/file/#sahi.utils.file.import_model_class","title":"<code>import_model_class(model_type, class_name)</code>","text":"<p>Imports a predefined detection class by class name.</p> <p>Parameters:</p> Name Type Description Default <p>str \"yolov5\", \"detectron2\", \"mmdet\", \"huggingface\" etc</p> required <p>str Name of the detection model class (example: \"MmdetDetectionModel\")</p> required <p>Returns:     class_: class with given path</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def import_model_class(model_type, class_name):\n    \"\"\"Imports a predefined detection class by class name.\n\n    Args:\n        model_type: str\n            \"yolov5\", \"detectron2\", \"mmdet\", \"huggingface\" etc\n        model_name: str\n            Name of the detection model class (example: \"MmdetDetectionModel\")\n    Returns:\n        class_: class with given path\n    \"\"\"\n    module = __import__(f\"sahi.models.{model_type}\", fromlist=[class_name])\n    class_ = getattr(module, class_name)\n    return class_\n</code></pre>"},{"location":"utils/file/#sahi.utils.file.import_model_class(model_type)","title":"<code>model_type</code>","text":""},{"location":"utils/file/#sahi.utils.file.import_model_class(model_name)","title":"<code>model_name</code>","text":""},{"location":"utils/file/#sahi.utils.file.increment_path","title":"<code>increment_path(path, exist_ok=True, sep='')</code>","text":"<p>Increment path, i.e. runs/exp --&gt; runs/exp{sep}0, runs/exp{sep}1 etc.</p> <p>Parameters:</p> Name Type Description Default <code>str | Path</code> <p>str The base path to increment.</p> required <code>bool</code> <p>bool If True, return the path as is if it already exists. If False, increment the path.</p> <code>True</code> <code>str</code> <p>str The separator to use between the base path and the increment number.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The incremented path.</p> Example <p>increment_path(\"runs/exp\", sep=\"\") 'runs/exp_0' increment_path(\"runs/exp_0\", sep=\"\") 'runs/exp_1'</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def increment_path(path: str | Path, exist_ok: bool = True, sep: str = \"\") -&gt; str:\n    \"\"\"Increment path, i.e. runs/exp --&gt; runs/exp{sep}0, runs/exp{sep}1 etc.\n\n    Args:\n        path: str\n            The base path to increment.\n        exist_ok: bool\n            If True, return the path as is if it already exists. If False, increment the path.\n        sep: str\n            The separator to use between the base path and the increment number.\n\n    Returns:\n        str: The incremented path.\n\n    Example:\n        &gt;&gt;&gt; increment_path(\"runs/exp\", sep=\"_\")\n        'runs/exp_0'\n        &gt;&gt;&gt; increment_path(\"runs/exp_0\", sep=\"_\")\n        'runs/exp_1'\n    \"\"\"\n    path = Path(path)  # os-agnostic\n    if (path.exists() and exist_ok) or (not path.exists()):\n        return str(path)\n    else:\n        dirs = glob.glob(f\"{path}{sep}*\")  # similar paths\n        matches = [re.search(rf\"%s{sep}(\\d+)\" % path.stem, d) for d in dirs]\n        indices = [int(m.groups()[0]) for m in matches if m]  # indices\n        n = max(indices) + 1 if indices else 2  # increment number\n        return f\"{path}{sep}{n}\"  # update path\n</code></pre>"},{"location":"utils/file/#sahi.utils.file.increment_path(path)","title":"<code>path</code>","text":""},{"location":"utils/file/#sahi.utils.file.increment_path(exist_ok)","title":"<code>exist_ok</code>","text":""},{"location":"utils/file/#sahi.utils.file.increment_path(sep)","title":"<code>sep</code>","text":""},{"location":"utils/file/#sahi.utils.file.is_colab","title":"<code>is_colab()</code>","text":"<p>Check if the current environment is a Google Colab instance.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the environment is a Google Colab instance, False otherwise.</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def is_colab():\n    \"\"\"Check if the current environment is a Google Colab instance.\n\n    Returns:\n        bool: True if the environment is a Google Colab instance, False otherwise.\n    \"\"\"\n    import sys\n\n    return \"google.colab\" in sys.modules\n</code></pre>"},{"location":"utils/file/#sahi.utils.file.list_files","title":"<code>list_files(directory, contains=['.json'], verbose=1)</code>","text":"<p>Walk given directory and return a list of file path with desired extension.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>str \"data/coco/\"</p> required <code>list</code> <p>list A list of strings to check if the target file contains them, example: [\"coco.png\", \".jpg\", \"jpeg\"]</p> <code>['.json']</code> <code>int</code> <p>int 0: no print 1: print number of files</p> <code>1</code> <p>Returns:</p> Name Type Description <code>filepath_list</code> <code>list[str]</code> <p>list List of file paths</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def list_files(\n    directory: str,\n    contains: list = [\".json\"],\n    verbose: int = 1,\n) -&gt; list[str]:\n    \"\"\"Walk given directory and return a list of file path with desired extension.\n\n    Args:\n        directory: str\n            \"data/coco/\"\n        contains: list\n            A list of strings to check if the target file contains them, example: [\"coco.png\", \".jpg\", \"jpeg\"]\n        verbose: int\n            0: no print\n            1: print number of files\n\n    Returns:\n        filepath_list : list\n            List of file paths\n    \"\"\"\n    # define verboseprint\n    verboseprint = print if verbose else lambda *a, **k: None\n\n    filepath_list: list[str] = []\n\n    for file in os.listdir(directory):\n        # check if filename contains any of the terms given in contains list\n        if any(strtocheck in file.lower() for strtocheck in contains):\n            filepath = str(os.path.join(directory, file))\n            filepath_list.append(filepath)\n\n    number_of_files = len(filepath_list)\n    folder_name = Path(directory).name\n\n    verboseprint(f\"There are {number_of_files!s} listed files in folder: {folder_name}/\")\n\n    return filepath_list\n</code></pre>"},{"location":"utils/file/#sahi.utils.file.list_files(directory)","title":"<code>directory</code>","text":""},{"location":"utils/file/#sahi.utils.file.list_files(contains)","title":"<code>contains</code>","text":""},{"location":"utils/file/#sahi.utils.file.list_files(verbose)","title":"<code>verbose</code>","text":""},{"location":"utils/file/#sahi.utils.file.list_files_recursively","title":"<code>list_files_recursively(directory, contains=['.json'], verbose=True)</code>","text":"<p>Walk given directory recursively and return a list of file path with desired extension.</p> <p>Parameters:</p> Name Type Description Default <p>str \"data/coco/\"</p> required <p>list A list of strings to check if the target file contains them, example: [\"coco.png\", \".jpg\", \"jpeg\"]</p> required <p>bool If true, prints some results</p> required <p>Returns:</p> Name Type Description <code>relative_filepath_list</code> <code>list</code> <p>list List of file paths relative to given directory</p> <code>abs_filepath_list</code> <code>list</code> <p>list List of absolute file paths</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def list_files_recursively(directory: str, contains: list = [\".json\"], verbose: bool = True) -&gt; tuple[list, list]:\n    \"\"\"Walk given directory recursively and return a list of file path with desired extension.\n\n    Args:\n        directory : str\n            \"data/coco/\"\n        contains : list\n            A list of strings to check if the target file contains them, example: [\"coco.png\", \".jpg\", \"jpeg\"]\n        verbose : bool\n            If true, prints some results\n\n    Returns:\n        relative_filepath_list : list\n            List of file paths relative to given directory\n        abs_filepath_list : list\n            List of absolute file paths\n    \"\"\"\n\n    # define verboseprint\n    verboseprint = print if verbose else lambda *a, **k: None\n\n    # walk directories recursively and find json files\n    abs_filepath_list = []\n    relative_filepath_list = []\n\n    # r=root, d=directories, f=files\n    for r, _, f in os.walk(directory):\n        for file in f:\n            # check if filename contains any of the terms given in contains list\n            if any(strtocheck in file.lower() for strtocheck in contains):\n                abs_filepath = os.path.join(r, file)\n                abs_filepath_list.append(abs_filepath)\n                relative_filepath = abs_filepath.split(directory)[-1]\n                relative_filepath_list.append(relative_filepath)\n\n    number_of_files = len(relative_filepath_list)\n    folder_name = directory.split(os.sep)[-1]\n\n    verboseprint(f\"There are {number_of_files} listed files in folder {folder_name}.\")\n\n    return relative_filepath_list, abs_filepath_list\n</code></pre>"},{"location":"utils/file/#sahi.utils.file.list_files_recursively(directory )","title":"<code>directory </code>","text":""},{"location":"utils/file/#sahi.utils.file.list_files_recursively(contains )","title":"<code>contains </code>","text":""},{"location":"utils/file/#sahi.utils.file.list_files_recursively(verbose )","title":"<code>verbose </code>","text":""},{"location":"utils/file/#sahi.utils.file.load_json","title":"<code>load_json(load_path, encoding='utf-8')</code>","text":"<p>Loads json formatted data (given as \"data\") from load_path Encoding type can be specified with 'encoding' argument.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>str \"dirname/coco.json\"</p> required <code>str</code> <p>str Encoding type, default is 'utf-8'</p> <code>'utf-8'</code> Example inputs <p>load_path: \"dirname/coco.json\"</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def load_json(load_path: str, encoding: str = \"utf-8\"):\n    \"\"\"Loads json formatted data (given as \"data\") from load_path Encoding type can be specified with 'encoding'\n    argument.\n\n    Args:\n        load_path: str\n            \"dirname/coco.json\"\n        encoding: str\n            Encoding type, default is 'utf-8'\n\n    Example inputs:\n        load_path: \"dirname/coco.json\"\n    \"\"\"\n    # read from path\n    with open(load_path, encoding=encoding) as json_file:\n        data = json.load(json_file)\n    return data\n</code></pre>"},{"location":"utils/file/#sahi.utils.file.load_json(load_path)","title":"<code>load_path</code>","text":""},{"location":"utils/file/#sahi.utils.file.load_json(encoding)","title":"<code>encoding</code>","text":""},{"location":"utils/file/#sahi.utils.file.load_pickle","title":"<code>load_pickle(load_path)</code>","text":"<p>Loads pickle formatted data (given as \"data\") from load_path</p> <p>Parameters:</p> Name Type Description Default <p>str \"dirname/coco.pickle\"</p> required Example inputs <p>load_path: \"dirname/coco.pickle\"</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def load_pickle(load_path):\n    \"\"\"\n    Loads pickle formatted data (given as \"data\") from load_path\n\n    Args:\n        load_path: str\n            \"dirname/coco.pickle\"\n\n    Example inputs:\n        load_path: \"dirname/coco.pickle\"\n    \"\"\"\n    with open(load_path, \"rb\") as json_file:\n        data = pickle.load(json_file)\n    return data\n</code></pre>"},{"location":"utils/file/#sahi.utils.file.load_pickle(load_path)","title":"<code>load_path</code>","text":""},{"location":"utils/file/#sahi.utils.file.save_json","title":"<code>save_json(data, save_path, indent=None)</code>","text":"<p>Saves json formatted data (given as \"data\") as save_path</p> <p>Parameters:</p> Name Type Description Default <p>dict Data to be saved as json</p> required <p>str \"dirname/coco.json\"</p> required <code>int | None</code> <p>int or None Indentation level for pretty-printing the JSON data. If None, the most compact representation will be used. If an integer is provided, it specifies the number of spaces to use for indentation. Example: indent=4 will format the JSON data with an indentation of 4 spaces per level.</p> <code>None</code> Example inputs <p>data: {\"image_id\": 5} save_path: \"dirname/coco.json\" indent: Train json files with indent=None, val json files with indent=4</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def save_json(data, save_path, indent: int | None = None):\n    \"\"\"\n    Saves json formatted data (given as \"data\") as save_path\n\n    Args:\n        data: dict\n            Data to be saved as json\n        save_path: str\n            \"dirname/coco.json\"\n        indent: int or None\n            Indentation level for pretty-printing the JSON data. If None, the most compact representation\n            will be used. If an integer is provided, it specifies the number of spaces to use for indentation.\n            Example: indent=4 will format the JSON data with an indentation of 4 spaces per level.\n\n    Example inputs:\n        data: {\"image_id\": 5}\n        save_path: \"dirname/coco.json\"\n        indent: Train json files with indent=None, val json files with indent=4\n    \"\"\"\n    # create dir if not present\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n\n    # export as json\n    with open(save_path, \"w\", encoding=\"utf-8\") as outfile:\n        json.dump(data, outfile, separators=(\",\", \":\"), cls=NumpyEncoder, indent=indent)\n</code></pre>"},{"location":"utils/file/#sahi.utils.file.save_json(data)","title":"<code>data</code>","text":""},{"location":"utils/file/#sahi.utils.file.save_json(save_path)","title":"<code>save_path</code>","text":""},{"location":"utils/file/#sahi.utils.file.save_json(indent)","title":"<code>indent</code>","text":""},{"location":"utils/file/#sahi.utils.file.save_pickle","title":"<code>save_pickle(data, save_path)</code>","text":"<p>Saves pickle formatted data (given as \"data\") as save_path</p> <p>Parameters:</p> Name Type Description Default <p>dict Data to be saved as pickle</p> required <p>str \"dirname/coco.pickle\"</p> required Example inputs <p>data: {\"image_id\": 5} save_path: \"dirname/coco.pickle\"</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def save_pickle(data, save_path):\n    \"\"\"\n    Saves pickle formatted data (given as \"data\") as save_path\n\n    Args:\n        data: dict\n            Data to be saved as pickle\n        save_path: str\n            \"dirname/coco.pickle\"\n\n    Example inputs:\n        data: {\"image_id\": 5}\n        save_path: \"dirname/coco.pickle\"\n    \"\"\"\n    # create dir if not present\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n\n    # export as json\n    with open(save_path, \"wb\") as outfile:\n        pickle.dump(data, outfile)\n</code></pre>"},{"location":"utils/file/#sahi.utils.file.save_pickle(data)","title":"<code>data</code>","text":""},{"location":"utils/file/#sahi.utils.file.save_pickle(save_path)","title":"<code>save_path</code>","text":""},{"location":"utils/file/#sahi.utils.file.unzip","title":"<code>unzip(file_path, dest_dir)</code>","text":"<p>Unzips compressed .zip file.</p> Example inputs <p>file_path: 'data/01_alb_id.zip' dest_dir: 'data/'</p> Source code in <code>sahi/utils/file.py</code> <pre><code>def unzip(file_path: str, dest_dir: str):\n    \"\"\"Unzips compressed .zip file.\n\n    Example inputs:\n        file_path: 'data/01_alb_id.zip'\n        dest_dir: 'data/'\n    \"\"\"\n\n    # unzip file\n    with zipfile.ZipFile(file_path) as zf:\n        zf.extractall(dest_dir)\n</code></pre>"}]}